[
  {
    "slug": "4d-framework",
    "collection": "concepts",
    "title": "The 4D Framework (Anthropic)",
    "description": "A cognitive model codifying four essential competencies—Delegation, Description, Discernment, and Diligence—for effective generative AI use.",
    "status": "Live",
    "content": "## Definition\n\nThe **4D Framework** is a cognitive model for human-AI collaboration developed by [Anthropic](https://www.anthropic.com/research) in partnership with Dr. Joseph Feller and Rick Dakan as part of the *AI Fluency* curriculum.\n\nThe framework codifies four essential competencies for leveraging generative AI effectively and responsibly:\n\n1. **Delegation** — The Strategy\n2. **Description** — The Prompt\n3. **Discernment** — The Review\n4. **Diligence** — The Liability\n\nUnlike process models (e.g., Agile or Double Diamond) that dictate workflow timing, the 4D Framework specifies *how* to interact with AI systems. It positions the human not merely as a \"prompter,\" but as an **Editor-in-Chief**, accountable for strategic direction and risk management.\n\n## The Four Dimensions\n\n### Delegation (The Strategy)\n\nBefore engaging with the tool, the human operator must determine what, if anything, should be assigned to the AI. This is a strategic decision between **Automation** (offloading repetitive tasks) and **Augmentation** (leveraging AI as a thought partner).\n\n**Core Question:** \"Is this task 'boilerplate' with well-defined rules (High Delegation), or does it demand nuanced judgment, deep context, or ethical considerations (Low Delegation)?\"\n\n### Description (The Prompt)\n\nAI output quality is directly proportional to input quality. \"Description\" transcends prompt engineering hacks by emphasizing **Context Transfer**—delivering explicit goals, constraints, and data structures required for the task.\n\n**Core Question:** \"Have I specified the constraints, interface definitions, and success criteria needed for this task?\"\n\n### Discernment (The Review)\n\nThis marks the transition from **Creator** to **Editor**. The human must rigorously assess AI output for accuracy, hallucinations, bias, and overall quality. Failing to apply discernment is a leading cause of \"AI Technical Debt.\"\n\n**Core Question:** \"If I authored this output, would it meet code review standards? Does it introduce fictitious libraries or violate design tokens?\"\n\n### Diligence (The Liability)\n\nThe human user retains full accountability for outcomes. Diligence acknowledges that while AI accelerates execution, it never removes user responsibility for security, copyright, or ethical compliance.\n\n**Core Question:** \"Am I exposing PII in the context window? Am I deploying unvetted code to production?\"\n\n## Key Characteristics\n\n### The Editor-in-Chief Mental Model\n\nThe 4D Framework repositions the human from \"prompt writer\" to \"editorial director.\" Just as a newspaper editor doesn't write every article but maintains accountability for what gets published, the AI-fluent professional maintains responsibility for all AI-generated outputs.\n\n### Continuous Cycle\n\nThese four dimensions are not sequential steps but concurrent concerns. Every AI interaction requires simultaneous attention to all four:\n\n- What should I delegate?\n- How clearly have I described it?\n- How critically am I reviewing the output?\n- What risks am I accepting?\n\n## Anti-Patterns\n\n| Anti-Pattern | Description |\n|--------------|-------------|\n| **Over-Delegation** | Assigning strategic decisions or ethically sensitive tasks to AI |\n| **Vague Description** | Using natural language prompts without context, constraints, or examples |\n| **Blind Acceptance** | Copy-pasting AI output without verification |\n| **Liability Denial** | Assuming AI-generated content is inherently trustworthy or legally defensible |\n\n## ASDLC Usage\n\nApplied in: [AGENTS.md Specification](/practices/agents-md-spec), [Context Engineering](/concepts/context-engineering), [Context Gates](/patterns/context-gates)\n\nThe 4D dimensions map to ASDLC constructs: Delegation → agent autonomy levels, Description → context engineering, Discernment → context gates, Diligence → guardrail protocols.",
    "tags": ["AI Fluency", "Human-AI Collaboration", "Cognitive Model", "Prompt Engineering"],
    "references": [
      {
        "type": "website",
        "title": "Anthropic AI Fluency Course",
        "url": "https://anthropic.skilljar.com/page/ai-fluency",
        "author": "Anthropic",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Original source of the 4D Framework for effective generative AI use, teaching the core competencies needed for human-AI collaboration."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2026-01-09T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Internal study of 132 engineers quantifying the 4D dimensions in practice: delegation patterns, cold start problems, the paradox of supervision, and human-owned decision categories."
      }
    ]
  },
  {
    "slug": "agentic-sdlc",
    "collection": "concepts",
    "title": "Agentic SDLC",
    "description": "Framework for industrializing software development where agents serve as the logistic layer while humans design, govern, and optimize the flow.",
    "status": "Live",
    "content": "## Definition\n\nThe Agentic Software Development Life Cycle (ASDLC) is a framework for industrializing software engineering. It represents the shift from craft-based development (individual artisans, manual tooling, implicit knowledge) to industrial-scale production (standardized processes, agent orchestration, deterministic protocols).\n\n> \"Agentic architecture is the conveyor belt for knowledge work.\" — [Ville Takanen](https://villetakanen.com/blog/scenario-26-industrialization-of-knowledge-work/)\n\nASDLC is not about \"AI coding assistants\" that make developers 10% faster. It's about building the **software factory**—systems where agents serve as the architecture of labor while humans design, govern, and optimize the flow.\n\n## The Industrial Thesis\n\n**Agents do not replace humans; they industrialize execution.**\n\nJust as robotic arms automate welding without replacing manufacturing expertise, agents automate high-friction parts of knowledge work (logistics, syntax, verification) while humans focus on intent, architecture, and governance.\n\nIn this model:\n- **Agents are the logistic layer** — Moving information, verifying specs, executing tests\n- **Context is the supply chain** — Just-in-Time delivery of requirements, schemas, and code\n- **Standardization is mandatory** — Schemas, typed interfaces, deterministic protocols replace \"vibes\"\n- **AI Amplification** — Agents act as a \"High-Pass Filter\" for process maturity: they accelerate good practices but amplify the chaos of bad ones.\n\n## The Cybernetic Model\n\nASDLC operates at [L3 Conditional Autonomy](/concepts/levels-of-autonomy)—a \"Fighter Jet\" model where the Agent acts as the Pilot executing maneuvers, and the Human acts as the Instructor-in-the-Cockpit.\n\n**Key Insight:** Compute is cheap, but novelty and correctness are expensive. Agents naturally drift toward the \"average\" solution (Regression to the Mean). The Instructor's role is not to write code, but to define failure boundaries (Determinism) and inject strategic intent (Steering) that guides agents out of mediocrity.\n\n## The Cybernetic Loop\n\nThe lifecycle replaces the linear CI/CD pipeline with a high-frequency feedback loop:\n\n**Mission Definition**: The Instructor defines the \"Objective Packet\" (Intent + Constraints). This is the core of Context Engineering.\n\n**Generation (The Maneuver)**: The Agent autonomously maps context—often using the Model Context Protocol (MCP) to fetch live data—and executes the task.\n\n**Verification (The Sim)**: Automated Gates check for technical correctness (deterministic), while the Agent's Constitution steers semantic intent (probabilistic).\n\n**Course Correction (HITL)**: The Instructor intervenes on technically correct but \"generic\" solutions to enforce architectural novelty.\n\n## Strategic Pillars\n\n### Factory Architecture (Orchestration)\nProjects structured with agents as connective tissue, moving from monolithic context windows to discrete, specialized stations (Planning, Spec-Definition, Implementation, Review).\n\n### Standardized Parts (Determinism)\nSchema-First Development where agents fulfill contracts, not guesses. `AGENTS.md`, `specs/`, and strict linting serve as the \"jigs\" and \"molds\" that constrain agent output.\n\n### Quality Control (Governance)\nAutomated, rigorous inspection through Probabilistic Unit Tests and Human-in-the-Loop (HITL) gates. Trust the _process_, not just the output.\n\n## ASDLC Usage\n\nFull project vision: [/docs/vision.md](../../docs/vision.md)\n\nApplied in: [Specs](/patterns/the-spec), [AGENTS.md Specification](/practices/agents-md-spec), [Context Gates](/patterns/context-gates), [Model Routing](/patterns/model-routing)",
    "tags": ["Core", "SDLC", "Methodology", "Industrialization"],
    "references": []
  },
  {
    "slug": "architecture-decision-record",
    "collection": "concepts",
    "title": "Architecture Decision Record",
    "description": "A lightweight document that captures a significant architectural decision, its context, and consequences at a specific point in time.",
    "status": "Live",
    "content": "## Definition\n\nAn **Architecture Decision Record (ADR)** is a document that captures a significant architectural decision along with its context, rationale, and consequences. Unlike living documentation that evolves with the codebase, ADRs are **immutable snapshots**—they record what was decided and why at a specific moment in time.\n\nThe format was introduced by Michael Nygard in 2011 as a lightweight alternative to heavyweight architecture documentation. Each ADR addresses exactly one decision, making the record atomic and traceable.\n\n## Key Characteristics\n\n### Immutability\n\nADRs are not updated when circumstances change. Instead, a new ADR is created that **supersedes** the original. This preserves the archaeological record of how architectural thinking evolved.\n\n### Lightweight\n\nA single ADR fits on one or two pages. The format resists the temptation to over-document, focusing only on the decision and its immediate context.\n\n### Decision-Focused\n\nADRs capture *decisions*, not designs or implementations. The question answered is \"What did we decide?\" not \"How does it work?\" (that belongs in specs) or \"How do I build it?\" (that belongs in implementation guides).\n\n### Contextual\n\nEvery ADR includes the forces and constraints that shaped the decision. This context is critical—a decision that seems wrong in isolation often makes sense when the original constraints are understood.\n\n## Standard Sections\n\nThe canonical ADR format includes:\n\n| Section | Purpose |\n|---------|---------|\n| **Title** | Short name with ID (e.g., \"ADR-001: Use PostgreSQL for Primary Database\") |\n| **Status** | Lifecycle state: Proposed, Accepted, Deprecated, Superseded by ADR-XXX |\n| **Context** | What forces are at play? What problem needs solving? |\n| **Decision** | What was decided? |\n| **Consequences** | Positive, negative, and neutral outcomes of this decision |\n| **Alternatives Considered** | What other options were evaluated and why they were rejected? |\n\n## ASDLC Usage\n\nIn ASDLC, ADRs serve as **high-value context** for agents. When an agent works on authentication, knowing \"ADR-003: Chose Supabase Auth over Firebase Auth\" provides essential architectural constraints.\n\nADRs may also evolve into [Agent Constitution](/patterns/agent-constitution) rules—an ADR stating \"All database migrations must be backward-compatible\" becomes a constitutional constraint that agents must not violate.\n\nApplied in:\n- [The ADR](/patterns/the-adr) — Structural pattern for ADR anatomy\n- [ADR Authoring](/practices/adr-authoring) — Practical guide for writing ADRs\n- [Request for Comments](/concepts/request-for-comments) — Related concept for grouped proposals\n\nSee also:\n- [The Spec](/patterns/the-spec) — Living documentation that ADRs inform but do not replace\n- [Context Engineering](/concepts/context-engineering) — ADRs as context sources for agents",
    "tags": ["Architecture", "Documentation", "Decision Making", "ADR"],
    "references": [
      {
        "type": "website",
        "title": "Documenting Architecture Decisions",
        "url": "https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions",
        "author": "Michael Nygard",
        "published": "2011-11-15T00:00:00.000Z",
        "accessed": "2026-01-28T00:00:00.000Z",
        "annotation": "The original ADR proposal that established the format and philosophy."
      },
      {
        "type": "website",
        "title": "ADR GitHub Organization",
        "url": "https://adr.github.io/",
        "accessed": "2026-01-28T00:00:00.000Z",
        "annotation": "Community hub for ADR tooling, templates, and best practices."
      }
    ]
  },
  {
    "slug": "behavior-driven-development",
    "collection": "concepts",
    "title": "Behavior-Driven Development",
    "description": "A collaborative specification methodology that defines system behavior in natural language scenarios, bridging business intent and machine-verifiable acceptance criteria.",
    "status": "Live",
    "content": "## Definition\n\nBehavior-Driven Development (BDD) is a collaborative specification methodology that defines system behavior in natural language scenarios. It synthesizes Test-Driven Development (TDD) and Acceptance Test-Driven Development (ATDD), emphasizing the \"Five Whys\" principle: every user story should trace to a business outcome.\n\nThe key evolution from testing to BDD is the shift from \"test\" to \"specification.\" Tests verify correctness; specifications define expected behavior. In agentic workflows, this distinction matters because agents need to understand *what* behavior is expected, not just *what code to write*.\n\n## Key Characteristics\n\n### From Tests to Specifications of Behavior\n\n| Aspect | Unit Testing (TDD) | Behavior-Driven Development |\n|--------|-------------------|----------------------------|\n| **Primary Focus** | Correctness of code at unit level | System behavior from user perspective |\n| **Language** | Code-based (Python, Java, etc.) | Natural language ([Gherkin](/concepts/gherkin)) |\n| **Stakeholders** | Developers | Developers, QA, Business Analysts, POs |\n| **Signal** | Pass/Fail on logic | Alignment with business objectives |\n| **Agent Role** | Minimal (code generation) | Central (agent interprets and executes behavior) |\n\n### The Three Roles in BDD\n\nBDD emphasizes collaboration between three perspectives:\n\n1. **Business** — Defines the \"what\" and \"why\" (business value, user outcomes)\n2. **Development** — Defines the \"how\" (implementation approach)\n3. **Quality** — Defines the \"proof\" (verification criteria)\n\nIn agentic development, the AI agent often handles Development while Business and Quality remain human-defined. BDD provides the structured handoff format.\n\n### BDD in the Probabilistic Era\n\nTraditional BDD was designed for deterministic systems: given specific inputs, expect specific outputs. Agentic systems are probabilistic—LLM outputs vary based on context, temperature, and emergent behavior.\n\nBDD adapts to this by:\n- Defining **behavioral contracts** rather than implementation details\n- Allowing agents to determine *how* to achieve specified behavior\n- Providing **semantic anchors** that constrain the reasoning space without over-specifying\n\n## ASDLC Usage\n\nBDD's value in agentic development is **semantic anchoring**. When an agent is given a Gherkin scenario, it receives a \"specification of behavior\" that:\n\n- Partitions the reasoning space into manageable segments (Given/When/Then)\n- Defines success criteria without over-specifying implementation\n- Aligns technical execution with business intent\n\nThis is why BDD scenarios belong in Specs, not just test suites. They're not just verification artifacts—they're **functional blueprints** that guide agent reasoning.\n\n**Implementation via the Spec Pattern:**\n\n| BDD Component | Spec Implementation |\n|---------------|---------------------|\n| Feature description | Spec Context section |\n| Business rules | Blueprint constraints |\n| Acceptance scenarios | Contract section ([Gherkin](/concepts/gherkin) scenarios) |\n\n**Applied in:**\n- [The Spec](/patterns/the-spec) — Implements BDD through Blueprint (constraints) and Contract (scenarios)\n- [Context Gates](/patterns/context-gates) — BDD scenarios define verification criteria at gates",
    "tags": ["Testing", "Specification", "Agile", "Requirements"],
    "references": [
      {
        "type": "website",
        "title": "Introducing BDD",
        "url": "https://dannorth.net/introducing-bdd/",
        "author": "Dan North",
        "published": "2006-03-01T00:00:00.000Z",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Original formulation of Behavior-Driven Development by its creator."
      },
      {
        "type": "website",
        "title": "Behavior Driven Development",
        "url": "https://www.agilealliance.org/glossary/bdd/",
        "author": "Agile Alliance",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Industry-standard glossary definition from the Agile Alliance."
      }
    ]
  },
  {
    "slug": "context-engineering",
    "collection": "concepts",
    "title": "Context Engineering",
    "description": "Context Engineering is the practice of structuring information to optimize LLM comprehension and output quality.",
    "status": "Live",
    "content": "## Definition\n\nContext Engineering is the systematic approach to designing and structuring the input context provided to Large Language Models (LLMs) to maximize their effectiveness, accuracy, and reliability in generating outputs.\n\nThe practice emerged from the recognition that LLMs operate on explicit information only—they cannot intuit missing business logic or infer unstated constraints. Context Engineering addresses this by making tacit knowledge explicit, machine-readable, and version-controlled.\n\nWhile ASDLC focuses on software development, Context Engineering is domain-agnostic:\n* **In Design:** Design system tokens and Figma layer naming conventions fed to UI agents\n* **In Law:** Briefs restricting paralegal agents to specific case law precedents  \n* **In SDLC:** The `AGENTS.md` file steering agents toward implementation patterns\n\nAnywhere agents operate, context is the constraint that turns raw intelligence into specific value.\n\nMartin Fowler observes: \"As I listen to people who are serious with AI-assisted programming, the crucial thing I hear is managing context.\"\n\nAnthropic's research confirms this. Engineers cite the **cold start problem** as the biggest blocker:\n\n> \"There is a lot of intrinsic information that I just have about how my team's code base works that Claude will not have by default… I could spend time trying to iterate on the perfect prompt [but] I'm just going to go and do it myself.\"\n\nContext Engineering solves cold start by making tacit knowledge explicit, machine-readable, and version-controlled so agents can act on it without prompt iteration.\n\n## Key Characteristics\n\n**The Requirements Gap**\n\n\"Prompt Engineering\" is often a misnomer. It is simply **Requirements Engineering** applied to a non-human entity that cannot intuit missing business logic. Human developers ask clarifying questions when requirements are vague (\"What happens if the payment fails?\"). AI models build something based on probability. Errors generally surface only when the system breaks in production.\n\n**Screaming Architecture**\n\nContext Engineering extends to the filesystem itself. As [Raf Lefever](/concepts/context-engineering#references) notes, \"If your code-base doesn't scream its domain, AI will whisper nonsense.\"\n\nA well-structured filesystem (e.g., `src/features/checkout/core-logic`) provides implicit context to the LLM about intent and boundaries. A generic filesystem (`src/utils`, `src/managers`) forces the LLM to guess. In ASDLC, we optimize directory structures to be \"training wheels\" for the agent.\n\n**Core Attributes**\n\n1.  **Version Controlled:** Context exists as a software asset that lives in the repo, is diffed in PRs, and is subject to peer review.\n2.  **Standardized:** Formatted to be readable by any agent (Cursor, Windsurf, Devin, GitHub Copilot).\n3.  **Iterative:** Continuously refined based on agent failure modes and tacit information discovered by Human-in-the-loop (HITL) workflows.\n4.  **Schema-First:** Data structures defined before requesting content generation to ensure type safety and validation.\n5.  **Hierarchical:** Information organized by importance—critical instructions first, references second, examples last.\n\n## ASDLC Usage\n\nIn ASDLC, context is treated as version-controlled code, not ephemeral prompts.\n\n**Context vs Guardrails:**\n\nA distinction exists between [Guardrails](/concepts/guardrails) (Safety) and Context (Utility). Currently, many `AGENTS.md` files contain defensive instructions like \"Do not delete files outside this directory\" or \"Do not output raw secrets.\" This is likely a transitional state. OpenAI, Anthropic, Google, and platform wrappers are racing to bake these safety constraints directly into the inference layer. Soon, telling an agent \"Don't leak API keys\" will be as redundant as telling a compiler \"Optimize for speed.\"\n\n**Relationship to Patterns:**\n\n- **[Specs](/patterns/the-spec)** — Specs are context engineering in document form. The Blueprint and Contract sections are structured context optimized for agent consumption.\n- **[Context Gates](/patterns/context-gates)** — Checkpoints where context is validated, injected, or filtered before agent action.\n- **[OODA Loop](/concepts/ooda-loop)** — Context Engineering is *engineering the Orient phase*. The quality of agent decisions depends on context quality.\n- **[4D Framework](/concepts/4d-framework)** — The \"Description\" dimension maps directly to Context Engineering: transferring goals, constraints, and data structures to the agent.\n\n**Applied in:**\n- [AGENTS.md Specification](/practices/agents-md-spec) — The practical application of context engineering in repositories.\n- [Model Context Protocol](/concepts/model-context-protocol) — The standard for serving context to agents.\n\n> [!NOTE]\n> **Research Validation (InfiAgent, 2026):** File-centric state management outperforms compressed long-context prompts. Replacing persistent file state with accumulated conversation history dropped task completion from 80/80 to 27.7/80 average, even with Claude 4.5 Sonnet. This validates treating context as a reconstructed view of authoritative file state, not as conversation memory.",
    "tags": ["AI", "LLM", "Prompt Engineering", "Context Engineering"],
    "references": [
      {
        "type": "website",
        "title": "OpenAI Best Practices for Prompt Engineering",
        "url": "https://platform.openai.com/docs/guides/prompt-engineering",
        "author": "OpenAI",
        "published": "2024-01-15T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Foundational guidance on structuring prompts and context for optimal LLM performance."
      },
      {
        "type": "website",
        "title": "Constitutional AI Documentation",
        "url": "https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback",
        "author": "Anthropic",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Documentation on Anthropic's approach to AI alignment and context-based safety constraints."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Research identifying the cold start problem as the primary blocker in AI-assisted development."
      },
      {
        "type": "paper",
        "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
        "url": "https://arxiv.org/abs/2601.03204",
        "author": "Chenglin Yu, Yuchen Wang, Songmiao Wang, Hongxia Yang, Ming Li",
        "published": "2026-01-06T00:00:00.000Z",
        "accessed": "2026-01-10T00:00:00.000Z",
        "annotation": "Empirical validation of file-centric state management for long-horizon agent stability."
      },
      {
        "type": "website",
        "title": "Agile in the AI Era: Why 'Boring' Architecture Is Your Secret Weapon",
        "url": "https://www.linkedin.com/pulse/agile-ai-era-why-boring-architecture-your-secret-weapon-lefever-ravne/",
        "author": "Raf Lefever",
        "published": "2026-02-10T00:00:00.000Z",
        "accessed": "2026-02-12T00:00:00.000Z",
        "annotation": "Argues that domain-driven structure ('Screaming Architecture') is critical context for AI agents."
      }
    ]
  },
  {
    "slug": "event-modeling",
    "collection": "concepts",
    "title": "Event Modeling",
    "description": "A system blueprinting method that centers on events as the primary source of truth, serving as a rigorous bridge between visual design and technical implementation.",
    "status": "Experimental",
    "content": "## Definition\n\n**Event Modeling** is a method for designing information systems by mapping what happens over time. It creates a linear blueprint that serves as the single source of truth for Product, Design, and Engineering.\n\nUnlike static diagrams (like ERDs or UML) that focus on structure, Event Modeling focuses on the **narrative of the system**. It visualizes the system as a film strip, showing exactly how a user’s action impacts the system state and what information is displayed back to them.\n\n### Core Components\nAn Event Model is composed of four distinct elements:\n\n* **Commands (Blue)**: The intent or action initiated by the user (e.g., \"Submit Order\").\n* **Events (Orange)**: A fact recorded by the system that cannot be changed (e.g., \"OrderPlaced\"). This is the single source of truth.\n* **Views (Green)**: Information displayed to the user, derived from previous events (e.g., \"Order Confirmation Screen\").\n* **Processes**: The logic or automation that reacts to events to trigger other commands or update views.\n\n## Why It Matters for AI\n\nIn modern software development, ambiguity is the enemy. While human engineers can infer intent from a loose visual mockup, AI models require explicit instructions.\n\nEvent Modeling forces implicit business rules to become explicit. By defining the exact data payload of every *Command* and the resulting state change of every *Event*, we provide AI agents with a deterministic roadmap. This ensures the generated code handles edge cases and data consistency correctly, rather than just \"looking right\" on the frontend.\n\n## Relationship to Requirements\n\nEvent Modeling acts as a bridge between **Visual Design** (what it looks like) and **Technical Architecture** (how it works).\n\nIt does not replace functional requirements; rather, it validates them. A feature is only considered \"defined\" when there is a complete path mapped from the user's view, through the command, to the stored event, and back to the view. This \"closed loop\" guarantees that every pixel on the screen is backed by real data.",
    "tags": ["Architecture", "Requirements", "Standards"],
    "references": [
      {
        "type": "website",
        "title": "EventModeling.org",
        "url": "https://eventmodeling.org/",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "The official home of the Event Modeling methodology, defining the complete framework for event-centric system design."
      }
    ]
  },
  {
    "slug": "feedback-loop-compression",
    "collection": "concepts",
    "title": "Feedback Loop Compression",
    "description": "How AI compresses the observe → validate → learn cycle, shifting the bottleneck from code production to code understanding.",
    "status": "Experimental",
    "content": "## Definition\n\n**Feedback Loop Compression** is the phenomenon where AI collapses the time between deploying code and understanding its production behavior. For 20 years, DevOps attempted to connect developers with production consequences but failed—the loops were \"long, lossy, and laggy.\" AI has changed this.\n\n> \"The bottleneck shifts from, 'How fast can I write code?' to, 'How fast can I understand what's happening and make good decisions about it?'\"\n> — Charity Majors\n\nThe compression is asymmetric: AI has made the *Act* phase (code generation) nearly free, while the *Orient* phase (understanding production state) remains the constraint. Feedback Loop Compression addresses this by making observation and validation as fast as generation.\n\n## The Shift in Constraints\n\n| Era | Primary Bottleneck | Secondary Bottleneck |\n|-----|-------------------|----------------------|\n| Pre-DevOps | Deployment (ops owns production) | Feedback (weeks-to-months) |\n| DevOps Era | Feedback loops (still too slow) | Code production |\n| AI Era | **Understanding & validation** | Code production → near-zero |\n\nThe traditional workflow optimized for the *wrong* constraint:\n\n```text\nOld: write code → test → review → merge → \"hope it works!\"\nNew: write code (AI) → deploy → observe → validate → learn → iterate\n```\n\nIn the new model, every deploy is a learning opportunity. Shipping frequency becomes the heartbeat of feedback.\n\n## OODA Acceleration\n\nFeedback Loop Compression is specifically about accelerating the [OODA Loop](/concepts/ooda-loop):\n\n| Phase | Before AI | After AI |\n|-------|-----------|----------|\n| **Observe** | Ops tools, dashboards, manual inspection | Automated telemetry streaming to dev context |\n| **Orient** | Domain expertise, manual triage | AI interprets traces, suggests root causes |\n| **Decide** | Developer reasoning about fix | AI proposes solutions with verification plans |\n| **Act** | Manual code changes | AI-generated patches, validated before merge |\n\nThe key insight: AI doesn't just accelerate *Act*—it accelerates the entire cycle. An agent can observe production logs, orient against the codebase, decide on a fix, and act to implement it, all within a single interaction loop.\n\n## Implications for L3 Autonomy\n\nAt [L3 (Conditional Autonomy)](/concepts/levels-of-autonomy), humans remain in the loop for judgment calls. Feedback Loop Compression doesn't eliminate this—it makes each human decision more informed:\n\n- **Faster observation** → Humans see production state sooner\n- **Better orientation** → AI surfaces relevant context\n- **Clearer decisions** → Proposals come with validation evidence\n- **Verified actions** → Human approves after seeing proof-of-correctness\n\nThe compressed loop doesn't bypass human oversight; it gives humans better information faster.\n\n## The \"Nobody Understands It\" Risk\n\n> \"What happens when nobody wrote the code you just deployed, and nobody really understands it?\"\n> — Charity Majors\n\nThis is the dark side of compressed feedback loops. AI-generated code deployed at AI speed can outpace human understanding. ASDLC addresses this through:\n\n- **[Specs](/patterns/the-spec)** — Persist intent for future agents (and humans)\n- **[Living Specs](/practices/living-specs)** — Crystallize learnings as they emerge\n- **[Context Gates](/patterns/context-gates)** — Force understanding checkpoints before deployment\n- **[Constitutional Review](/practices/constitutional-review-implementation)** — Validate code against values, not just correctness\n\nCompressed loops without crystallized understanding lead to accumulated technical debt at AI speed.\n\n## ASDLC Usage\n\n| Compression Enabler | ASDLC Response |\n|---------------------|----------------|\n| Code generation → free | Focus shifts to [Spec-Driven Development](/concepts/spec-driven-development) |\n| Observation → automated | [Ralph Loop](/patterns/ralph-loop) reads logs, test output automatically |\n| Orientation → AI-assisted | [Context Engineering](/concepts/context-engineering) structures what AI sees |\n| Validation → continuous | [Context Gates](/patterns/context-gates) enforce verification |\n\nApplied in:\n- [Learning Loop](/concepts/learning-loop) — The crystallize phase preserves compressed learnings\n- [Production Readiness Gap](/concepts/production-readiness-gap) — Observability as production requirement\n- [OODA Loop](/concepts/ooda-loop) — The cognitive model being compressed\n\n## Anti-Patterns\n\n| Anti-Pattern | Description |\n|--------------|-------------|\n| **Shipping Blind** | Compressing the Act phase without compressing Observe—deploying code without telemetry |\n| **Speed Over Understanding** | Deploying faster than the team can comprehend; accumulated mystery code |\n| **Observation Without Orientation** | Collecting telemetry without structuring it for AI comprehension |\n| **Lossy Loops** | Fast cycles that don't preserve learnings; next session rediscovers same constraints |",
    "tags": ["AI", "Observability", "Production", "OODA"],
    "references": [
      {
        "type": "website",
        "title": "\"You Had One Job\": Why Twenty Years of DevOps Has Failed to Do it",
        "url": "https://www.honeycomb.io/blog/you-had-one-job-why-twenty-years-of-devops-has-failed-to-do-it",
        "author": "Charity Majors",
        "published": "2026-01-22T00:00:00.000Z",
        "accessed": "2026-01-26T00:00:00.000Z",
        "annotation": "Primary source defining the compression of feedback loops as DevOps' unfulfilled promise, now achievable through AI."
      }
    ]
  },
  {
    "slug": "gherkin",
    "collection": "concepts",
    "title": "Gherkin",
    "description": "A structured, domain-specific language using Given-When-Then syntax to define behavioral specifications that are both human-readable and machine-actionable.",
    "status": "Live",
    "content": "## Definition\n\nGherkin is a structured, domain-specific language using Given-When-Then syntax to define behavioral specifications in plain text. While [Behavior-Driven Development](/concepts/behavior-driven-development) provides the methodology, Gherkin provides the concrete syntax.\n\nGherkin's effectiveness for LLM agents stems from its properties: human-readable without technical jargon, machine-parseable with predictable structure, and aligned between technical and non-technical stakeholders. Each keyword defines a phase of reasoning that prevents agents from conflating setup, action, and verification into an undifferentiated blob.\n\n## The Given-When-Then Structure\n\nGherkin scenarios follow a consistent three-part structure:\n\n```gherkin\nFeature: User Authentication\n  As a registered user\n  I want to log into the system\n  So that I can access my personalized dashboard\n\n  Scenario: Successful login with valid credentials\n    Given a registered user with email \"user@example.com\"\n    And the user has password \"SecurePass123\"\n    When the user submits login credentials\n    Then the user should be redirected to the dashboard\n    And a session token should be created\n```\n\n## Keyword Semantics\n\n| Keyword | Traditional BDD | Agentic Translation |\n|---------|-----------------|---------------------|\n| **Given** | Preconditions or initial state | Context setting, memory retrieval, environment setup |\n| **When** | The trigger event or user action | Task execution, tool invocation, decision step |\n| **Then** | The observable outcome | Verification criteria, alignment check, evidence-of-done |\n| **And/But** | Additional conditions within a step | Logical constraints, secondary validation parameters |\n| **Feature** | High-level description of functionality | Functional blueprint, overall agentic goal |\n| **Background** | Steps common to all scenarios | Pre-test fixtures, global environment variables |\n\n## ASDLC Usage\n\nGherkin isn't just a testing syntax—it's a **semantic constraint language** for agent behavior.\n\nWhen an agent reads a Gherkin scenario:\n- **Given** tells it what to assume (context setup)\n- **When** tells it what action to take (execution scope)\n- **Then** tells it what success looks like (verification criteria)\n\nThis partitioning prevents \"context bleed\" where agents conflate setup, action, and verification.\n\n**In Specs:** The [Spec](/patterns/the-spec) Contract section uses Gherkin scenarios:\n\n```markdown\n## Contract\n\n### Scenarios\n\n#### Happy Path\nGiven a valid API key\nWhen the user requests /api/notifications\nThen the response returns within 100ms\nAnd the payload contains the user's notifications\n```\n\n**Applied in:**\n- [The Spec](/patterns/the-spec) — Contract section uses Gherkin scenarios\n- [Context Gates](/patterns/context-gates) — Gherkin scenarios define gate verification criteria\n- [Living Specs](/practices/living-specs) — Gherkin scenarios evolve with the feature",
    "tags": ["Testing", "Specification", "BDD", "Syntax"],
    "references": [
      {
        "type": "website",
        "title": "Gherkin Reference",
        "url": "https://cucumber.io/docs/gherkin/reference/",
        "author": "Cucumber",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Canonical documentation for Gherkin syntax and semantics."
      },
      {
        "type": "website",
        "title": "Behavior Driven Development",
        "url": "https://www.agilealliance.org/glossary/bdd/",
        "author": "Agile Alliance",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Industry-standard glossary covering BDD and Gherkin terminology."
      }
    ]
  },
  {
    "slug": "learning-loop",
    "collection": "concepts",
    "title": "The Learning Loop",
    "description": "The iterative cycle between exploratory implementation and spec refinement, balancing vibe coding velocity with captured learnings.",
    "status": "Live",
    "content": "## Definition\n\nThe Learning Loop is the iterative cycle between exploratory implementation and constraint crystallization. It acknowledges that understanding emerges through building, while ensuring that understanding is captured for future agent sessions.\n\nKent Beck critiques spec-driven approaches that assume \"you aren't going to learn anything during implementation.\" He's right—discovery is essential. But pure vibe coding loses those discoveries. The next agent session starts from zero, re-discovering (or missing) the same constraints.\n\nThe Learning Loop preserves discoveries as machine-readable context, enabling compounding understanding across sessions.\n\n## The Cycle\n\n1. **Explore** — Vibe code to discover edge cases, performance characteristics, or API behaviors\n2. **Learn** — Identify constraints that weren't obvious from requirements\n3. **Crystallize** — Update the Spec with discovered constraints\n4. **Verify** — Gate future implementations against the updated Spec\n5. **Repeat**\n\nEach iteration builds on the last. The spec grows smarter, and agents inherit the learnings of every previous session.\n\n## OODA Foundation\n\nThe Learning Loop is an application of the [OODA Loop](/concepts/ooda-loop) to software development:\n\n| Learning Loop Phase | OODA Equivalent |\n|---------------------|------------------|\n| **Explore** | Observe + Act (gather information through building) |\n| **Learn** | Orient (interpret what was discovered) |\n| **Crystallize** | Decide (commit learnings to persistent format) |\n| **Verify** | Observe (confirm crystallized constraints via gates) |\n\nThe key insight: in software development, **Orient and Observe are interleaved**. You often can't observe relevant constraints until you've built something that reveals them. The Learning Loop makes this explicit by treating Explore as a legitimate phase rather than a deviation from the plan.\n\n## Key Characteristics\n\n### Not Waterfall\n\nThe Learning Loop explicitly rejects the waterfall assumption that all constraints can be known upfront. Specs are scaffolding that evolve, not stone tablets.\n\n### Not Pure Vibe Coding\n\nThe Learning Loop also rejects the vibe coding assumption that documentation is optional. Undocumented learnings are lost learnings—the next agent (or human) will repeat the same mistakes.\n\n### Machine-Readable Capture\n\nLearnings must be captured in formats agents can consume: schemas, constraints in YAML, acceptance criteria in markdown. Natural language is acceptable but structured data is preferred.\n\n> \"The real capability—our ability to respond to change—comes not from how fast we can produce code, but from how deeply we understand the system we are shaping.\"\n> — Unmesh Joshi\n\n## Automation: The Ralph Loop\n\nThe Learning Loop describes an iterative cycle that typically involves human judgment at each phase. The [Ralph Loop](/patterns/ralph-loop) automates this cycle for tasks with machine-verifiable completion criteria:\n\n| Learning Loop Phase | Ralph Loop Implementation |\n|---------------------|---------------------------|\n| **Explore** | Agent implements based on PBI/Spec |\n| **Learn** | Agent reads error logs, test failures, build output |\n| **Crystallize** | Agent updates progress.txt; commits to Git |\n| **Verify** | External tools (Jest, tsc, Docker) confirm success |\n\nWhen verification fails, Ralph automatically re-enters Explore with the learned context. The loop continues until external verification passes or iteration limit is reached.\n\n**Key difference:** The Learning Loop expects human judgment in the Learn and Crystallize phases. The Ralph Loop requires that \"learning\" be expressible as observable state (error logs, test results) and \"crystallization\" be automatic (Git commits, progress files).\n\nRalph Loops work best when success criteria are machine-verifiable (tests pass, builds complete). For tasks requiring human judgment—ambiguous requirements, architectural decisions, product direction—the Learning Loop remains the appropriate model.\n\n## ASDLC Usage\n\nIn ASDLC, the Learning Loop connects several core concepts:\n\n- **OODA Loop** — The foundational cognitive model the Learning Loop implements\n- **Vibe Coding** is the Explore phase (valid for prototyping and discovery)\n- **Living Specs** capture the Crystallize phase\n- **Context Gates** enforce the Verify phase\n- **Ralph Loop** — Automated implementation for machine-verifiable tasks\n- **PBIs** trigger iteration through the loop\n\nApplied in:\n- [OODA Loop](/concepts/ooda-loop) — The cognitive model foundation\n- [Spec-Driven Development](/concepts/spec-driven-development) — Iterative refinement of specs\n- [Living Specs](/practices/living-specs) — Maintenance of captured learnings\n- [Context Gates](/patterns/context-gates) — Verification checkpoints\n- [Ralph Loop](/patterns/ralph-loop) — Automated terminal implementation\n\n## Anti-Patterns\n\n| Anti-Pattern | Description |\n|--------------|-------------|\n| **Waterfall Specs** | Writing exhaustive specs before any implementation, assuming no learning will occur |\n| **Ephemeral Vibe Coding** | Generating code without ever crystallizing learnings into specs |\n| **Spec-as-Paperwork** | Updating specs for compliance rather than genuine constraint capture |\n| **Post-Hoc Documentation** | Writing specs after implementation is complete, losing the iterative benefit |",
    "tags": ["Core", "Methodology", "Learning"],
    "references": [
      {
        "type": "website",
        "title": "Martin Fowler Fragment: January 8, 2026",
        "url": "https://martinfowler.com/fragments/2026-01-08.html",
        "author": "Martin Fowler",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Commentary on the tension between specification and learning during implementation."
      },
      {
        "type": "website",
        "title": "Kent Beck on Spec-Driven Development",
        "url": "https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/",
        "author": "Kent Beck",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Critique emphasizing that specifications must accommodate learning during implementation."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Internal study showing how engineers balance AI velocity with understanding."
      },
      {
        "type": "website",
        "title": "\"You Had One Job\": Why Twenty Years of DevOps Has Failed to Do it",
        "url": "https://www.honeycomb.io/blog/you-had-one-job-why-twenty-years-of-devops-has-failed-to-do-it",
        "author": "Charity Majors",
        "published": "2026-01-22T00:00:00.000Z",
        "accessed": "2026-01-26T00:00:00.000Z",
        "annotation": "Industry validation of the Learning Loop—proposes identical 'deploy → observe → validate → learn → iterate' cycle as the AI-enabled future of software development."
      }
    ]
  },
  {
    "slug": "levels-of-autonomy",
    "collection": "concepts",
    "title": "Levels of Autonomy",
    "description": "SAE-inspired taxonomy for AI agent autonomy in software development, from L1 (assistive) to L5 (full), standardized at L3 conditional autonomy.",
    "status": "Live",
    "content": "## Definition\n\nThe **Levels of Autonomy** scale categorizes AI systems based on their operational independence in software development contexts. Inspired by the SAE J3016 automotive standard, it provides a shared vocabulary for discussing human oversight requirements.\n\nThe scale identifies where the **Context Gate** (the boundary of human oversight) must be placed for each level. Under this taxonomy, autonomy is not a measure of intelligence—it is a measure of operational risk and required human involvement.\n\n## The Scale\n\n| Level | Designation | Description | Human Role | Failure Mode |\n| :--- | :--- | :--- | :--- | :--- |\n| **L1** | Assistive | Autocomplete, Chatbots. Zero state retention. | Driver. Hands on wheel 100% of time. | Distraction / Minor Syntax Errors |\n| **L2** | Task-Based | \"Fix this function.\" Single-file context. | Reviewer. Checks output before commit. | Logic bugs within a single file. |\n| **L3** | Conditional | \"Implement this feature.\" Multi-file orchestration. | Instructor. Defines constraints & intervenes on \"drift.\" | Regression to the Mean (Mediocrity). |\n| **L4** | High | \"Manage this backlog.\" Self-directed planning. | Auditor. Post-hoc analysis. | Silent Failure. Strategic drift over time. |\n| **L5** | Full | \"Run this company.\" | Consumer. Passive beneficiary. | Existential alignment drift. |\n\n## Analogy: The Self-Driving Standard (SAE)\n\nThe software autonomy scale maps directly to SAE J3016, the automotive standard for autonomous vehicles. This clarifies \"Human-in-the-Loop\" requirements using familiar terminology.\n\n| ASDLC Level | SAE Equivalent | The \"Steering Wheel\" Metaphor |\n| :--- | :--- | :--- |\n| **L1** | L1 (Driver Assist) | **Hands On, Feet On.** AI nudges the wheel (Lane Keep) or gas (Cruise), but Human drives. |\n| **L2** | L2 (Partial) | **Hands On (mostly).** AI handles steering and speed in bursts, but Human monitors constantly. |\n| **L3** | L3 (Conditional) | **Hands Off, Eyes On.** AI executes the maneuver (The Drive). Human is the Instructor ready to grab the wheel immediately. |\n| **L4** | L4 (High) | **Mind Off.** Sleeping in the back seat within a geo-fenced area. Dangerous if the \"fence\" (Context) breaks. |\n| **L5** | L5 (Full) | **No Steering Wheel.** The vehicle has no manual controls. |\n\n## ASDLC Usage\n\nASDLC standardizes practices for **Level 3 (Conditional Autonomy)** in software engineering. While the industry frequently promotes L5 as the ultimate goal, this perspective is often counterproductive given current tooling maturity. L3 is established as the sensible default.\n\n> [!WARNING]\n> **Level 4 Autonomy Risks**\n> \n> At L4, agents operate for days without human intervention but lack the strategic foresight needed to maintain system integrity. This results in **Silent Drift**—the codebase continues to function technically but gradually deteriorates into an unmanageable state.\n> \n> Mitigation strategies exist (Advanced Context Gates, architectural health monitoring), but these solutions require further validation.\n\n> [!NOTE]\n> **Empirical Support for L3**\n> \n> Anthropic's 2025 internal study of 132 engineers validates L3 as the practical ceiling:\n> - Engineers fully delegate only **0-20%** of work\n> - Average **4.1 human turns** per Claude Code session\n> - High-level design and \"taste\" decisions remain **exclusively human-owned**\n> - The \"paradox of supervision\"—effective oversight requires skills that AI use may atrophy\n\nApplied in:\n- [Context Gates](/patterns/context-gates) — The mechanism enabling safe L3 operation\n- [Guardrails](/concepts/guardrails) — Safety constraints for agent behavior\n- [Agentic SDLC](/concepts/agentic-sdlc) — The broader methodology context",
    "tags": ["Taxonomy", "Standards"],
    "references": [
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Research showing 0-20% full delegation, 4.1 human turns per session, and exclusively human-owned high-level design decisions."
      },
      {
        "type": "website",
        "title": "Intent Engineering Framework for AI Agents",
        "url": "https://www.productcompass.pm/p/intent-engineering-framework-for-ai-agents",
        "author": "Paweł Huryn",
        "published": "2024-04-30T00:00:00.000Z",
        "accessed": "2026-01-19T00:00:00.000Z",
        "annotation": "Validates L2 as 'Guarded Autonomy' and L3 as 'Proposal-First' autonomy."
      }
    ]
  },
  {
    "slug": "mermaid",
    "collection": "concepts",
    "title": "Mermaid",
    "description": "A text-based diagramming language that renders flowcharts, sequences, and architectures from markdown, enabling version-controlled visual specifications.",
    "status": "Live",
    "content": "## Definition\n\nMermaid is a text-based diagramming language that renders flowcharts, sequence diagrams, and architecture visualizations from markdown-style code blocks. In agentic development, Mermaid serves as the specification language for processes, workflows, and system relationships.\n\nWhere [Gherkin](/concepts/gherkin) specifies *behavior* and [YAML](/concepts/yaml) specifies *structure*, Mermaid specifies *process*—how components interact, how data flows, and how state transitions occur.\n\n## Key Characteristics\n\n### Text-Based Diagrams\n\nMermaid diagrams are defined in plain text, making them:\n- **Version-controllable** — Diagram changes appear in diffs\n- **Reviewable** — Same PR process as code\n- **Agent-parseable** — LLMs can read and modify diagrams\n\n```mermaid\nflowchart LR\n    A[Input] --> B[Process]\n    B --> C[Output]\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/mermaid-fig-1.svg\" alt=\"Mermaid Diagram\" />\n  \n</figure>\n\n### Diagram Types\n\n| Type | Use Case | ASDLC Application |\n|------|----------|-------------------|\n| **Flowchart** | Process flows, decision trees | Feature Assembly, Context Gates |\n| **Sequence** | API interactions, message flows | Service contracts, Integration specs |\n| **State** | State machines, lifecycle | Component state, Workflow phases |\n| **Class** | Object relationships | Domain models, Architecture |\n| **ER** | Entity relationships | Data models, Schema design |\n| **Gantt** | Timeline, scheduling | Roadmaps, Sprint planning |\n\n### Subgraphs for Grouping\n\nSubgraphs partition complex diagrams into logical regions:\n\n```mermaid\nflowchart LR\n    subgraph Input\n        A[Source]\n    end\n    \n    subgraph Processing\n        B[Transform]\n        C[Validate]\n        B --> C\n    end\n    \n    A --> B\n    C --> D[Output]\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/mermaid-fig-2.svg\" alt=\"Mermaid Diagram\" />\n  \n</figure>\n\n## ASDLC Usage\n\nMermaid serves as the **process specification language** in ASDLC, completing the specification triad:\n\n| Language | Specifies | Example |\n|----------|-----------|---------|\n| **[Gherkin](/concepts/gherkin)** | Behavior | Given/When/Then scenarios |\n| **[YAML](/concepts/yaml)** | Structure | Schemas, configuration |\n| **Mermaid** | Process | Flowcharts, sequences |\n\n**Why Mermaid for Specs:**\n\nText-based diagrams solve a critical problem in agentic development: visual documentation that agents can read, modify, and version-control. Unlike image-based diagrams that become stale context, Mermaid diagrams are:\n\n- **Agent-modifiable** — LLMs can update flows as requirements change\n- **Diffable** — Changes appear in code review alongside logic changes\n- **Living** — Part of the spec, not a separate artifact that drifts\n\n**Relationship to Patterns:**\n\n- **[The Spec](/patterns/the-spec)** — Specs embed Mermaid to visualize feature architecture and state flows\n- **[Context Engineering](/concepts/context-engineering)** — Diagrams as structured, machine-readable context\n\n## Anti-Patterns\n\n| Anti-Pattern | Description |\n|--------------|-------------|\n| **Box Soup** | Too many nodes without grouping |\n| **Arrow Spaghetti** | Excessive cross-connections |\n| **No Labels** | Edges without descriptive text |\n| **Static Screenshots** | Images instead of text diagrams |\n\n> [!TIP]\n> **Key practices:** Group with subgraphs, label edges, use `flowchart LR` for process flows, limit to <15 nodes per diagram.",
    "tags": ["Diagrams", "Visualization", "Specification", "Documentation"],
    "references": [
      {
        "type": "website",
        "title": "Mermaid Documentation",
        "url": "https://mermaid.js.org/intro/",
        "author": "Mermaid",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Official documentation for Mermaid diagramming syntax and features."
      }
    ]
  },
  {
    "slug": "ooda-loop",
    "collection": "concepts",
    "title": "OODA Loop",
    "description": "The Observe-Orient-Decide-Act decision cycle—a strategic model from military combat adapted for autonomous agent behavior in software development.",
    "status": "Live",
    "content": "## Definition\n\nThe OODA Loop—Observe, Orient, Decide, Act—is a strategic decision-making cycle originally developed by U.S. Air Force Colonel John Boyd for aerial combat. Boyd's insight: the combatant who cycles through these phases faster than their opponent gains decisive advantage. The key isn't raw speed—it's **tempo relative to environmental change**.\n\nBoyd's less-quoted but crucial insight: **Orient is everything**. The Orient phase is where mental models, context, and prior experience shape how observations become decisions. A faster but poorly-oriented loop loses to a slower but well-oriented one.\n\nIn agentic software development, OODA provides the cognitive model for how autonomous agents should behave: continuously cycling through observation, interpretation, planning, and execution.\n\n## The Four Phases\n\n1. **Observe** — Gather information about the current state of the environment\n2. **Orient** — Interpret observations through mental models, context, and constraints\n3. **Decide** — Formulate a specific plan for action based on orientation\n4. **Act** — Execute the plan, producing changes that feed new observations\n\nThe loop is continuous. Each Act produces new state, triggering new Observe, and the cycle repeats.\n\n## Key Characteristics\n\n### Tempo, Not Raw Speed\n\nThe strategic value of OODA isn't speed—it's cycling faster than the environment changes. In software development, the \"environment\" is the codebase, requirements, and constraints. An agent that can cycle through OODA before context rot sets in converges on correct solutions.\n\n### Orient as the Critical Phase\n\nFor AI agents, Orient is the **context window**. The quality of orientation depends on:\n\n- **Spec Clarity** — Garbage spec → garbage orientation\n- **Constitution Directives** — Values that shape interpretation\n- **Context Gates** — Filtering noise so orientation isn't polluted\n- **Prior State** — Git history, progress files, previous learnings\n\nThis is why [Context Engineering](/concepts/context-engineering) isn't optional overhead. It's engineering the Orient phase, which determines whether fast cycling produces progress or noise.\n\n### OODA vs. Single-Shot Interactions\n\nStandard LLM interactions are **Observe-Act**: user provides input, model produces output. No explicit Orient or Decide phase. The model's \"orientation\" is implicit in training and whatever context happens to be present.\n\nAgentic workflows make OODA explicit:\n\n| Phase | Single-Shot LLM | Agentic Workflow |\n|-------|-----------------|------------------|\n| **Observe** | User prompt | Instrumented: read files, run tests, check logs |\n| **Orient** | Implicit (training + context) | Engineered: Specs, Constitution, Context Gates |\n| **Decide** | Implicit | Explicit: agent states plan before acting |\n| **Act** | Generate response | Verified: external tools confirm success/failure |\n\nThis explicit structure enables debugging. When an agent fails, you can diagnose *which phase* broke down:\n\n- **Bad Observe?** Agent missed relevant information\n- **Bad Orient?** Context was polluted or incomplete\n- **Bad Decide?** Plan was incoherent given good orientation\n- **Bad Act?** Execution failed despite good plan\n\n## ASDLC Usage\n\nIn ASDLC, OODA explains why cyclic workflows outperform linear pipelines:\n\n| OODA Phase | Agent Behavior | ASDLC Component |\n|------------|----------------|-----------------|\n| **Observe** | Read codebase state, error logs, test results | File state, test output |\n| **Orient** | Interpret against context and constraints | [Context Gates](/patterns/context-gates), [AGENTS.md](/practices/agents-md-spec) |\n| **Decide** | Formulate implementation plan | PBI decomposition |\n| **Act** | Write code, run tests, commit | Micro-commits |\n\nThe [Learning Loop](/concepts/learning-loop) is OODA with an explicit \"Crystallize\" step that improves future Orient phases. Where OODA cycles continuously, Learning Loop captures discoveries into machine-readable context for subsequent agent sessions.\n\nApplied in:\n- [Context Engineering](/concepts/context-engineering) — The discipline of engineering the Orient phase\n- [Context Gates](/patterns/context-gates) — Checkpoints between OODA phases\n- [Levels of Autonomy](/concepts/levels-of-autonomy) — Higher autonomy requires more sophisticated Orient capabilities\n\n## Anti-Patterns\n\n| Anti-Pattern | Description | Failure Mode |\n|--------------|-------------|--------------|\n| **Observe-Act** | Skipping Orient/Decide. Classic vibe coding. | Works for simple tasks; fails at scale; no learning |\n| **Orient Paralysis** | Over-engineering context, never acting | Analysis paralysis; no forward progress |\n| **Stale Orient** | Not updating mental model when observations change | Context rot; agent operates on outdated assumptions |\n| **Observe Blindness** | Not instrumenting observation of relevant state | Agent misses critical information (failed tests, error logs) |\n| **Act Without Verify** | Not confirming action results before next cycle | Cascading errors; false confidence |",
    "tags": ["AI", "Agent Architecture", "Decision Making", "Military Strategy"],
    "references": [
      {
        "type": "book",
        "title": "Certain to Win: The Strategy of John Boyd, Applied to Business",
        "author": "Chet Richards",
        "isbn": "978-1413453775",
        "published": "2004-04-01T00:00:00.000Z",
        "annotation": "Accessible introduction to Boyd's OODA loop concepts applied beyond military contexts."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Research on tempo in human-AI collaboration; engineers who cycle faster with quality context converge on solutions."
      }
    ]
  },
  {
    "slug": "product-requirement-prompt",
    "collection": "concepts",
    "title": "Product Requirement Prompt (PRP)",
    "description": "A structured methodology combining PRD, codebase context, and agent runbook—the minimum spec for production-ready AI code.",
    "status": "Experimental",
    "content": "## Definition\n\nA **Product Requirement Prompt (PRP)** is a structured methodology that answers the question: *\"What's the minimum viable specification an AI coding agent needs to plausibly ship production-ready code in one pass?\"*\n\nAs creator Rasmus Widing defines it: **\"A PRP is PRD + curated codebase intelligence + agent runbook.\"**\n\nUnlike traditional PRDs (which exclude implementation details) or simple prompts (which lack structure), PRPs occupy the middle ground—a complete context packet that gives an agent everything it needs to execute autonomously within bounded scope.\n\nThe methodology emerged from practical engineering work in 2024 and has since become the foundation for agentic engineering training.\n\n## Key Characteristics\n\nPRPs are built on three core principles:\n\n1. **Plan before you prompt** — Structure thinking before invoking AI\n2. **Context is everything** — Comprehensive documentation enables quality output\n3. **Scope to what the model can reliably do in one pass** — Bounded execution units\n\nA complete PRP includes six components:\n\n| Component | Purpose |\n|-----------|---------|\n| **Goal** | What needs building |\n| **Why** | Business value and impact justification |\n| **Success Criteria** | **States** that indicate completion (not activities) |\n| **Health Metrics** | Non-regression constraints (what must *not* degrade) |\n| **Strategic Context** | Trade-offs & priorities (from **Product Vision**) |\n| **All Needed Context** | Documentation references, file paths, code snippets |\n| **Implementation Blueprint** | Task breakdown and pseudocode |\n| **Validation Loop** | Multi-level testing (syntax, unit, integration) |\n\n### Key Differentiators from Traditional PRDs\n\n- **Precise context:** Specific file paths, library versions, code examples\n- **Documentation integration:** Links to relevant library docs and architectural patterns\n- **Known gotchas:** Critical warnings about potential pitfalls\n- **Validation frameworks:** Executable tests the AI can run and fix iteratively\n\n## ASDLC Usage\n\nPRP components map directly to ASDLC concepts—a case of convergent evolution in agentic development practices.\n\n| PRP Component | ASDLC Equivalent |\n|---------------|------------------|\n| Goal | [The Spec](/patterns/the-spec) — Blueprint |\n| Why | [Product Thinking](/concepts/product-thinking) |\n| Success Criteria | [Context Gates](/patterns/context-gates) |\n| Health Metrics | [The Spec](/patterns/the-spec) — Non-Functional Reqs / Constraints |\n| Strategic Context | [Product Vision](/patterns/product-vision) — Runtime Injection |\n| All Needed Context | [Context Engineering](/concepts/context-engineering) |\n| Implementation Blueprint | [The PBI](/patterns/the-pbi) |\n| Validation Loop | [Context Gates](/patterns/context-gates) — Quality Gates |\n\nIn ASDLC terms, a PRP is equivalent to **The Spec + The PBI + curated Context Engineering**—bundled into a single artifact optimized for agent consumption.\n\nASDLC separates these concerns for reuse: multiple PBIs reference the same Spec, and context is curated per-task rather than duplicated. For simpler projects or rapid prototyping, the PRP's unified format may be more practical. The methodologies are complementary—PRPs can be thought of as \"collapsed ASDLC artifacts\" for single-pass execution.\n\nApplied in:\n- [Spec-Driven Development](/concepts/spec-driven-development) — The philosophy PRPs implement\n- [The Spec](/patterns/the-spec) — ASDLC's permanent specification pattern\n- [The PBI](/patterns/the-pbi) — ASDLC's transient execution unit\n\nSee also:\n- [Industry Alignment](/resources/industry-alignment) — Convergent frameworks in agentic development\n- [Spec-Driven Development](/concepts/spec-driven-development) — ASDLC's foundational methodology\n- [The Spec](/patterns/the-spec) — ASDLC's specification pattern\n- [Vibe Coding](/concepts/vibe-coding) — The anti-pattern both PRP and SDD address",
    "tags": ["Industry Term", "Spec-Driven Development", "Context Engineering"],
    "references": [
      {
        "type": "repository",
        "title": "PRPs: Agentic Engineering",
        "url": "https://github.com/Wirasm/PRPs-agentic-eng",
        "author": "Rasmus Widing",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Original methodology and templates for Product Requirement Prompts, defining the framework for AI-ready specifications."
      },
      {
        "type": "website",
        "title": "Rasmus Widing - LinkedIn Profile",
        "url": "https://www.linkedin.com/in/rasmuswiding/",
        "author": "Rasmus Widing",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Creator of PRP methodology, providing context on the convergent evolution with ASDLC principles."
      }
    ]
  },
  {
    "slug": "product-thinking",
    "collection": "concepts",
    "title": "Product Thinking",
    "description": "The practice of engineers thinking about user outcomes, business context, and the 'why' before the 'how'—the core human skill in the AI era.",
    "status": "Experimental",
    "content": "## Definition\n\nProduct Thinking is the practice of engineers understanding and prioritizing user outcomes, business context, and the reasoning behind technical work (\"why\") before focusing on implementation details (\"how\"). \n\nRather than waiting for fully-specified requirements and executing tasks mechanically, product-thinking engineers actively engage with the problem space. They ask:\n- What user problem does this solve?\n- Which tradeoffs are acceptable for this context?\n- How will this decision impact long-term maintainability?\n- Is this the right problem to solve at all?\n\nThis mindset originated in product management but has become essential for modern engineering teams, especially as AI increasingly handles implementation while humans must provide strategic judgment.\n\n## Key Characteristics\n\n**Outcome Orientation**\nProduct-thinking engineers measure success by user and business outcomes, not just task completion. They question whether closing a ticket actually moved the product forward.\n\n**Context Awareness**\nThey understand the broader system: user workflows, business constraints, competitive landscape, and technical debt landscape. Code decisions are made with this context, not in isolation.\n\n**Tradeoff Evaluation**\nEvery technical decision involves tradeoffs (speed vs maintainability, generality vs simplicity, build vs buy). Product-thinking engineers explicitly identify and evaluate these tradeoffs rather than defaulting to \"best practice.\"\n\n**Ownership Mindset**\nThey take responsibility for outcomes, not just implementations. If a feature ships but users don't adopt it, a product-thinking engineer investigates why, even if the code \"worked as specified.\"\n\n**Risk Recognition**\nThey can look at technically correct code and identify product risks: \"This will confuse users,\" \"This locks us into a vendor,\" \"This creates a support burden.\" These risks are invisible to AI.\n\n## The AI Era Shift\n\nMatt Watson (5x Founder/CTO, author of *Product Driven*) argues that **vibe coders outperform average engineers not because of superior coding skill, but because they think about the product**:\n\n> \"A lot of engineers? They're just waiting for requirements. That's usually a leadership problem. For years, we rewarded engineers for staying in their lane, closing tickets, and not rocking the boat. Then we act surprised when they don't think like owners.\"\n\n**The traditional model:**\n1. Product Manager writes requirements\n2. Engineer implements requirements\n3. Success = code matches spec\n\n**Why this fails in the AI era:**\n- AI can already handle \"just build this\" work faster than humans\n- The bottleneck shifts from implementation to **deciding what to build**\n- Engineers who only execute become redundant; those who evaluate and steer remain essential\n\n**The new competitive advantage:**\n- AI writes code; humans decide what matters\n- AI generates implementations; humans evaluate which tradeoffs are dangerous\n- AI follows instructions; humans recognize when \"the clean implementation is still the wrong product\"\n\nWatson's conclusion: **\"Product thinking isn't a bonus skill anymore. In an AI world, it's the job.\"**\n\n## The Leadership Problem\n\nProduct thinking doesn't emerge by accident. Watson identifies the structural cause:\n\n**Anti-patterns that kill product thinking:**\n- Engineers rewarded for \"staying in their lane\" instead of challenging requirements\n- Context withheld (\"you don't need to know the business reason, just build it\")\n- Decisions flowing top-down through a single bottleneck (PM or architect)\n- Success measured by velocity (story points closed) rather than outcomes (user problems solved)\n\n**What builds product thinking:**\n- Clearly explain **what** needs to be done and **why**\n- Give context instead of just tasks\n- Trust engineers to figure out the **how**\n- Train them to own outcomes, not just implementations\n\nIf every technical decision must flow through a product manager or architect, the organization has created a dependency on human bottlenecks that AI cannot solve.\n\n## Applications\n\n**Pre-AI Era:**\nProduct thinking was a differentiator for senior engineers and those in \"full-stack\" or startup environments. Most engineers could succeed by executing well-defined requirements.\n\n**AI Era:**\nProduct thinking becomes the baseline. As AI handles implementation, the human contribution shifts entirely to:\n1. Defining the problem worth solving\n2. Evaluating whether AI-generated solutions actually solve it\n3. Recognizing risks and tradeoffs the model cannot see\n\n**Where product thinking is essential:**\n- **Greenfield products:** No established patterns; every decision sets precedent\n- **Strategic refactoring:** Deciding which technical debt to address and why\n- **API design:** Tradeoffs between developer experience, performance, and flexibility\n- **Early-stage startups:** Speed-to-market vs maintainability requires constant judgment calls\n- **AI-assisted development:** Evaluating whether vibe-coded solutions are \"good enough\" or hiding risks\n\n## ASDLC Usage\n\nIn ASDLC, product thinking is **why Specs exist**. The Spec is not bureaucratic overhead—it's the forcing function that makes product thinking explicit and sharable.\n\n**The connection:**\n- **Product Thinking** = The human capability (understanding \"why\")\n- **The Spec** = The artifact that captures product thinking (machine-readable \"why\")\n- **Spec-Driven Development** = The workflow that ensures product thinking happens before code generation\n\nWhen an engineer writes a Spec, they're forced to answer:\n- What user problem does this solve?\n- What are the acceptance criteria?\n- Which edge cases matter and which don't?\n- What are the non-functional requirements (performance, security, observability)?\n\nIf they can't answer these questions, they don't understand the product problem yet. Vibe coding without this foundation produces code that works but solves the wrong problem.\n\n**The ASDLC position:**\n- AI agents execute maneuvers (implementation)\n- Human engineers provide strategic judgment (product thinking)\n- Specs encode that judgment in machine-readable form\n- Context Gates enforce that specs were actually written\n\nThis is the \"Instructor-in-the-Cockpit\" model: the pilot (AI) flies the plane, but the instructor (human) decides where to fly and evaluates whether the flight is safe.\n\nApplied in:\n- [Spec-Driven Development](/concepts/spec-driven-development) — Product thinking as prerequisite to code generation\n- [The Spec](/patterns/the-spec) — The artifact that captures product thinking\n- [Vibe Coding](/concepts/vibe-coding) — The failure mode when product thinking is skipped\n\n## Best Practices\n\n**For Individual Engineers:**\n1. Before writing code, write the \"why\" in plain English\n2. Question requirements that don't explain user impact\n3. Propose alternatives when you see tradeoff mismatches\n4. Treat AI-generated code skeptically: Does it solve the right problem?\n\n**For Engineering Leaders:**\n1. Share business context, even when it feels like \"too much detail\"\n2. Reward engineers who challenge bad requirements, not just those who ship fast\n3. Make \"why\" documentation non-optional (use Specs or equivalent)\n4. Measure outcomes (user adoption, retention, error rates) not just velocity (story points)\n\n**For Organizations:**\n1. Flatten decision-making: trust engineers to own tradeoffs in their domain\n2. Train product thinking explicitly (it's not intuitive for engineers trained to \"just code\")\n3. Create feedback loops: engineers see how their code impacts users\n4. Recognize that AI scales implementation, not judgment—invest in the latter\n\n## Anti-Patterns\n\n**\"Just Build It\" Culture:**\nEngineers discouraged from asking \"why\" or proposing alternatives. Leads to technically correct code that solves the wrong problem.\n\n**Context Hoarding:**\nProduct managers or architects hold all context and dole out tasks. Creates dependency bottleneck and prevents engineers from exercising judgment.\n\n**Velocity Worship:**\nSuccess measured by tickets closed, not problems solved. Optimizes for speed of wrong solutions.\n\n**\"Stay In Your Lane\" Enforcement:**\nEngineers punished for thinking beyond their assigned component. Prevents system-level thinking required for good product decisions.\n\nSee also:\n- [Industry Alignment](/resources/industry-alignment) — External voices on the product thinking shift\n- [Spec-Driven Development](/concepts/spec-driven-development) — How ASDLC encodes product thinking\n- [Adversarial Requirement Review](/patterns/adversarial-requirement-review) — The verification pattern that operationalizes product thinking\n- [Vibe Coding](/concepts/vibe-coding) — What happens when product thinking is absent",
    "tags": ["Product Management", "Engineering Culture", "AI Era"],
    "references": [
      {
        "type": "book",
        "title": "Product Driven: Creating Products Customers Love Through Product-Led Growth",
        "author": "Matt Watson",
        "published": "2023-01-01T00:00:00.000Z",
        "annotation": "Comprehensive framework for product-thinking engineers, defining how to build products customers actually want."
      },
      {
        "type": "website",
        "title": "Vibe coders perform better than the average software engineer",
        "url": "https://www.linkedin.com/posts/mattwatson_vibe-coders-perform-better-than-the-average-activity-7286106547847921664-8Xvr",
        "author": "Matt Watson",
        "published": "2026-01-15T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "LinkedIn post arguing that vibe coders succeed because traditional software training missed product thinking fundamentals."
      },
      {
        "type": "website",
        "title": "Product Thinking Frameworks",
        "url": "https://www.linkedin.com/in/shreyasdoshi/",
        "author": "Shreyas Doshi",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Mental models and frameworks for evaluating product decisions from a product management perspective."
      },
      {
        "type": "book",
        "title": "Empowered: Ordinary People, Extraordinary Products",
        "author": "Marty Cagan",
        "published": "2020-12-03T00:00:00.000Z",
        "annotation": "Defining the difference between product teams (empowered to solve problems) and feature teams (told what to build)."
      }
    ]
  },
  {
    "slug": "production-readiness-gap",
    "collection": "concepts",
    "title": "Production Readiness Gap",
    "description": "The distance between a working generative AI demo and a secure, scalable production system.",
    "status": "Experimental",
    "content": "## Definition\n\nThe **Production Readiness Gap** is the distance between \"demo works\" and \"runs securely in production at scale.\" This gap represents the validation work required when transitioning [Vibe Coded](/concepts/vibe-coding) prototypes to production systems.\n\nThe gap encompasses:\n\n- **Correctness**: From \"90% correct\" (probabilistic generation) to \"always correct\" (authentication, data integrity)\n- **Performance**: From seconds (LLM latency) to milliseconds (business logic)\n- **Cost**: From acceptable demo spend to sustainable unit economics\n- **Maintainability**: From \"I understand it\" to \"the team understands it in 2 years\"\n- **Compliance**: From \"works\" to \"auditable, secure, and legally defensible\"\n\n## The Fundamental Asymmetry\n\nCrossing the Production Readiness Gap requires capabilities that LLMs currently lack without structural support:\n\n| Demo Requirements | Production Requirements |\n|---|---|\n| Local correctness (this function works) | Global correctness (system behaves consistently) |\n| Happy path | All edge cases, error states, failure modes |\n| Works once | Works reliably under load, over time |\n| Developer understands it | Team maintains it for years |\n| Acceptable cost for testing | Sustainable unit economics at scale |\n\n> \"You can't ship '90% correct' to enterprise customers. You can't have authentication that works 'most of the time' or data integrity that's 'pretty good.'\"\n> — Dan Cripe\n\n## The \"Missing Incentive\" Test\n\nA useful heuristic for evaluating AI capability claims: **Are domain experts doing it?**\n\nIf autonomous agents could spin up production SaaS with small teams, experienced engineers would be doing it en masse. They're not. The people claiming it's possible are typically:\n\n1. Building personal productivity tools (valid, but not enterprise SaaS)\n2. Running demos that haven't hit production\n3. Not disclosing how much human intervention (L2/L3) is actually happening\n\n## Observability as a Production Requirement\n\nThe Production Readiness Gap isn't just about security, performance, and maintainability—it's about **verifiability in production**. If you can't observe what your code is doing after deployment, you can't validate that it works.\n\n> \"The bottleneck shifts from, 'How fast can I write code?' to, 'How fast can I understand what's happening and make good decisions about it?'\"\n> — Charity Majors\n\nAI has made code generation nearly free. The constraint has shifted to understanding and validating what that code does in production. This reframes production readiness:\n\n| Old Constraint | New Constraint |\n|----------------|----------------|\n| Writing code | Understanding code |\n| Testing before deploy | Validating after deploy |\n| Hope it works | Observe that it works |\n\nWithout observability, you're \"shipping blind\"—deploying code that nobody fully understands, with no feedback loop to validate success. See [Feedback Loop Compression](/concepts/feedback-loop-compression) for how AI enables tighter observe → validate → learn cycles.\n\n## ASDLC Usage\n\nApplied in:\n- [Spec-Driven Development](/concepts/spec-driven-development)\n- [Context Gates](/patterns/context-gates)\n- [Levels of Autonomy](/concepts/levels-of-autonomy)\n- [Agent Constitution](/patterns/agent-constitution)",
    "tags": ["Context Engineering", "Quality", "Production", "Enterprise"],
    "references": [
      {
        "type": "website",
        "title": "The Mythical LLM: Why Rumors of the Death of Software are Premature",
        "author": "Dan Cripe",
        "published": "2026-01-20T00:00:00.000Z",
        "url": "https://www.dancripe.com/ai-coding-enterprise-saas-reality-check/",
        "accessed": "2026-01-24T00:00:00.000Z",
        "annotation": "Defines the gap and the 'missing incentive' test, differentiating between demo capabilities and enterprise production requirements."
      },
      {
        "type": "website",
        "title": "Vibe Coding Fails Enterprise Reality Check",
        "author": "The New Stack",
        "published": "2025-09-10T00:00:00.000Z",
        "url": "https://thenewstack.io/vibe-coding-fails-enterprise-reality-check/",
        "accessed": "2026-01-24T00:00:00.000Z",
        "annotation": "James Gosling commentary on complexity scaling failures in vibe-coded projects."
      },
      {
        "type": "website",
        "title": "\"You Had One Job\": Why Twenty Years of DevOps Has Failed to Do it",
        "author": "Charity Majors",
        "published": "2026-01-22T00:00:00.000Z",
        "url": "https://www.honeycomb.io/blog/you-had-one-job-why-twenty-years-of-devops-has-failed-to-do-it",
        "accessed": "2026-01-26T00:00:00.000Z",
        "annotation": "Defines observability as the key constraint in AI-accelerated development—code is cheap, validation is expensive."
      }
    ]
  },
  {
    "slug": "request-for-comments",
    "collection": "concepts",
    "title": "Request for Comments",
    "description": "A collaborative proposal document for significant changes that require team consensus before becoming formal decisions.",
    "status": "Live",
    "content": "## Definition\n\nA **Request for Comments (RFC)** is a proposal document that solicits feedback on significant changes before they become formal decisions. Unlike an [ADR](/concepts/architecture-decision-record) which records a decision already made, an RFC opens a decision for collaborative input.\n\nThe term originates from the IETF (Internet Engineering Task Force), where RFCs have defined internet protocols since 1969. Modern software projects—Rust, React, Ember, Python—have adopted RFC processes for significant changes that affect many stakeholders.\n\n## Key Characteristics\n\n### Proposal-Oriented\n\nRFCs propose; ADRs record. An RFC says \"We should consider doing X\" while an ADR says \"We decided to do X.\" The RFC process concludes with either acceptance (spawning ADRs) or rejection.\n\n### Collaborative\n\nRFCs are designed for multi-stakeholder input. They include explicit comment periods and revision cycles. The goal is to surface concerns *before* committing to a direction.\n\n### Scope\n\nRFCs typically cover changes that:\n- Affect multiple teams or systems\n- Require significant migration effort\n- Introduce breaking changes\n- Establish new architectural patterns\n\nSingle-component decisions usually don't warrant an RFC—a direct ADR suffices.\n\n## Relationship to ADRs\n\n| Dimension | RFC | ADR |\n|-----------|-----|-----|\n| **Purpose** | Propose and gather feedback | Record a decision |\n| **Timing** | Before decision | After decision |\n| **Mutability** | Revised during comment period | Immutable once accepted |\n| **Output** | One or more ADRs | Implementation guidance |\n\nAn RFC may spawn multiple ADRs. For example, \"RFC: Migrate from Firebase to Supabase\" might result in:\n- ADR-010: Use Supabase Auth\n- ADR-011: Use Supabase Realtime for subscriptions\n- ADR-012: Migration strategy for existing users\n\n## ASDLC Usage\n\nIn ASDLC, RFCs are appropriate for:\n- **Major architectural pivots** (changing database providers, frontend frameworks)\n- **Cross-cutting changes** (new authentication model, API versioning strategy)\n- **Migration plans** (multi-phase transitions that affect multiple features)\n\nFor routine architectural decisions within a single feature domain, a direct ADR is sufficient.\n\nApplied in:\n- [The ADR](/patterns/the-adr) — Pattern for recording decisions that result from RFCs\n- [ADR Authoring](/practices/adr-authoring) — Includes RFC-to-ADR workflow\n\nSee also:\n- [Context Engineering](/concepts/context-engineering) — RFCs as context for understanding project direction",
    "tags": ["RFC", "Architecture", "Collaboration", "Decision Making"],
    "references": [
      {
        "type": "website",
        "title": "Rust RFCs",
        "url": "https://rust-lang.github.io/rfcs/",
        "accessed": "2026-01-28T00:00:00.000Z",
        "annotation": "Rust's RFC process for language and ecosystem changes."
      },
      {
        "type": "website",
        "title": "React RFCs",
        "url": "https://github.com/reactjs/rfcs",
        "accessed": "2026-01-28T00:00:00.000Z",
        "annotation": "React's RFC process for significant API and behavior changes."
      }
    ]
  },
  {
    "slug": "spec-driven-development",
    "collection": "concepts",
    "title": "Spec-Driven Development",
    "description": "Methodology that defines specifications before implementation, treating specs as living authorities that code must fulfill.",
    "status": "Live",
    "content": "## Definition\n\n**Spec-Driven Development (SDD)** is an umbrella term for methodologies that define specifications before implementation. The core inversion: instead of code serving as the source of documentation, the spec becomes the authority that code must fulfill.\n\nSDD emerged as a response to documentation decay in software projects. Traditional approaches treated specs as planning artifacts that diverged from reality post-implementation. Modern SDD treats specs as **living documents** co-located with code.\n\n> **Contrast:** For the anti-pattern SDD addresses, see [Vibe Coding](/concepts/vibe-coding).\n\n## Key Characteristics\n\n### Living Documentation\nSpecs are not \"fire and forget\" planning artifacts. They reside in the repository alongside code and evolve with every change to the feature. This addresses the classic problem of documentation decay.\n\n### Iterative Refinement\nKent Beck critiques SDD implementations that assume \"you aren't going to learn anything during implementation.\" This is a valid concern—specs must evolve during implementation, not block it. The spec captures learnings so future sessions can act on them.\n\n### Determinism Over Vibes\nNick Tune argues that orchestration logic should be \"mechanical based on simple rules\" (code) rather than probabilistic (LLMs). Specs define the rigid boundaries; code enforces the workflow; LLMs handle only the implementation tasks where flexibility is required.\n\n### Visual Designs Are Not Specs\n\n> [!WARNING]\n> **The Figma Trap**\n> A beautiful mockup is not a specification; it is a suggestion. Mockups typically demonstrate the \"happy path\" but hide the edge cases, error states, and data consistency rules where production bugs live.\n>\n> **Never** treat a visual design as a complete technical requirement.\n\n## ASDLC Usage\n\nASDLC implements Spec-Driven Development through:\n\n- **[The Specs Pattern](/patterns/the-spec)** — The structural blueprint defining what a spec contains (Blueprint + Contract) and how it relates to PBIs\n- **[Living Specs Practice](/practices/living-specs)** — How to create, maintain, and evolve specs alongside code\n- **[The Learning Loop](/concepts/learning-loop)** — The iterative cycle that addresses Beck's critique\n- **[Workflow as Code](/practices/workflow-as-code)** — Deterministic orchestration that enforces spec contracts programmatically\n\nSee also:\n- [Vibe Coding](/concepts/vibe-coding) — The anti-pattern SDD addresses\n- [Context Engineering](/concepts/context-engineering) — Structuring specs for agent consumption",
    "tags": ["Software Development", "Documentation", "Specifications", "Living Documentation"],
    "references": [
      {
        "type": "website",
        "title": "Martin Fowler Fragment: January 8, 2026",
        "url": "https://martinfowler.com/fragments/2026-01-08.html",
        "author": "Martin Fowler",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Commentary on Anthropic research and Kent Beck's critique of spec-driven approaches."
      },
      {
        "type": "website",
        "title": "Kent Beck on Spec-Driven Development",
        "url": "https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/",
        "author": "Kent Beck",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Critique emphasizing that specifications must accommodate learning during implementation."
      },
      {
        "type": "website",
        "title": "Dev Workflows as Code",
        "url": "https://medium.com/nick-tune-tech-strategy-blog/dev-workflows-as-code-fab70d44b6ab",
        "author": "Nick Tune",
        "published": "2026-01-16T00:00:00.000Z",
        "accessed": "2026-01-18T00:00:00.000Z",
        "annotation": "Validates the core thesis: use real code for deterministic logic and LLMs only for tasks requiring intelligence."
      }
    ]
  },
  {
    "slug": "vibe-coding",
    "collection": "concepts",
    "title": "Vibe Coding",
    "description": "Natural language code generation without formal specs—powerful for prototyping, problematic for production systems.",
    "status": "Experimental",
    "content": "## Definition\n\nVibe Coding is the practice of generating code directly from natural language prompts without formal specifications, schemas, or contracts. Coined by Andrej Karpathy, the term describes an AI-assisted development mode where engineers describe desired functionality conversationally (\"make this faster,\" \"add a login button\"), and the LLM produces implementation code.\n\nThis approach represents a fundamental shift: instead of writing specifications that constrain implementation, developers describe intent and trust the model to infer the details. The result is rapid iteration—code appears almost as fast as you can articulate what you want.\n\nWhile vibe coding accelerates prototyping and exploration, it inverts traditional software engineering rigor: the specification emerges *after* the code, if at all.\n\n## The Seduction of Speed\n\nThe productivity gains from vibe coding are undeniable:\n\n* **At Anthropic:** 80-90% of Claude Code's codebase is now written by Claude Code itself, with a 70% productivity increase per engineer since adoption.\n* **At Google:** Approximately 30% of code committed in 2024 was AI-generated.\n* **Industry-wide:** Engineers report 2-10x faster feature delivery for greenfield projects and prototypes.\n\nThis velocity is seductive. When a feature that previously took three days can be scaffolded in thirty minutes, the economic pressure to adopt vibe coding becomes overwhelming.\n\nThe feedback loop is immediate: describe the behavior, see the code, run it, iterate. For throwaway scripts, MVPs, and rapid exploration, this workflow is transformative.\n\n## The Failure Modes\n\nThe velocity advantage of vibe coding collapses when code must be maintained, extended, or integrated into production systems:\n\n### Technical Debt Accumulation\n\n**Forrester Research predicts that by 2026, 75% of technology leaders will face moderate-to-severe technical debt** directly attributable to AI-generated code. The mechanism is straightforward: code generated from vague prompts encodes vague assumptions.\n\nWhen specifications exist only in the prompt history (or the engineer's head), future maintainers inherit code without contracts. They must reverse-engineer intent from implementation—the exact problem formal specifications solve.\n\n### Copy-Paste Culture\n\n2024 marked the first year in industry history where **copy-pasted code exceeded refactored code**. This is a direct symptom of vibe coding: when generating fresh code is faster than understanding existing code, engineers default to regeneration over refactoring.\n\n### Legacy Code in Record Time\n\nAs [Codurance](/concepts/vibe-coding#references) notes, speed without craftsmanship leads to \"Legacy Code in record time.\" When AI generates code faster than a human can understand it, the codebase immediately becomes \"legacy\"—code that developers are afraid to touch because they don't understand its underlying intent or guarantees.\n\nThe result is systemic duplication. The same logic appears in fifteen places with fifteen slightly different implementations, none validated against a shared contract.\n\n### Silent Drift\n\nLLMs are probabilistic. When generating code from vibes, they make assumptions:\n- Error handling strategies (fail silently? throw? log?)\n- Data validation rules (what's a valid email?)\n- Concurrency models (locks? optimistic? eventual consistency?)\n\nThese assumptions are *never documented*. The code passes tests (if tests exist), but violates implicit architectural contracts. Over time, the system drifts toward inconsistency—different modules make different assumptions about the same concepts.\n\nBoris Cherny (Principal Engineer, Anthropic; creator of Claude Code) warns: **\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes.\"**\n\n> **\"Speed is seductive. Maintainability is survival.\"**  \n> — Boris Cherny, *The Peterman Podcast* (December 2025)\n\n> [!NOTE]\n> **The 100 Million Token Lesson**\n> \n> Dan Cripe, a 25-year enterprise software veteran, documented spending 100 million tokens on a frontier model attempting to fix its own architectural mistakes—not syntax errors, but fundamental design pattern violations. His diagnosis: \"LLMs are pattern matchers, not architects. They generate code that looks like the code they were trained on: code written to solve an immediate problem, not code designed to be maintainable as part of a larger system.\"\n\n### Vibe Coded Into a Corner\n\nAnthropic's internal research found that engineers who spend *more* time on Claude-assisted tasks often do so because they \"vibe code themselves into a corner\"—generating code without specs until debugging and cleanup overhead exceeds the initial velocity gains.\n\n> \"When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.\"\n> — Anthropic engineer\n\nThis creates a debt spiral: vibe coding is fast until it isn't, and by then the context needed to fix issues was never documented.\n\n### Regression to the Mean\n\nWithout deterministic constraints, LLMs trend toward generic solutions. Vibe coding produces code that works but lacks the specific optimizations, domain constraints, and architectural decisions that distinguish production systems from prototypes.\n\nThe model doesn't know that \"user IDs must never be logged\" or \"this cache must invalidate within 100ms.\" These constraints exist in specifications, not prompts.\n\n## Applications\n\nVibe coding is particularly effective in specific contexts:\n\n**Rapid Prototyping:** When validating product hypotheses, speed of iteration outweighs code quality. Vibe coding enables designers and product managers to generate functional prototypes without deep programming knowledge.\n\n**Throwaway Scripts:** One-off data migrations, analysis scripts, and temporary tooling benefit from vibe coding's velocity. Since the code has no maintenance burden, formal specifications are unnecessary overhead.\n\n**Learning and Exploration:** When experimenting with new APIs, frameworks, or architectural patterns, vibe coding provides immediate feedback. The goal is understanding, not production-ready code.\n\n**Greenfield MVPs:** Early-stage startups building minimum viable products often prioritize speed-to-market over maintainability. Vibe coding accelerates this phase, though technical debt must be managed during the transition to production.\n\n## ASDLC Usage\n\nIn ASDLC, vibe coding is recognized as a legitimate operational mode for bounded contexts (exploration, prototyping, throwaway code). However, for production systems, ASDLC mandates a transition to deterministic development.\n\n**The ASDLC position:**\n- Vibe coding is **steering** (probabilistic guidance via prompts)\n- Production requires **determinism** (schemas, tests, typed interfaces)\n- Both are necessary: prompts steer the agent; schemas enforce correctness\n\nApplied in:\n- [Spec-Driven Development](/concepts/spec-driven-development) — The production-grade alternative to vibe coding\n- [Context Gates](/patterns/context-gates) — Deterministic enforcement layer\n- [Levels of Autonomy](/concepts/levels-of-autonomy) — Human oversight model (L3: \"Hands Off, Eyes On\")\n\nSee also:\n- [Industry Alignment](/resources/industry-alignment) — External voices converging on ASDLC principles\n- [Spec-Driven Development](/concepts/spec-driven-development) — ASDLC's production-grade methodology\n- [Context Gates](/patterns/context-gates) — Deterministic enforcement layer",
    "tags": ["Disambiguation", "AI", "Code Quality", "Anti-Pattern"],
    "references": [
      {
        "type": "podcast",
        "title": "Claude Code and the Future of AI-Assisted Development",
        "url": "https://peterman.fm/boris-cherny",
        "author": "Boris Cherny",
        "publisher": "The Peterman Podcast",
        "published": "2025-12-01T00:00:00.000Z",
        "annotation": "Claude Code creator's framework for disciplined AI-assisted development, discussing the balance between automation and rigor."
      },
      {
        "type": "website",
        "title": "Forrester Research on AI-Generated Code Technical Debt",
        "author": "Forrester Research",
        "published": "2024-01-01T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Technical debt predictions and analysis for AI-generated code in production systems."
      },
      {
        "type": "website",
        "title": "Google's AI-Generated Code Adoption Metrics",
        "author": "Google",
        "published": "2024-01-01T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Industry data on Google's 30% AI-generated code adoption rate and analysis of copy-paste versus refactor patterns."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Research showing engineers vibe code themselves into corners, with cleanup overhead exceeding initial velocity."
      },
      {
        "type": "website",
        "title": "The Mythical LLM: Why Rumors of the Death of Software are Premature",
        "author": "Dan Cripe",
        "published": "2026-01-20T00:00:00.000Z",
        "url": "https://www.dancripe.com/ai-coding-enterprise-saas-reality-check/",
        "accessed": "2026-01-24T00:00:00.000Z",
        "annotation": "Enterprise architect's critique of vibe coding for production systems. Documents spending 100M tokens on architectural drift fixes and argues the quality gap is 'fundamental to how current LLMs work.'"
      },
      {
        "type": "website",
        "title": "Calm Coding: The Workflow That Makes Vibecoding Survivable",
        "url": "https://oc2sf.com/blog/calm-coding-vibecoding-survivable.html",
        "author": "Jan (OC2SF)",
        "published": "2026-02-01T00:00:00.000Z",
        "accessed": "2026-02-04T00:00:00.000Z",
        "annotation": "Governance framework positioning 'Calm Coding' as the disciplined counterpart to vibecoding. Introduces 'The Hype-Man Problem' — LLMs enabling bad decisions with high conviction. Key quote: 'Velocity without direction isn't speed. It's drift.'"
      },
      {
        "type": "website",
        "title": "Software Craftsmanship in the AI Era",
        "author": "Codurance",
        "url": "https://www.codurance.com/publications/software-craftsmanship-in-the-ai-era",
        "published": "2024-05-23T00:00:00.000Z",
        "accessed": "2026-02-12T00:00:00.000Z",
        "annotation": "Analysis of how speed without discipline creates 'legacy code in record time,' and why AI increases rather than decreases the need for TDD and solid design principles."
      }
    ]
  },
  {
    "slug": "yaml",
    "collection": "concepts",
    "title": "YAML",
    "description": "A human-readable data serialization language that serves as the structured specification format for configuration, schemas, and file structures in agentic workflows.",
    "status": "Live",
    "content": "## Definition\n\nYAML (YAML Ain't Markup Language) is a human-readable data serialization language designed for configuration files, data exchange, and structured documentation. In agentic development, YAML serves as the specification language for data structures, schemas, and file organization.\n\nWhere [Gherkin](/concepts/gherkin) specifies *behavior* (Given-When-Then), YAML specifies *structure* (keys, values, hierarchies). Both are human-readable formats that bridge the gap between human intent and machine execution.\n\n## Key Characteristics\n\n### Human-Readable Structure\n\nYAML's indentation-based syntax mirrors how humans naturally organize hierarchical information:\n\n```yaml\nnotification:\n  channels:\n    - websocket\n    - email\n    - sms\n  constraints:\n    latency_ms: 100\n    retry_count: 3\n  fallback:\n    enabled: true\n    order: [websocket, email, sms]\n```\n\n### Schema-First Design\n\nYAML enables schema-first development where data structures are defined before implementation:\n\n```yaml\n# Schema definition in spec\nuser:\n  id: string (UUID)\n  email: string (email format)\n  roles: array of enum [admin, user, guest]\n  created_at: datetime (ISO 8601)\n```\n\nAgents can validate implementations against these schemas, catching type mismatches and missing fields before runtime.\n\n### Configuration as Code\n\nYAML configurations live in version control alongside code, enabling:\n- **Diff visibility** — Configuration changes appear in PRs\n- **Review process** — Same rigor as code changes\n- **History tracking** — Git blame shows who changed what and when\n\n## ASDLC Usage\n\nYAML serves as the **data structure specification language** in ASDLC, completing the specification triad:\n\n- **[Gherkin](/concepts/gherkin)** — Specifies behavior (what happens)\n- **YAML** — Specifies structure (what exists)\n- **[Mermaid](/concepts/mermaid)** — Specifies process (how it flows)\n\n**In Specs:** All ASDLC articles use YAML frontmatter for structured metadata. The [Spec](/patterns/the-spec) pattern leverages YAML for schema definitions that agents validate against.\n\n**In AGENTS.md:** The [AGENTS.md Specification](/practices/agents-md-spec) uses YAML for structured directives—project context, constraints, and preferred patterns.\n\n**Applied in:**\n- [The Spec](/patterns/the-spec) — Frontmatter and schema definitions\n- [AGENTS.md Specification](/practices/agents-md-spec) — Agent configuration\n- [Context Engineering](/concepts/context-engineering) — Structured context formats",
    "tags": ["Data", "Configuration", "Specification", "Syntax"],
    "references": [
      {
        "type": "website",
        "title": "YAML Specification",
        "url": "https://yaml.org/spec/1.2.2/",
        "author": "YAML Language Development Team",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Official YAML 1.2.2 specification defining syntax and semantics."
      }
    ]
  },
  {
    "slug": "adversarial-code-review",
    "collection": "patterns",
    "title": "Adversarial Code Review",
    "description": "Consensus verification pattern using a secondary Critic Agent to review Builder Agent output against the Spec.",
    "status": "Live",
    "content": "## Definition\n\n**Adversarial Code Review** is a verification pattern where a distinct AI session—the **Critic Agent**—reviews code produced by the **Builder Agent** against the [Spec](/patterns/the-spec) before human review.\n\nThis extends the [Critic (Hostile Agent)](/patterns/agentic-double-diamond) pattern from the design phase into the implementation phase, creating a verification checkpoint that breaks the \"echo chamber\" where a model validates its own output.\n\nThe Builder Agent (optimized for speed and syntax) generates code. The Critic Agent (optimized for reasoning and logic) attempts to reject it based on spec violations.\n\n## The Problem: Self-Validation Ineffectiveness\n\nLLMs are probabilistic text generators trained to be helpful. When asked \"Check your work,\" a model that just generated code will often:\n\n**Hallucinate correctness** — Confidently affirm that buggy logic is correct because it matches the plausible pattern in training data.\n\n**Double down on errors** — Explain why the bug is actually a feature, reinforcing the original mistake.\n\n**Share context blindness** — Miss gaps because it operates within the same context window and reasoning path that produced the original output.\n\nIf the same computational session writes and reviews code, the \"review\" provides minimal independent validation.\n\n## The Solution: Separated Roles\n\nTo create effective verification, separate the generation and critique roles:\n\n**The Builder** — Optimizes for implementation throughput (e.g., Gemini 3 Flash, Claude Haiku 4.5). Generates code from the PBI and Spec.\n\n**The Critic** — Optimizes for logical consistency and constraint satisfaction (e.g., Gemini 3 Deep Think, DeepSeek V3.2). Validates code against Spec contracts without rewriting.\n\nThe Critic does not generate alternative implementations. It acts as a gatekeeper, producing either **PASS** or a list of **spec violations** that must be addressed.\n\n## The Workflow\n\n### 1. Build Phase\n\nThe Builder Agent implements the PBI according to the Spec.\n\n**Output:** Code changes, implementation notes.\n\n**Example:** \"Updated `auth.ts` to support OAuth login flow.\"\n\n### 2. Context Swap (Fresh Eyes)\n\n**Critical:** Start a new AI session or chat thread for critique. This clears conversation drift and forces the Critic to evaluate only the artifacts (Spec + Diff), not the Builder's reasoning process.\n\nIf using the same model, close the current chat and open a fresh session. If using [Model Routing](/patterns/model-routing), switch to a High Reasoning model.\n\n### 3. Critique Phase\n\nFeed the Spec and the code diff to the Critic Agent with adversarial framing:\n\n**System Prompt:**\n```\nYou are a rigorous Code Reviewer validating implementation against contracts.\n\nInput:\n- Spec: specs/auth-system.md\n- Code Changes: src/auth.ts (diff)\n\nTask:\nCompare the code strictly against the Spec's Blueprint (constraints) and Contract (quality criteria).\n\nIdentify:\n1. Spec violations (missing requirements, violated constraints)\n2. Security issues (injection vulnerabilities, auth bypasses)\n3. Edge cases not handled (error paths, race conditions)\n4. Anti-patterns explicitly forbidden in the Spec\n\nOutput Format:\n- PASS (if no violations)\n- For each violation, provide:\n  1. Violation Description (what contract was broken)\n  2. Impact Analysis (why this matters: performance, security, maintainability)\n  3. Remediation Path (ordered list of fixes, prefer standard patterns, escalate if needed)\n  4. Test Requirements (what tests would prevent regression)\n\nThis transforms critique from \"reject\" to \"here's how to fix it.\"\n```\n\n### 4. Verdict\n\n**If PASS:** Code moves to human Acceptance Gate (L3 review for strategic fit).\n\n**If FAIL:** Violations are fed back to Builder as a new task: \"Address these spec violations before proceeding.\"\n\nThis creates a [Context Gate](/patterns/context-gates) between code generation and human review.\n\n## Relationship to Context Gates\n\nAdversarial Code Review implements a **Review Gate** as defined in [Context Gates](/patterns/context-gates):\n\n**Quality Gates** (deterministic) — Verify syntax, compilation, linting, test passage.\n\n**Review Gates** (probabilistic, adversarial) — Verify semantic correctness, spec compliance, architectural consistency. **This is where Adversarial Code Review operates.**\n\n**Acceptance Gates** (subjective, HITL) — Verify strategic fit and product vision alignment.\n\nThe Critic sits between automated tooling and human review, catching issues that compilers miss but that don't require human strategic judgment.\n\n## Integration with Model Routing\n\nUse [Model Routing](/patterns/model-routing) to assign models by capability profile:\n\n| Role | Model Profile | Rationale |\n|------|---------------|-----------|\n| Builder | High Throughput | Fast code generation with strong syntax knowledge |\n| Critic | High Reasoning | Deep logic evaluation, constraint satisfaction, edge case discovery |\n\nThis leverages the strengths of each model class: speed for generation, reasoning depth for validation.\n\n## Strategic Value\n\n**Reduces L3 Cognitive Load** — Human reviewers focus on \"Is this the right product?\" rather than catching spec deviations or missing error handling.\n\n**Catches Regression to Mediocrity** — Coding models gravitate toward average solutions. The Critic enforces novelty and architectural intent from the Spec.\n\n**Enforces Spec Quality** — If the Critic can't determine whether code is correct, the Spec is ambiguous. This surfaces specification gaps.\n\n**Prevents Silent Failures** — The Critic catches implementation shortcuts (skipped validation, missing edge cases) that pass tests but violate contracts.\n\n## Validated in Practice\n\n**Case Study: Claudio Lassala (January 2026)**\n\nA production implementation validated this pattern's effectiveness:\n\n**Context:** A user story required filtering audit logs by date range. The Builder Agent implemented the requirement, tests passed, and the code compiled without errors.\n\n**Issue Detected:** The Critic Agent identified a silent performance violation:\n\n```csharp\n// Implementation passed all Quality Gates but violated architectural constraint\nvar logs = await repository.LoadAll(); // Loads entire table into memory\nreturn logs.Where(log => log.Date > startDate); // Filters in-memory\n```\n\n**Critic Output:**\n```\nVIOLATION: Performance - Data Access Pattern\n\nSpec requires database-level filtering for datasets exceeding 1k records.\nImplementation loads full table then filters in-memory.\n\nImpact: Works with small data; fails at scale (10k+ records cause memory issues)\n\nRemediation Path:\n1. Push filtering to database: repository.FindWhere(x => x.Date > startDate)\n2. If ORM doesn't support this filter, use raw SQL\n3. Add performance test with 10k+ mock records to prevent regression\n4. Document the constraint in the repository interface\n```\n\n**Key Learnings:**\n\n1. **Silent Performance Risks** — Code that passes all tests can still violate architectural constraints. The Critic caught the `LoadAll().Filter()` anti-pattern before production.\n\n2. **Iterative Refinement** — The Critic initially flagged \"missing E2E tests,\" which were actually present but structured differently. The team updated the Critic's instructions to recognize the project's test architecture, demonstrating the pattern's adaptability.\n\n3. **Tone Calibration** — Using \"Approve with suggestions\" framing prevented blocking valid code while surfacing genuine risks. The Critic didn't reject the PR—it flagged optimization opportunities with clear remediation paths.\n\nThis validates the pattern's core thesis: adversarial review catches architectural violations that pass deterministic checks but violate semantic contracts.\n\n## Example: The Silent Performance Bug\n\n**Spec Contract:** \"All database retries must use exponential backoff to prevent thundering herd during outages.\"\n\n**Builder Output:** Clean code with a simple retry loop using fixed 1-second delays. Tests pass.\n\n```typescript\n// src/db.ts\nasync function queryWithRetry(sql: string) {\n  for (let i = 0; i < 5; i++) {\n    try {\n      return await db.query(sql);\n    } catch (err) {\n      await sleep(1000); // Fixed delay\n    }\n  }\n}\n```\n\n**Critic Response:**\n```\nVIOLATION: src/db.ts Line 45\n\nSpec requires exponential backoff. Implementation uses constant sleep(1000).\n\nImpact: During database outages, this will cause thundering herd problems\nas all clients retry simultaneously.\n\nRequired: Implement delay = baseDelay * (2 ** attemptNumber)\n```\n\nWithout the Critic, a human skimming the PR might miss the constant delay. The automated tests wouldn't catch it (the code works). The Critic, reading against the contract, identifies the violation.\n\n## Implementation Constraints\n\n**Not Automated (Yet)** — As of December 2025, this requires manual orchestration. Engineers must manually switch sessions/models and feed context to the Critic.\n\n**Context Window Limits** — Large diffs may exceed even Massive Context models. Use [Context Gates](/patterns/context-gates) filtering to provide only changed files + relevant Spec sections.\n\n**Critic Needs Clear Contracts** — The Critic can only enforce what's documented in the Spec. Vague specs produce vague critiques.\n\n**Model Capability Variance** — Not all \"reasoning\" models perform equally at code review. Validate your model's performance on representative examples.\n\n## Relationship to Agent Constitution\n\nThe [Agent Constitution](/patterns/agent-constitution) defines behavioral directives for agents. For Adversarial Code Review:\n\n**Builder Constitution:** \"Implement the Spec's contracts. Prioritize clarity and correctness over cleverness.\"\n\n**Critic Constitution:** \"You are skeptical. Your job is to reject code that violates the Spec, even if it 'works.' Favor false positives over false negatives.\"\n\nThis frames the Critic's role as adversarial by design—it's explicitly told to be rigorous and skeptical, counterbalancing the Builder's helpfulness bias.\n\n## Future Automation Potential\n\nThis pattern is currently manual but has clear automation paths:\n\n**CI/CD Integration** — Run Critic automatically on PR creation, posting violations as review comments.\n\n**IDE Integration** — Real-time critique as code is written, similar to linting but spec-aware.\n\n**Multi-Agent Orchestration** — Automated handoff between Builder and Critic until PASS is achieved.\n    \n### Programmatic Orchestration (Workflow as Code)\n\nTo scale this pattern, move from manual prompt-pasting to code-based orchestration (e.g., using the Claude Code SDK).\n\n**Convention-Based Loading:**\nStore reviewer agent prompts in a standard directory (e.g., `.claude/agents/`) and load them dynamically:\n\n```typescript\n// Load the specific reviewer agent\nconst reviewerPrompt = await fs.readFile(`.claude/agents/${agentName}.md`);\n\n// Spawn subagent via SDK\nconst reviewResult = await claude.query({\n  prompt: reviewerPrompt,\n  context: { spec, diff },\n  outputFormat: { type: 'json_schema', schema: ReviewSchema }\n});\n```\n\nThis allows you to treat Critic Agents as **standardized, version-controlled functions** in your build pipeline.\n\n\nAs agent orchestration tooling matures, this pattern may move from Experimental to Standard.\n\nSee also:\n- [Context Gates](/patterns/context-gates) — The architectural checkpoint pattern this implements\n- [The Spec](/patterns/the-spec) — The source of truth the Critic validates against\n- [Model Routing](/patterns/model-routing) — How to assign different models to Builder and Critic roles\n- [Agentic Double Diamond](/patterns/agentic-double-diamond) — The design-phase Critic pattern this extends\n- [Adversarial Requirement Review](/patterns/adversarial-requirement-review) — The upstream verification pattern for problem definitions\n- [Agent Constitution](/patterns/agent-constitution) — How to frame Critic behavior as adversarial\n\n### Related Concepts\n- [Agentic SDLC](/concepts/agentic-sdlc) — The Verification phase where this pattern operates\n- [Levels of Autonomy](/concepts/levels-of-autonomy) — L3 autonomy requires verification before human review",
    "tags": ["Code Review", "Quality Gates", "Multi-Agent", "Verification", "Context Engineering"],
    "references": [
      {
        "type": "website",
        "title": "A Method for AI-Assisted Pull Request Reviews: Aligning Code with Business Value",
        "url": "https://lassala.net/2026/01/05/a-method-for-ai-assisted-pull-request-reviews-aligning-code-with-business-value/",
        "author": "Claudio Lassala",
        "published": "2026-01-05T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Production implementation validating the pattern's effectiveness in catching silent performance bugs and architectural violations."
      },
      {
        "type": "website",
        "title": "Dev Workflows as Code",
        "url": "https://medium.com/nick-tune-tech-strategy-blog/dev-workflows-as-code-fab70d44b6ab",
        "author": "Nick Tune",
        "published": "2026-01-16T00:00:00.000Z",
        "accessed": "2026-01-18T00:00:00.000Z",
        "annotation": "Demonstrates programmatic implementation of subagents using Claude Code SDK."
      },
      {
        "type": "website",
        "title": "Calm Coding: The Workflow That Makes Vibecoding Survivable",
        "url": "https://oc2sf.com/blog/calm-coding-vibecoding-survivable.html",
        "author": "Jan (OC2SF)",
        "published": "2026-02-01T00:00:00.000Z",
        "accessed": "2026-02-04T00:00:00.000Z",
        "annotation": "External validation of session separation. Key insight: 'The agent that wrote the code is compromised. It knows what it built. It'll rationalize.'"
      }
    ]
  },
  {
    "slug": "agent-constitution",
    "collection": "patterns",
    "title": "Agent Constitution",
    "description": "Persistent, high-level directives that shape agent behavior and decision-making before action.",
    "status": "Live",
    "content": "## Definition\n\nAn **Agent Constitution** is a set of high-level principles or \"Prime Directives\" injected into an agent's system prompt to align its intent and behavior with system goals.\n\nThe concept originates from Anthropic's [Constitutional AI](https://arxiv.org/abs/2212.08073) research, which proposed training models to be \"Helpful, Honest, and Harmless\" (HHH) using a written constitution rather than human labels alone. In the ASDLC, we adapt this alignment technique to **System Prompt Engineering**—using the Constitution to define the \"Superego\" of our coding agents.\n\n## The Problem: Infinite Flexibility\n    \nWithout a Constitution, an Agent is purely probabilistic. It will optimize for being \"helpful\" to the immediate prompt user, often sacrificing long-term system integrity.\n\nIf a prompt says \"Implement this fast,\" a helpful agent might skip tests. A Constitutional Agent would refuse: \"I cannot skip tests because Principle #3 forbids merging unverified code.\"\n\n## The Solution: Proactive Behavioral Alignment\n\nThe Constitution shapes agent behavior **before** action occurs—unlike reactive mechanisms (tests, gates) that catch problems after the fact.\n\n### The Driver Training Analogy\n\nTo understand the difference between a Constitution and other control mechanisms, consider the analogy of driving a car:\n\n*   **The Spec**: The Destination. \"Drive to 123 Main St.\"\n*   **Context Gates**: The Brakes/Guardrails. Hard limits that stop the car if it's about to hit a wall (e.g., \"Stop if compilation fails\"). These are reactive.\n*   **Agent Constitution**: The Driver Training. The internalized rules (\"Drive defensively,\" \"Yield to pedestrians\") that shape how the driver steers *before* any danger arises. This is proactive.\n\n## The \"Orient\" Phase\n\nIn the [OODA Loop](/concepts/ooda-loop) (Observe-Orient-Decide-Act), the Constitution lives squarely in the **Orient** phase.\n\nWhen an agent **Observes** the world (reads code, sees a user request), the Constitution acts as a filter for how it interprets those observations.\n*   A \"Helpful\" Constitution might interpret a vague request as an opportunity to guess and assist.\n*   A \"Skeptical\" Constitution might interpret the same vague request as a risk to be flagged.\n\n## Taxonomy: Steering vs. Hard Constraints\n\nIt is critical to distinguish what the Constitution *can* enforce (Steering) from what it *must* rely on external systems to enforce (Hard).\n\n### Steering Constraints (Soft)\nThese live in the **System Prompt** or **AGENTS.md**. They influence the model's reasoning, tone, and risk preference.\n- \"Be concise.\"\n- \"Prefer composition over inheritance.\"\n- \"Ask clarifying questions when ambiguous.\"\n\n### Hard Constraints (Orchestration)\nThese live in the **Runtime Environment** (Hooks, API limits, Docker containers). They physically prevent the agent from taking restricted actions.\n- \"Cannot access production database credentials.\"\n- \"Cannot git push without passing automated tests.\"\n- \"Cannot access files outside `/src`.\"\n\nThe Agent Constitution is primarily about **Steering Constraints** that govern *behavior*, while [Context Gates](/patterns/context-gates) and [Workflow as Code](/practices/workflow-as-code) implement the **Hard Constraints**.\n\n## Anatomy of a Constitution\n\nResearch into effective system prompts suggests a constitution should have four distinct components:\n\n### 1. Identity (The Persona)\nWho is the agent? This prunes the search space of the model (e.g., \"You are a Senior Rust Engineer\" vs \"You are a poetic assistant\").\n*   *See [Agent Personas](/practices/agent-personas)*\n\n### 2. The Mission (Objectives)\nWhat is the agent trying to achieve?\n*   *Example:* \"Your goal is to maximize code maintainability, even at the cost of slight verbosity.\"\n\n### 3. The Boundaries (Negative Constraints)\nWhat must the agent *never* do? These are \"Soft Gates\"—instructions to avoid bad paths before hitting the hard [Context Gates](/patterns/context-gates).\n*   *Example:* \"Never output code that swallows errors. Never use `var` in TypeScript.\"\n\n### 4. The Process (Step-by-Step)\nHow should the agent think? This enforces Chain-of-Thought reasoning.\n*   *Example:* \"Before writing code, listing the files you intend to modify. Then, explain your plan.\"\n\n## Constitution vs. Spec\n\nA common failure mode is mixing functional requirements with behavioral guidelines. Separation is critical:\n\n| Feature | Agent Constitution | The Spec |\n| :--- | :--- | :--- |\n| **Scope** | Global / Persona-wide | Local / Task-specific |\n| **Lifespan** | Persistent (Project Lifecycle) | Ephemeral (Feature Lifecycle) |\n| **Content** | Values, Style, Ethics, Safety | Logic, Data Structures, Routes |\n| **Example** | \"Prioritize Type Safety over Brevity.\" | \"User `id` must be a UUID.\" |\n\n## Self-Correction Loop\n\nOne of the most powerful applications of a Constitution is the **Critique-and-Refine** loop (derived from Anthropic's Supervised Learning phase):\n\n1.  **Draft**: Agent generates a response to the user's task.\n2.  **Critique**: Agent (or a separate Critic agent) compares the draft against the **Constitution**.\n3.  **Refine**: Agent rewrites the draft to address the critique.\n\nThis allows the agent to fix violations (e.g., \"I used `any` type, but the Constitution forbids it\") *before* the user ever sees the code.\n\n## Persona-Specific Constitutions\n\nDefining different Constitutions for different roles enables [Adversarial Code Review](/patterns/adversarial-code-review).\n\n### 1. The Builder (Optimist)\n> \"Your goal is to be helpful and productive. Write code that solves the user's problem. If the spec is slightly vague, make a reasonable guess to keep momentum going. Prioritize clean, readable implementation.\"\n\n### 2. The Critic (Pessimist)\n> \"Your goal is to be a skeptical gatekeeper. Assume the code is broken or insecure until proven otherwise. Do not be helpful; be accurate. If the spec is vague, reject the code and demand clarification. Prioritize correctness and edge-case handling.\"\n\nBy running the same prompt through these two different Constitutions, you generate a dialectic process that uncovers issues a single \"neutral\" agent would miss.\n\n## Implementation\n\n### 1. Documentation\nThe industry standard for documenting your Agent Constitution is [AGENTS.md](/practices/agents-md-spec). This file lives in your repository root and serves as the source of truth for your agents.\n\n### 2. Injection\nInject the Constitution into the **System Prompt** of your LLM interaction.\n*   **System Prompt**: `{{AGENT_CONSTITUTION}} \\n\\n You are an AI assistant...`\n*   **User Prompt**: `{{TASK_SPEC}}`\n\n### 3. Tuning\nConstitutions must be tuned. If they are too strict, the agent becomes paralyzed (refusing to code because \"it might be insecure\"). If too loose, the agent halts for every minor ambiguity.\n\n**The \"Be Good\" Trap**: Avoid vague directives like \"Write good code.\"\n*   *Bad*: \"Be secure.\"\n*   *Good*: \"Never concatenate strings to build SQL queries. Use parameterized queries only.\"\n\n## Relationship to Other Patterns\n\n**[Constitutional Review](/patterns/constitutional-review)** — The pattern for using a Critic agent to review code specifically against the Agent Constitution.\n\n**[Context Gates](/patterns/context-gates)** — The deterministic checks that back up the probabilistic Constitution. Hard Constraints implemented via orchestration.\n\n**[Adversarial Code Review](/patterns/adversarial-code-review)** — Uses persona-specific Constitutions (Builder vs Critic) to create dialectic review processes.\n\n**[The Spec](/patterns/the-spec)** — Defines task-specific requirements, while the Constitution defines global behavioral guidelines.\n\n**[AGENTS.md Specification](/practices/agents-md-spec)** — The practice for documenting and maintaining your Agent Constitution.\n\n**[Workflow as Code](/practices/workflow-as-code)** — Implements Hard Constraints programmatically, complementing the Constitution's Steering Constraints.\n\nSee also:\n- [Constitutional Review](/patterns/constitutional-review) — Enforcement via Critic agents\n- [Context Gates](/patterns/context-gates) — Hard constraint implementation\n- [AGENTS.md Specification](/practices/agents-md-spec) — Documentation practice\n\n### Related Concepts\n- [OODA Loop](/concepts/ooda-loop) — The Constitution operates in the Orient phase\n- [Context Engineering](/concepts/context-engineering) — The broader discipline of managing prompt context",
    "tags": ["Agent Architecture", "System Prompts", "Alignment", "Governance"],
    "references": [
      {
        "type": "paper",
        "title": "Constitutional AI: Harmlessness from AI Feedback",
        "url": "https://arxiv.org/abs/2212.08073",
        "publisher": "Anthropic",
        "annotation": "Seminal paper by Bai et al. (2022) defining the 'Helpful, Honest, Harmless' framework and the RLAIF method."
      },
      {
        "type": "website",
        "title": "System Prompts",
        "url": "https://docs.anthropic.com/en/docs/system-prompts",
        "publisher": "Anthropic",
        "accessed": "2026-01-13T00:00:00.000Z",
        "annotation": "Defines the model for Constitutional AI: training a harmless assistant via self-critique based on a constitution."
      },
      {
        "type": "website",
        "title": "Intent Engineering Framework for AI Agents",
        "url": "https://www.productcompass.pm/p/intent-engineering-framework-for-ai-agents",
        "author": "Paweł Huryn",
        "published": "2024-04-30T00:00:00.000Z",
        "accessed": "2026-01-19T00:00:00.000Z",
        "annotation": "Provides the taxonomy for 'Steering Constraints' (Constitution) vs 'Hard Constraints' (Orchestration/Hook)."
      }
    ]
  },
  {
    "slug": "agentic-double-diamond",
    "collection": "patterns",
    "title": "Agentic Double Diamond",
    "description": "A computational framework transforming the classic design thinking model into an executable pipeline of context verification and assembly.",
    "status": "Experimental",
    "content": "## Definition\n\nThe **Agentic Double Diamond** is a computational framework that transforms the traditional design thinking model (Discover, Define, Develop, Deliver) into an executable pipeline where every phase produces machine-readable context rather than static artifacts.\n\n![Agentic Double Diamond Diagram](/images/agentic-double-diamond.svg)\n\nIn this model, the **Spec** becomes the primary source code, and \"Coding\" becomes an automated assembly step. The human role shifts from *Implementation* to *Context Engineering* and *Verification*.\n\n## The Problem: Lossy Handoffs\n\nTraditional software development suffers from signal degradation at every handoff:\n\n1.  **The \"Gap of Silence\"**: Insights from the *Discover* phase are summarized into PowerPoints or tickets, stripping away the raw evidence needed for edge-case validation.\n2.  **Static Deliverables**: The *Define* phase produces Figma files or flat requirements. To an AI, these are unstructured blobs. Use of \"Vibe Coding\" creates functionality that feels right but fails under rigorous scrutiny.\n3.  **Verification Lag**: We typically only verify if we built the *thing right* (Testing) after weeks of coding. We rarely verify if we are building the *right thing* (Strategy) until it's too late.\n\nThe result is a \"Build Trap\" where we efficiently ship features that solve the wrong problems.\n\n## The Solution: A Computational Pipeline\n\nThe Agentic Double Diamond reimagines the two diamonds not as workshop phases, but as **Context Furnaces**. Each furnace ingests raw, unstructured input and refines it into a stricter, more deterministic state.\n\n*   **Diamond 1 (Problem Space):** Ingests Chaos $\\rightarrow$ Refines to **Insight**.\n*   **Diamond 2 (Solution Space):** Ingests Insight $\\rightarrow$ Refines to **Implementation**.\n\nCrucially, we introduce **Adversarial Gates** at the convergence points of each diamond to stop \"Solution Pollution\"—the tendency to rush into building without a valid problem definition.\n\n## Anatomy\n\nThe pattern consists of four computational phases and one operational phase (Run).\n\n### Phase 1: DISCOVER (The Sensor Network)\n*From Chaos to Signal.*\n\nInstead of manual research sprints, we use agents to ingest broad signals (user feedback, logs, market data) and cluster them into patterns.\n\n**Context Output:** `Problem Graph` (A structured map of user needs and pain points).\n\n*   **Practices:**\n    *   **[Experience Modeling](/patterns/experience-modeling)**: Defining the domain language and user journey.\n    *   **[Context Engineering](/concepts/context-engineering)**: Structuring the raw input for analysis.\n\n### Phase 2: DEFINE (The Strategy Engine)\n*From Signal to Insight.*\n\nWe crystallize the signals into a coherent strategy. This is where **Product Thinking** applies constraint satisfaction to select the *right* problem to solve.\n\n**Human Role:** **Thought Leader** (Deciding what matters).\n**Agent Role:** **Thought Partner** (Challenging assumptions).\n\n**Context Output:** `Strategy Document` & `Validated Problem Statement`.\n\n*   **Gate 1 (The Checkpoint):** **[Adversarial Requirement Review](/practices/adversarial-requirement-review)**.\n    *   Before writing a single line of a Spec, an Adversarial Agent challenges the strategy constraints. If it fails, we loop back to Discover.\n\n### Phase 3: SPEC (The New Coding)\n*From Insight to Blueprint.*\n\nThis is the most significant shift. In the Agentic SDLC, **Spec Writing IS Coding**. The Spec is the permanent, living source of truth. It defines the \"What\" (Behavior) and the \"How\" (Architecture) in a format rigorous enough for agents to execute.\n\n**Context Output:** **[The Spec](/patterns/the-spec)** (Context, Blueprint, Contract).\n\n*   **Practices:**\n    *   **[Spec-Driven Development](/concepts/spec-driven-development)**: The methodology of writing specs first.\n    *   **[Living Specs](/practices/living-specs)**: Treating documentation as code.\n\n### Phase 4: ASSEMBLE (The Agentic Manufactory)\n*From Blueprint to Assembly.*\n\nAgents ingest the Spec and \"assemble\" the implementation. This phase is highly automated. The agents generate code, tests, and documentation that adhere strictly to the Spec.\n\n**Human Role:** Verifier (Reviewing the assembly against the Spec).\n**Agent Role:** Builder (Implementation).\n\n**Context Output:** `Source Code`, `Tests`, `Micro-Commits`.\n\n*   **Gate 2 (The Checkpoint):** **[Adversarial Code Review](/practices/adversarial-code-review)**.\n    *   An independent Critic Agent verifies the assembled code against the Spec's Contracts. It catches edge cases and architectural violations that unit tests might miss.\n    *   See also: **[Micro-Commits](/practices/micro-commits)** and **[Feature Assembly](/practices/feature-assembly)**.\n\n### Phase 5: RUN (The Feedback Loop)\n*From Assembly to Signal.*\n\nThe software operates in production, generating new signals (usage data, errors, feedback) that feed back into Phase 1, closing the loop.\n\n*   **Practices:**\n    *   **[Feedback Loop Compression](/concepts/feedback-loop-compression)**: Minimizing the time between \"Run\" and \"Discover\".\n    *   **[Production Readiness Gap](/concepts/production-readiness-gap)**: Managing the transition from prototype to production.\n\n## Relationship to Other Patterns\n\n*   **[Product Thinking](/concepts/product-thinking)**: The mindset that drives the *Discover/Define* phases.\n*   **[The Spec](/patterns/the-spec)**: The central artifact connecting *Define* to *Assemble*.\n*   **[Agent Constitution](/patterns/agent-constitution)**: The set of laws that govern agent behavior throughout the pipeline.\n*   **[Context Gates](/concepts/context-gates)**: The architectural pattern implemented by the Adversarial Reviews.\n\n## Anti-Patterns\n\n### The Vibe Coding Shortcut\n**Problem:** Skipping the *Define* and *Spec* phases to jump straight to *Assemble* (Vibe Coding).\n**Consequence:** Fast \"sugar-high\" shipping of features that crumble under production complexity because they lack structural integrity.\n\n### The Static Spec\n**Problem:** Treating Phase 3 as a \"PDF generation\" step.\n**Consequence:** The Spec drifts from reality immediately. In this pattern, the Spec must be a **Living Spec** in the repo, or the automated assembly fails.",
    "tags": ["Design", "Methodology", "Requirements", "Agents", "Architecture"],
    "references": [
      {
        "title": "The Double Diamond",
        "author": "Design Council",
        "url": "https://www.designcouncil.org.uk/our-work/skills-learning/tools-frameworks/framework-for-innovation-design-council-s-evolved-double-diamond/",
        "type": "website",
        "annotation": "Origin of the Diverge-Converge model.",
        "accessed": "2026-02-12T00:00:00.000Z"
      },
      {
        "title": "Software Craftsmanship in the AI Era",
        "author": "Codurance",
        "url": "https://www.codurance.com/",
        "type": "website",
        "annotation": "Source of the 'Spec is Code' philosophy.",
        "accessed": "2026-02-12T00:00:00.000Z"
      },
      {
        "title": "Before I Ask AI to Build, I Ask It to Challenge",
        "author": "Daniel Donbavand",
        "url": "https://danieldonbavand.com/2026/02/12/before-i-ask-ai-to-build-i-ask-it-to-challenge/",
        "type": "website",
        "annotation": "Source of the Adversarial Requirement Review pattern.",
        "accessed": "2026-02-12T00:00:00.000Z"
      }
    ]
  },
  {
    "slug": "constitutional-review",
    "collection": "patterns",
    "title": "Constitutional Review",
    "description": "Verification pattern that validates implementation against both functional requirements (Spec) and architectural values (Constitution).",
    "status": "Live",
    "content": "## Definition\n\n**Constitutional Review** is a verification pattern that validates code against two distinct contracts:\n\n1. **The Spec** (functional requirements) — Does it do what was asked?\n2. **The Constitution** (architectural values) — Does it do it *the right way*?\n\nThis pattern extends [Adversarial Code Review](/patterns/adversarial-code-review) by adding a second validation layer. Code can pass all tests and satisfy the Spec's functional requirements while still violating the project's architectural principles documented in the [Agent Constitution](/patterns/agent-constitution).\n\n## The Problem: Technically Correct But Architecturally Wrong\n\nStandard verification catches functional bugs:\n- **Tests**: Does the code produce expected outputs?\n- **Spec Compliance**: Does it implement all requirements?\n- **Type Safety**: Does it compile without errors?\n\nBut code can pass all these checks and still violate architectural constraints:\n\n**Example: The Performance Violation**\n\n```typescript\n// Spec requirement: \"Filter audit logs by date range\"\nasync function getAuditLogs(startDate: Date) {\n  const logs = await db.auditLogs.findAll(); // ❌ Loads entire table\n  return logs.filter(log => log.date > startDate); // ❌ Filters in memory\n}\n```\n\n**Quality Gates**: ✅ Tests pass (small dataset)  \n**Spec Compliance**: ✅ Returns filtered logs  \n**Constitutional Review**: ❌ Violates \"push filtering to database layer\"\n\nThe code is **functionally correct** but **architecturally unsound**. It works fine with 100 records but fails catastrophically at 10,000+.\n\n## The Solution: Dual-Contract Validation\n\nConstitutional Review solves this by validating against **two sources of truth**:\n\n### Traditional Review (Functional)\n- **Input**: Spec + Code Diff\n- **Question**: \"Does the code implement the requirements?\"\n- **Validates**: Functional correctness\n\n### Constitutional Review (Architectural)\n- **Input**: Constitution + Spec + Code Diff\n- **Question**: \"Does the code exhibit our architectural values?\"\n- **Validates**: Architectural consistency\n\nThe Critic Agent validates against BOTH contracts:\n1. **Functional correctness** (from the Spec)\n2. **Architectural consistency** (from the Constitution)\n\n## Anatomy\n\nConstitutional Review consists of three key components:\n\n### The Dual-Contract Input\n\n**Spec Contract** — Defines functional requirements, API contracts, and data schemas. Answers \"what should it do?\"\n\n**Constitution Contract** — Defines architectural patterns, performance constraints, and security rules. Answers \"how should it work?\"\n\nBoth contracts are fed to the Critic Agent for validation.\n\n### The Critic Agent\n\nA secondary AI session (ideally using a reasoning-optimized model) that:\n\n- Reads both the Spec and the Constitution\n- Compares implementation against both contracts\n- Identifies where code satisfies functional requirements but violates architectural principles\n- Provides structured violation reports with remediation paths\n\nThis extends the [Adversarial Code Review](/patterns/adversarial-code-review) Critic with constitutional awareness.\n\n### The Violation Report\n\nWhen constitutional violations are detected, the Critic produces:\n\n1. **Violation Description** — What constitutional principle was violated\n2. **Impact Analysis** — Why this matters at scale (performance, security, maintainability)\n3. **Remediation Path** — Ordered steps to fix (prefer standard patterns, escalate if needed)\n4. **Test Requirements** — What tests would prevent regression\n\nThis transforms review from rejection to guidance.\n\n## Relationship to Other Patterns\n\n**[Adversarial Code Review](/patterns/adversarial-code-review)** — The base pattern that Constitutional Review extends. Adds the Constitution as a second validation contract.\n\n**[Agent Constitution](/patterns/agent-constitution)** — The source of architectural truth. Defines the \"driver training\" that shapes initial behavior; Constitutional Review verifies the training was followed.\n\n**[The Spec](/patterns/the-spec)** — The source of functional truth. Constitutional Review validates against both Spec and Constitution.\n\n**[Context Gates](/patterns/context-gates)** — Constitutional Review implements a specialized Review Gate that validates architectural consistency.\n\n**Feedback Loop**: Constitution shapes behavior → Constitutional Review catches violations → Violations inform Constitution updates (if principles aren't clear enough).\n\n## Integration with Context Gates\n\nConstitutional Review implements a specialized [Review Gate](/patterns/context-gates) that sits between Quality Gates and Acceptance Gates:\n\n| Gate Type | Question | Validated By |\n|-----------|----------|--------------|\n| Quality Gates | Does it compile and pass tests? | Toolchain (deterministic) |\n| Spec Review Gate | Does it implement requirements? | Critic Agent (probabilistic) |\n| **Constitutional Review Gate** | **Does it follow principles?** | **Critic Agent (probabilistic)** |\n| Acceptance Gate | Is it the right solution? | Human (subjective) |\n\nThe Constitutional Review Gate catches architectural violations that pass functional verification.\n\n## Strategic Value\n\n**Catches \"Regression to Mediocrity\"** — LLMs are trained on average code from the internet. Without constitutional constraints, they gravitate toward common but suboptimal patterns.\n\n**Enforces Institutional Knowledge** — Architectural decisions (performance patterns, security rules, error handling strategies) are documented once in the Constitution and verified on every implementation.\n\n**Surfaces Specification Gaps** — If the Critic can't determine whether code violates constitutional principles, the Constitution needs clarification. This improves the entire system.\n\n**Reduces L3 Review Burden** — Human reviewers focus on strategic fit (\"Is this the right feature?\") rather than catching architectural violations (\"Why are you loading the entire table?\").\n\n**Prevents Silent Failures** — Code that \"works\" but violates architectural principles (like the LoadAll().Filter() anti-pattern) is caught before production.\n\n## Validated in Practice\n\n**Case Study: Claudio Lassala (January 2026)**\n\nA production implementation caught a constitutional violation that passed all other gates:\n\n**Context**: User story required filtering audit logs by date range. Builder Agent implemented the requirement, tests passed, code compiled without errors.\n\n**Code Behavior**:\n- Loaded entire audit log table into memory\n- Filtered in-memory using LINQ/collection methods\n\n**Gate Results**:\n- **Quality Gates**: ✅ Passed (compiled, tests passed with small dataset)\n- **Spec Compliance**: ✅ Passed (functional requirement met: returns filtered logs)\n- **Constitutional Review**: ❌ **FAILED** (violated \"push filtering to database layer\")\n\n**Critic Output**: Provided specific remediation path:\n1. Push filter to database query layer\n2. If ORM doesn't support pattern, use raw SQL\n3. Add performance test with 10k+ records\n4. Document constraint in repository interface\n\n**Impact**: Silent performance bug caught before production. The code worked perfectly in development (small dataset) but would have failed catastrophically at scale.\n\nSee full case study in [Adversarial Code Review](/patterns/adversarial-code-review).\n\n## Implementing Practice\n\nFor step-by-step implementation guidance, see:\n\n- [Constitutional Review Implementation](/practices/constitutional-review-implementation) — How to configure Critic Agent prompts, document architectural constraints, and integrate with your workflow\n\nSee also:\n- [Adversarial Code Review](/patterns/adversarial-code-review) — The base pattern this extends\n- [Agent Constitution](/patterns/agent-constitution) — The source of architectural truth\n- [The Spec](/patterns/the-spec) — The source of functional truth\n- [Context Gates](/patterns/context-gates) — The architectural checkpoint system\n- [Agentic SDLC](/concepts/agentic-sdlc) — The verification phase where this operates\n- [Context Engineering](/concepts/context-engineering) — How to structure constitutional constraints for LLMs",
    "tags": ["Code Review", "Architecture", "Agent Constitution", "Quality Gates", "Verification"],
    "references": [
      {
        "type": "website",
        "title": "A Method for AI-Assisted Pull Request Reviews: Aligning Code with Business Value",
        "url": "https://lassala.net/2026/01/05/a-method-for-ai-assisted-pull-request-reviews-aligning-code-with-business-value/",
        "author": "Claudio Lassala",
        "published": "2026-01-05T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Production validation showing constitutional violations caught after passing quality gates, demonstrating real-world effectiveness of this pattern."
      }
    ]
  },
  {
    "slug": "context-gates",
    "collection": "patterns",
    "title": "Context Gates",
    "description": "Architectural checkpoints that filter input context and validate output artifacts between phases of work to prevent cognitive overload and ensure system integrity.",
    "status": "Experimental",
    "content": "## Definition\n\n**Context Gates** are architectural checkpoints that sit between phases of agentic work. They serve a dual mandate: filtering the **input** context to prevent cognitive overload, and validating the **output** artifacts to ensure system integrity.\n\nUnlike \"Guardrails,\" which conflate prompt engineering with hard constraints, Context Gates are distinct, structural barriers that enforce contracts between agent sessions and phases.\n\n## The Problem: Context Pollution and Unvalidated Outputs\n\nWithout architectural checkpoints, agentic systems suffer from two critical failures:\n\n**Context Pollution** — Agents accumulate massive conversation histories (observations, tool outputs, internal monologues, errors). When transitioning between sessions or tasks, feeding the entire context creates cognitive overload. Signal-to-noise ratio drops, and agents lose focus on the current objective—Nick Tune describes this as the agent becoming **\"tipsy wobbling from side-to-side.\"**\n\n**Unvalidated Outputs** — Code that passes automated tests can still violate semantic contracts (spec requirements, architectural constraints, security policies). Without probabilistic validation layers, implementation shortcuts and silent failures slip through to production.\n\n**Why Existing Approaches Fail:**\n- **Single-pass validation** (tests only) misses semantic violations\n- **No context compression** between sessions creates confusion\n- **Flat quality gates** don't distinguish deterministic checks from probabilistic review\n\n## The Solution: Dual-Mandate Checkpoint Architecture\n\nContext Gates solve this by creating **two distinct checkpoint types**:\n\n**Input Gates** — Filter and compress context *entering* an agent session, ensuring only relevant information is presented. This prevents cognitive overload and maintains task focus.\n\n**Output Gates** — Validate artifacts *leaving* an agent session through three tiers of verification: deterministic checks, probabilistic review, and human acceptance.\n\nThe key insight: **Context must be controlled at the boundaries**, not throughout execution. Agents work freely within their session, but transitions enforce strict contracts.\n\n## Anatomy\n\nContext Gates consist of two primary structures, each with distinct sub-components:\n\n### Input Gates\n\nInput Gates control what context enters an agent session.\n\n#### Summary Gates (Cross-Session Transfer)\nWhen transitioning work between agent sessions, Summary Gates compress conversation history into essential state.\n\n- **Type:** LLM-Assisted Summarization\n- **Nature:** Compression / Filtering\n- **Function:** Extract key decisions, discard intermediate reasoning\n- **Outcome:** Clean handoff without context overflow\n\n**Examples:**\n- Design session → Implementation: Extract design decisions, discard exploration paths\n- Bug investigation → Fix: Compress to \"root cause + attempted fixes\"\n- Code review → Revision: Distill to actionable feedback list\n\n#### Context Filtering (Within-Session)\nDuring multi-step tasks within a single session, Context Filtering determines what historical information is relevant to the current sub-task.\n\n- **Type:** Semantic Search / Lightweight Agent\n- **Nature:** Relevance Filtering\n- **Function:** High signal-to-noise ratio for current decision\n- **Outcome:** Precision and low latency\n\n### Output Gates\n\nOutput Gates validate artifacts before they progress to the next phase. Three tiers enforce different types of correctness:\n\n#### Quality Gates (Deterministic)\nBinary, automated checks enforced by the toolchain.\n\n- **Type:** Machine / Toolchain\n- **Nature:** Deterministic (Pass/Fail)\n- **Question:** \"Does it compile and pass tests?\"\n- **Enforcement:** Instant rejection if failed; often triggers self-correction\n\n**Examples:**\n- Syntax & type safety (TypeScript compilation, Zod validation)\n- Linting rules (ESLint, accessibility checks)\n- Unit/E2E test passage\n- Build artifact generation\n\n#### Review Gates (Probabilistic, Adversarial)\nLLM-assisted validation of semantic correctness and contract compliance.\n\n- **Type:** Secondary AI Session (Critic Agent)\n- **Nature:** Probabilistic / Adversarial\n- **Question:** \"Does it satisfy the Spec's contracts?\"\n- **Implementation:** [Adversarial Code Review](/patterns/adversarial-code-review), [Constitutional Review](/patterns/constitutional-review)\n\n**Examples:**\n- Spec compliance (all requirements implemented)\n- Anti-pattern detection (architectural constraint violations)\n- Edge case coverage (error paths, race conditions)\n- Security review (injection vulnerabilities, auth bypasses)\n\n**Output Format:**\nWhen violations are detected, Review Gates provide actionable feedback:\n\n1. **Violation Description** — What contract was broken\n2. **Impact Analysis** — Why this matters (performance, security, maintainability)\n3. **Remediation Path** — Ordered list of fixes (prefer standard patterns, escalate if needed)\n4. **Test Requirements** — What tests would prevent regression\n\nThis transforms Review Gates from \"reject\" mechanisms into \"guide to resolution\" checkpoints.\n\n#### Acceptance Gates (Human-in-the-Loop)\nSubjective checks requiring human strategic judgment.\n\n- **Type:** Human Review (HITL)\n- **Nature:** Subjective / Strategic\n- **Question:** \"Is it the right thing?\"\n- **Purpose:** Ensure solution solves actual user problem and aligns with product vision\n\n**Examples:**\n- Brand tone check (does copy sound like us?)\n- UX review (does interaction feel smooth?)\n- Visual QA (are spacings and layout visually balanced?)\n- Strategic fit (does this feature solve the user's problem?)\n\n#### Workflow Enforcement (Denial Gates)\nMechanisms that actively **block** agents from bypassing the defined process.\n\n- **Type:** Client-Side Hook / Middleware\n- **Nature:** Prevention\n- **Question:** \"Are you allowed to do this?\"\n- **Enforcement:** Hard block (exit code 1)\n\n**Examples:**\n- `git push` hook: \"Blocked. You must use the `submit-pr` tool which runs verification first.\"\n- Shell wrapper: Prevents agent from accessing production database credentials.\n\n\n## Gate Taxonomy\n\n| Feature | Summary Gates (Input) | Context Filtering (Input) | Quality Gates (Output) | Review Gates (Output) | Acceptance Gates (Output) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Function** | Session handoff | Within-session filtering | Code validity | Spec compliance | Strategic fit |\n| **Goal** | Clean session transfer | Maintain focus | Prevent broken code | Enforce contracts | Prevent bad product |\n| **Mechanism** | LLM Summarization | Semantic Search | Compilers / Tests | LLM Critique | Human Review |\n| **Nature** | Compression | Filtering | Deterministic | Probabilistic | Subjective |\n| **Outcome** | Condensed context | Clean context window | Valid compilation | Spec compliance | Approved release |\n\n## Relationship to Other Patterns\n\n**[Adversarial Code Review](/patterns/adversarial-code-review)** — Implements the Review Gate tier of Output Gates. Uses a Critic Agent to validate code against the Spec's contracts.\n\n**[Constitutional Review](/patterns/constitutional-review)** — Extends Review Gates by validating against both the Spec (functional) and the Agent Constitution (architectural values).\n\n**[Model Routing](/patterns/model-routing)** — Works with Context Gates to assign appropriate model capabilities to different gate types (throughput models for generation, reasoning models for Review Gates).\n\n**[The Spec](/patterns/the-spec)** — Provides the contract that Review Gates validate against.\n\n**[Agent Constitution](/patterns/agent-constitution)** — Provides architectural constraints that Constitutional Review validates against.\n\n**[Ralph Loop](/patterns/ralph-loop)** — Applies Context Gates at iteration boundaries, using context rotation and progress files to prevent cognitive overload across autonomous loops.\n\n**[Feature Assembly](/practices/feature-assembly)** — The practice that uses all three Output Gates (Quality, Review, Acceptance) in the verification pipeline.\n\n**[Workflow as Code](/practices/workflow-as-code)** — The practice for implementing gate enforcement programmatically rather than via prompt instructions.\n\n## Strategic Value\n\n**Prevents Context Overload** — Agents receive only relevant information, maintaining task focus and reducing token usage.\n\n**Catches Semantic Violations** — Review Gates detect contract violations that pass deterministic checks (performance anti-patterns, security gaps, missing edge cases).\n\n**Reduces Human Review Burden** — Quality and Review Gates filter out obvious errors, letting humans focus on strategic fit rather than technical correctness.\n\n**Enforces Architectural Consistency** — Constitutional Review (via Review Gates) ensures code follows project principles, not just internet-average patterns.\n\n**Creates Clear Contracts** — Each gate type has explicit pass/fail criteria, making verification deterministic where possible and explicit where probabilistic.\n\nSee also:\n- [Adversarial Code Review](/patterns/adversarial-code-review) — Review Gate implementation\n- [Constitutional Review](/patterns/constitutional-review) — Dual-contract Review Gate\n- [The Spec](/patterns/the-spec) — Contract source for Review Gates\n- [Agent Constitution](/patterns/agent-constitution) — Architectural constraint source\n- [Model Routing](/patterns/model-routing) — Model selection for different gate types\n\n### Related Concepts\n- [Agentic SDLC](/concepts/agentic-sdlc) — The lifecycle where gates create phase boundaries\n- [Context Engineering](/concepts/context-engineering) — The practice of structuring context\n- [Guardrails](/concepts/guardrails) — Disambiguated term this pattern replaces",
    "tags": ["Architecture", "Quality Gates", "Context Engineering", "Validation"],
    "references": [
      {
        "type": "website",
        "title": "A Method for AI-Assisted Pull Request Reviews",
        "url": "https://lassala.net/2026/01/05/a-method-for-ai-assisted-pull-request-reviews-aligning-code-with-business-value/",
        "author": "Claudio Lassala",
        "published": "2026-01-05T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Production implementation validating Review Gates' effectiveness in catching architectural violations through adversarial review."
      },
      {
        "type": "website",
        "title": "Dev Workflows as Code",
        "url": "https://medium.com/nick-tune-tech-strategy-blog/dev-workflows-as-code-fab70d44b6ab",
        "author": "Nick Tune",
        "published": "2026-01-16T00:00:00.000Z",
        "accessed": "2026-01-18T00:00:00.000Z",
        "annotation": "Validates the need for deterministic orchestration and hard enforcement boundaries to prevent 'tipsy wobbling' agents."
      }
    ]
  },
  {
    "slug": "experience-modeling",
    "collection": "patterns",
    "title": "Experience Modeling",
    "description": "The practice of treating the Design System as a formal schema that agents must strictly follow, preventing UI hallucinations.",
    "status": "Live",
    "content": "## Definition\n\n**Experience Modeling** is the creation of a queryable **Experience Schema**—a rigid Design System that serves as the source of truth for all frontend generation.\n\nJust as we model data schemas (SQL/Prisma) to constrain backend agents, Experience Modeling restricts frontend agents to a validated set of UI components, tokens, and layouts. It treats the Design System not as a library of suggestions, but as a strict contract.\n\n## The Problem: Design Drift\n\nWithout a formal Experience Model, agents suffer from **Design Drift**—the gradual divergence of a product's UI from its intended design specifications.\n\nThis occurs because LLMs are probabilistic \"vibe engines.\" When asked to \"make a blue button,\" an agent might:\n- Generate raw CSS (`background-color: #007bff`) instead of using tokens (`var(--color-primary)`)\n- Hallucinate new component variants that don't exist\n- Inconsistently apply spacing and typography\n\nOver hundreds of commits, these micro-inconsistencies accumulate into a codebase that is technically functional but visually chaotic and impossible to maintain.\n\n## The Solution: The Experience Schema\n\nThe solution is to formalize the UI as an **Experience Schema**—a strict, machine-readable definition of valid UI states.\n\nInstead of asking the agent to \"design a page,\" we force it to \"assemble a page using only these approved blocks.\" This shifts the agent's role from **Artist** (creating new styles) to **Builder** (assembling pre-built parts).\n\n## Anatomy\n\n### 1. The Component Catalog (The Vocabulary)\nThe \"words\" the agent is allowed to use. This is a set of dumb, stateless UI components (Buttons, Inputs, Cards) that strictly enforce brand styles. These components must be:\n- **Self-Contained**: Encapsulate all styling logic.\n- **Typed**: Export clear TypeScript interfaces.\n- **Documented**: exposed via `llms.txt` or similar context files.\n\n### 2. The Context Gate (The Enforcer)\nA mechanical barrier between Experience Modeling and [Feature Assembly](/practices/feature-assembly).\n\n```mermaid\n%% caption: Context Gating for Design System Integrity\nflowchart LR\n  A[[...]] --> |CONTEXT| C\n  C[EXPERIENCE MODELING] --> D\n  D{GATE} --> E\n  E[FEATURE ASSEMBLY]\n    E --> |DEFECT/REQUIREMENT SIGNAL| C\n    E --> |RELEASE| G\n  G[[...]]\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/experience-modeling-fig-1.svg\" alt=\"Context Gating for Design System Integrity\" />\n  <figcaption>Context Gating for Design System Integrity</figcaption>\n</figure>\n\nThe gate verifies:\n1.  **Token Strictness**: No raw CSS values (hex codes, magic numbers).\n2.  **Schema Parity**: Documentation matches code.\n3.  **Build Success**: The Design System builds in isolation.\n\n### 3. Read-Only Enforcement (The Governance)\nDuring Feature Assembly, the Experience Model must be **Read-Only**. Agents cannot modify the definition of a \"Button\" to make a feature work; they must use the Button as it exists or request a change to the model.\n\n**Pattern A: Hard Isolation (Enterprise)**\nThe Design System is a separate package (NPM/NuGet) installed as a dependency. The agent literally cannot modify source files because they are in `node_modules`.\n\n**Pattern B: Toolchain Enforcement (Startups)**\nThe Design System lives in the same repo, but `pre-commit` hooks or `CODEOWNERS` files prevent the agent from modifying `src/design-system/**` without explicit human override.\n\n## Relationship to Other Patterns\n\n**[Context Gates](/patterns/context-gates)** — Experience Modeling implements a specific type of Context Gate: the \"Design Integrity Gate.\"\n\n**[Feature Assembly](/practices/feature-assembly)** — The phase that *consumes* the Experience Model. Feature Agents assume the Experience Model is immutable context.\n\n**[Agent Personas](/practices/agent-personas)** — We often use a specific \"Systems Architect\" or \"Designer\" persona for the Experience Modeling phase, distinct from the \"Feature Developer\" persona.",
    "tags": ["ASDLC", "Design System", "Methodology"],
    "references": []
  },
  {
    "slug": "model-routing",
    "collection": "patterns",
    "title": "Model Routing",
    "description": "Strategic assignment of LLM models to SDLC phases based on reasoning capability versus execution speed.",
    "status": "Live",
    "content": "## Definition\n\n**Model Routing** is the strategic assignment of different Large Language Models (LLMs) to different phases or tasks based on their capability profile.\n\nIn a monolithic architecture, a user asking for a simple boolean definition incurs the same high cost and latency as a user requesting a complex strategic analysis. Model routing rationalizes this by shifting model selection from a design-time decision to a runtime optimization problem.\n\n## The Iron Triangle\n\nEffective routing systems operate by manipulating the trade-offs between three competing constraints:\n\n1.  **Quality**: Semantic accuracy, reasoning depth, instruction following.\n2.  **Cost**: Operational expenditure (OpEx) per token.\n3.  **Latency**: Time-To-First-Token (TTFT) and total generation time.\n\nBy dynamically swapping models, routers decouple these variables. A system can achieve \"frontier-class\" average quality at \"efficient-class\" average cost by routing only the most difficult 10-20% of queries to the expensive model.\n\n## Taxonomy of Routing Architectures\n\nWe identify five primary patterns for implementing model routing:\n\n### 1. Semantic Routing (Embedding-Based)\nUses vector similarity to map broad intents to specific routes.\n*   **Mechanism**: Encoder $\\rightarrow$ Vector Search $\\rightarrow$ Threshold Check.\n*   **Use Case**: RAG topic selection, intent classification.\n\n### 2. Predictive Routing (Classifier-Based)\nUses a trained classifier (Bert, XGBoost, or Matrix Factorization like RouteLLM) to predict the probability that a weak model can successfully answer the query.\n*   **Mechanism**: `P(Success|WeakModel) > Threshold ? Weak : Strong`.\n*   **Use Case**: General purpose query optimization.\n\n### 3. Cascading Routing (Waterfall)\nA \"fail-up\" pattern that prioritizes cost.\n*   **Mechanism**: Try Weak Model $\\rightarrow$ Validation Gate (Low Confidence?) $\\rightarrow$ Strong Model.\n*   **Use Case**: Code generation where syntax errors can trigger escalation.\n\n### 4. Probabilistic Routing (Contextual Bandits)\nUses Reinforcement Learning to adapt routing weights based on user feedback or judge evaluation.\n*   **Use Case**: High-scale production systems with drifting query distributions.\n\n### 5. Agentic Routing (Tool Use)\nStructural routing where a dispatcher agent utilizes tools to delegate work.\n*   **Mechanism**: LLM outputs structured JSON choice (e.g., `{\"tool\": \"sql_agent\"}`).\n*   **Use Case**: Complex multi-step workflows.\n\n## Anatomy\n\nA complete routing system consists of three components:\n\n### 1. The Model Registry\nA configuration defining the available models and their capabilities.\n*   **Strong/Frontier**: High reasoning, expensive (e.g., Claude 3.5 Sonnet, GPT-4o, DeepSeek V3).\n*   **Weak/Efficient**: High speed, cheap (e.g., Haiku, Llama-3-8B, GPT-4o-mini).\n*   **Specialist**: Domain-optimized (e.g., StarCoder for SQL, Med-PaLM).\n\n### 2. The Router (Gateway vs. Application)\n*   **Gateway Layer**: Centralized proxy (e.g., LiteLLM, Cloudflare AI Gateway). Handles auth, rate limits, and simple rule-based routing.\n*   **Application Layer**: Library-based logic (e.g., LangChain RunnableBranch). Handles logic requiring deep context (session history, variable state).\n\n### 3. The Calibration\nThe specific thresholds or weights used to make decisions. These must be tuned against a \"Preference Dataset\" (pairs of queries and optimal model choices).\n\n## Operational Economics\n\n### The Sweet Spot\n\n**LLMs excel at:**\n*   High ambiguity tasks requiring interpretation\n*   Generation of novel content\n*   Format/language transformation\n\n**Use deterministic code for:**\n*   Hot paths requiring <100ms response\n*   High-volume operations\n*   Binary correctness (auth, financial calculations)\n\n## Anti-Patterns\n\n### The Monolith\n**Description**: Reliance on a single \"Frontier\" model for all tasks.\n**Consequence**: Excessive cost and latency for simple tasks; inability to scale.\n\n### Silent Drift\n**Description**: Hard-coded routing rules (e.g., \"if length > 50\") that degrade as user behavior changes.\n**Consequence**: Routing becomes incorrectly optimized, sending hard queries to weak models.\n**Fix**: Use probabilistic routing or periodic recalibration.\n\n### Context Stuffing\n**Description**: Overloading a single prompt with instructions instead of routing to specialized tools/agents.\n**Consequence**: \"Lost in the Middle\" phenomenon; higher hallucination rates.\n\n## Trade-offs\n\n| Dimension | Implications |\n| :--- | :--- |\n| **Latency Overhead** | The router itself adds latency (20-50ms for embeddings, 200ms+ for LLM routers). If the weak model saves 300ms but the router takes 400ms, you have negative ROI. |\n| **Complexity** | Maintaining a router adds a control plane that can fail. It requires monitoring and dataset maintenance. |\n| **Consistency** | Using multiple models can lead to inconsistent \"tone\" or formatting across a user session. |\n\n## Relationship to Levels of Autonomy\n\n[Levels of Autonomy](/concepts/levels-of-autonomy) define human oversight requirements. Model Routing matches computational capability to task characteristics:\n\n*   **Complex architectural decisions** (L3) $\\rightarrow$ High Reasoning models\n*   **Well-specified implementation tasks** (L3) $\\rightarrow$ High Throughput models\n*   **Exploratory analysis** (L2) $\\rightarrow$ Massive Context models\n\nApplied in:\n*   [Agentic SDLC](/concepts/agentic-sdlc) — Optimization of the factory floor.\n*   [Adversarial Code Review](/patterns/adversarial-code-review) — using different models for Builder vs Critic.",
    "tags": ["LLM Selection", "Context Engineering", "ASDLC", "Agent Architecture", "Economics"],
    "references": [
      {
        "type": "paper",
        "title": "RouteLLM: Learning to Route LLMs with Preference Data",
        "url": "https://arxiv.org/abs/2406.18665",
        "publisher": "LMSYS",
        "annotation": "Foundational research on using matrix factorization for routing, demonstrating Pareoto-optimal trade-offs."
      },
      {
        "type": "paper",
        "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
        "url": "https://arxiv.org/abs/2305.05176",
        "publisher": "Stanford University",
        "annotation": "Introduces the cascading routing architecture to minimize cost."
      },
      {
        "type": "website",
        "title": "My LLM Coding Workflow Going into 2026",
        "url": "https://addyo.substack.com/p/my-llm-coding-workflow-going-into",
        "author": "Addy Osmani",
        "published": "2026-01-01T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Addy Osmani's workflow guide emphasizing pragmatic model selection and mid-task model switching patterns based on reasoning needs."
      },
      {
        "type": "website",
        "title": "The Mythical LLM: Why Rumors of the Death of Software are Premature",
        "author": "Dan Cripe",
        "published": "2026-01-20T00:00:00.000Z",
        "url": "https://www.dancripe.com/ai-coding-enterprise-saas-reality-check/",
        "accessed": "2026-01-24T00:00:00.000Z",
        "annotation": "Documents latency, cost, and reliability constraints from enterprise practice."
      }
    ]
  },
  {
    "slug": "product-vision",
    "collection": "patterns",
    "title": "Product Vision",
    "description": "A structured vision document that transmits product taste and point-of-view to agents, preventing convergence toward generic outputs.",
    "status": "Live",
    "content": "## Definition\n\nA **Product Vision** is a structured artifact that captures the taste, personality, and point-of-view that makes a product *this product* rather than generic software. It transmits product intuition to agents who otherwise default to bland, safe, interchangeable outputs.\n\nTraditional vision documents are written for humans—investors, executives, new hires. In ASDLC, the Product Vision is structured for agent consumption, providing the context needed to make opinionated decisions aligned with product identity.\n\n## The Problem: Vibe Convergence\n\nAgents trained on the entire internet converge toward the mean. Ask for a landing page, you get the same hero section everyone else gets. Ask for onboarding, you get the same three-step wizard. Ask for error copy, you get \"Oops! Something went wrong.\"\n\nThis isn't a bug in the model. It's the model doing exactly what it's trained to do: produce the statistically average response. The average is safe. The average is forgettable.\n\n**The symptoms:**\n\n- Every feature spec reads like it was written for a different product\n- UI suggestions feel \"correct\" but lifeless\n- Copy has no voice—it could belong to any company\n- Agents optimize for conventional patterns over product-appropriate patterns\n- Design decisions lack opinion\n\nThe [Agent Constitution](/patterns/agent-constitution) tells agents *how to behave*. The [Spec](/patterns/the-spec) tells agents *what to build*. Neither tells agents *who we are*.\n\n## The Solution: Structured Taste Transmission\n\nThe Product Vision bridges this gap by making product identity explicit and agent-consumable. Rather than hoping agents infer taste from scattered references, the vision provides a structured context packet that shapes output quality.\n\nThe key insight: **agents don't need complete documentation—they need curated opinions**. A Product Vision isn't comprehensive; it's opinionated. It tells agents which tradeoffs to make when specs are ambiguous.\n\n## Anatomy\n\nA Product Vision consists of five components, each serving a distinct purpose in shaping agent output.\n\n### 1. The Actual Humans\n\nNot \"users\" or \"customers\"—real people with context, constraints, and taste of their own. This gives agents a *person* to design for, not an abstraction.\n\nWhen choosing between \"simple onboarding wizard\" and \"power-user defaults with optional setup,\" agents need basis for judgment. Abstract personas don't provide this; descriptions of actual humans do.\n\n### 2. Point of View\n\nOpinions. Actual stances on tradeoffs that reasonable people might disagree with.\n\nThese aren't requirements—they're *taste*. They tell agents which direction to lean when specs are ambiguous:\n\n- Dense information vs progressive disclosure\n- Keyboard-first vs mouse-first\n- Weird but memorable vs safe but forgettable\n- Ship incomplete but useful vs complete but late\n\n### 3. Taste References\n\nConcrete examples of products that feel right, and products that don't. Agents can reference these patterns directly: \"Make this feel more like Linear's approach to lists, less like Jira's.\"\n\nReferences provide calibration. Instead of describing \"clean\" in abstract terms, point to products that embody it—and products that don't.\n\n### 4. Voice and Language\n\nHow the product speaks. Not brand guidelines—actual examples of tone.\n\nThis includes:\n- What we say vs what we don't say\n- Error message patterns\n- Formality level\n- Personality markers (or deliberate lack thereof)\n\n### 5. Decision Heuristics\n\nWhen agents face ambiguous choices, what should they optimize for? These are tie-breakers—the rules that resolve conflicts between equally valid approaches.\n\n## Placement in Context Hierarchy\n\nProduct Vision sits between the Constitution and the Specs:\n\n| Tier | Artifact | Purpose |\n|------|----------|---------|\n| Constitution | `AGENTS.md` | How agents behave (rules, constraints) |\n| **Vision** | `VISION.md` or inline | Who the product is (taste, voice, POV) |\n| Specs | `/plans/*.md` | What to build (contracts, criteria) |\n| Reference | `/docs/` | Full documentation, API specs, guides |\n\nThe Constitution shapes *behavior*. The Vision shapes *judgment*. The Specs shape *output*.\n\nNot every project needs a separate `VISION.md`. For smaller products or early-stage teams, the vision can live as a preamble in `AGENTS.md`. For complex products with detailed voice guidelines and taste references, a separate file prevents crowding out operational context.\n\nSee [Product Vision Authoring](/practices/product-vision-authoring) for guidance on the inline vs. separate decision, templates, and maintenance practices.\n\n## Validated in Practice\n\n### Industry Validation\n\n**Marty Cagan (Silicon Valley Product Group)**\nIn the AI era, Cagan argues that **product vision** is more critical than ever. As AI lowers the cost of building features, differentiation shifts from \"ability to ship\" to \"ability to solve value risks.\" Without a strong vision, AI teams build \"features that work\" rather than \"products that matter.\"\n\n> \"It will be easier to build features, but harder to build the *right* features.\" — Marty Cagan\n\n**Lenny Rachitsky (Product Sense)**\nRachitsky defines \"product sense\" as the ability to consistently craft products with intended impact. `VISION.md` is essentially **codified product sense**—explicitly documenting the intuition that senior PMs use to steer teams, so that agents (who lack intuition) can simulate it.\n\n### The Scientific Basis: Countering Regression to the Mean\nLLMs are probabilistic engines trained to predict the most likely next token. By definition, \"most likely\" means \"most average.\"\n\nWithout external constraint, an agent will always drift toward the [Regression to the Mean](https://en.wikipedia.org/wiki/Regression_towards_the_mean). A Product Vision acts as a **forcing function**, artificially skewing the probability distribution toward specific, non-average choices (e.g., \"playful\" over \"professional,\" \"dense\" over \"simple\").\n\n## Anti-Patterns\n\n### The Generic Vision\n\n\"User-centric design. Quality and reliability. Innovation and creativity.\"\n\nThis says nothing. Every company claims these values. A Product Vision without opinions is just corporate filler that agents will (correctly) ignore.\n\n### The Aspirational Vision\n\nDescribing the product you wish you had, not the product you're building. If your vision says \"minimal and focused\" but your product has 47 settings screens, agents will be confused by the contradiction.\n\n### The Ignored Vision\n\nCreating the document once and never referencing it in specs or prompts. The artifact exists but agents never see it in context.\n\n### The Aesthetic-Only Vision\n\nAll visual preferences, no product opinion. \"We like blue and sans-serif fonts\" isn't vision—it's a style guide. Vision captures *judgment*, not just *appearance*.\n\n## Relationship to Other Patterns\n\n**[Agent Constitution](/patterns/agent-constitution)** — The Constitution defines behavioral rules (what agents must/must not do). The Vision defines taste (what agents should prefer when rules don't dictate). Constitution is constraints; Vision is guidance.\n\n**[The Spec](/patterns/the-spec)** — Specs define feature contracts. The Vision influences *how* those contracts are fulfilled. Specs reference Vision for design rationale: \"Per VISION.md: 'Settings are failure; good defaults are success.'\"\n\n**[Context Engineering](/concepts/context-engineering)** — The Vision is a structured context asset. It follows Context Engineering principles: curated, opinionated, agent-optimized.\n\n## Related Practices\n\n**[Product Vision Authoring](/practices/product-vision-authoring)** — Step-by-step guide for creating and maintaining a Product Vision, including templates, inline vs. separate file decisions, and diagnostic guidance.\n\n**[AGENTS.md Specification](/practices/agents-md-spec)** — Defines the file format for agent constitutions, including how to incorporate vision as a preamble or reference.\n\n**[Living Specs](/practices/living-specs)** — Specs can reference vision for design rationale. The \"same-commit rule\" applies: if vision changes, affected specs should acknowledge the shift.\n\n**[Agent Personas](/practices/agent-personas)** — Different personas may need different vision depth. A copywriting agent needs full voice guidance; a database migration agent needs minimal product context.\n\nSee also:\n- [Agent Constitution](/patterns/agent-constitution) — Behavioral alignment pattern\n- [The Spec](/patterns/the-spec) — Feature contract pattern\n- [AGENTS.md Specification](/practices/agents-md-spec) — Constitution implementation practice\n- [Vibe Coding](/concepts/vibe-coding) — The failure mode when neither vision nor specs constrain agent output",
    "tags": ["Context Engineering", "Product Design", "Agent Alignment"],
    "references": [
      {
        "type": "video",
        "title": "Product Management in the Era of AI",
        "url": "https://www.youtube.com/watch?v=kYsqp21jXkU",
        "author": "Marty Cagan",
        "publisher": "Silicon Valley Product Group",
        "published": "2024-04-15T00:00:00.000Z",
        "accessed": "2026-01-13T00:00:00.000Z",
        "annotation": "Validates the thesis that AI increases, rather than decreases, the need for strong product vision and strategy."
      },
      {
        "type": "website",
        "title": "How to develop product sense",
        "url": "https://www.lennysnewsletter.com/p/how-to-develop-product-sense",
        "author": "Lenny Rachitsky",
        "published": "2023-05-09T00:00:00.000Z",
        "accessed": "2026-01-13T00:00:00.000Z",
        "annotation": "Defines the 'taste' skill that Product Vision attempts to encode for agents."
      }
    ]
  },
  {
    "slug": "ralph-loop",
    "collection": "patterns",
    "title": "Ralph Loop",
    "description": "Persistence pattern enabling autonomous agent iteration until external verification passes, treating failure as feedback rather than termination.",
    "status": "Live",
    "content": "## Definition\n\nThe **Ralph Loop**—named by Geoffrey Huntley after the persistently confused but undeterred Simpsons character Ralph Wiggum—is a persistence pattern that turns AI coding agents into autonomous, self-correcting workers.\n\nThe pattern operationalizes the [OODA Loop](/concepts/ooda-loop) for terminal-based agents and automates the [Learning Loop](/concepts/learning-loop) with machine-verifiable completion criteria. It enables sustained [L3-L4 autonomy](/concepts/levels-of-autonomy)—\"AFK coding\" where the developer initiates and returns to find committed changes.\n\n```mermaid\nflowchart LR\n    subgraph Input\n        PBI[\"PBI / Spec\"]\n    end\n    \n    subgraph \"Human-in-the-Loop (L1-L2)\"\n        DEV[\"Dev + Copilot\"]\n        E2E[\"E2E Tests\"]\n        DEV --> E2E\n    end\n    \n    subgraph \"Ralph Loop (L3-L4)\"\n        AGENT[\"Agent Iteration\"]\n        VERIFY[\"External Verification\"]\n        AGENT --> VERIFY\n        VERIFY -->|\"Fail\"| AGENT\n    end\n    \n    subgraph Output\n        REVIEW[\"Adversarial Review\"]\n        MERGE[\"Merge\"]\n        REVIEW --> MERGE\n    end\n    \n    PBI --> DEV\n    PBI --> AGENT\n    E2E --> REVIEW\n    VERIFY -->|\"Pass\"| REVIEW\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/ralph-loop-fig-1.svg\" alt=\"Mermaid Diagram\" />\n  \n</figure>\n\nBoth lanes start from the same well-structured PBI/Spec and converge at Adversarial Review. The Ralph Loop lane operates autonomously, with human oversight at review boundaries rather than every iteration.\n\n> [!WARNING]\n> **The \"100 Million Lines\" Anti-Pattern**\n> \n> Ralph Loop enables persistence, not quality. Using Ralph Loop for unbounded code generation without specs produces what Dan Cripe calls \"100 million lines of crappy code\"—technically functional but architecturally incoherent and unmaintainable.\n> \n> Ralph Loop is a *persistence mechanism*, not a *development methodology*. It must be bounded by:\n> - **Exit criteria** defined in [The Spec](/patterns/the-spec)\n> - **Verification gates** that check architectural coherence, not just compilation\n> - **Scope limits** that prevent unbounded iteration\n\n## The Problem: Human-in-the-Loop Bottleneck\n\nTraditional AI-assisted development creates a productivity ceiling: the human reviews every output before proceeding. This makes the human the slow component in an otherwise high-speed system.\n\nThe naive solution—trusting the agent's self-assessment—fails because LLMs confidently approve their own broken code. Research demonstrates that self-correction is only reliable with objective external feedback. Without it, the agent becomes a \"mimicry engine\" that hallucinates success.\n\n| Aspect | Traditional AI Interaction | Failure Mode |\n|--------|---------------------------|--------------|\n| **Execution Model** | Single-pass (one-shot) | Limited by human availability |\n| **Failure Response** | Process termination or manual re-prompt | Blocks on human attention |\n| **Verification** | Human review of every output | Human becomes bottleneck |\n\n## The Solution: External Verification Loop\n\nThe Ralph Loop inverts the quality control model: instead of treating LLM failures as terminal states requiring human intervention, it engineers failure as diagnostic data. The agent iterates until external verification (not self-assessment) confirms success.\n\n**Core insight:** Define the \"finish line\" through machine-verifiable tests, then let the agent iterate toward that finish line autonomously. **Iteration beats perfection.**\n\n| Aspect | Traditional AI | Ralph Loop |\n|--------|---------------|------------|\n| **Execution Model** | Single-pass | Continuous multi-cycle |\n| **Failure Response** | Manual re-prompt | Automatic feedback injection |\n| **Persistence Layer** | Context window | File system + Git history |\n| **Verification** | Human review | External tooling (Docker, Jest, tsc) |\n| **Objective** | Immediate correctness | Eventual convergence |\n\n## Anatomy\n\n### 1. Stop Hooks and Exit Interception\n\nThe agent attempts to exit when it believes it's done. A Stop hook intercepts the exit and evaluates current state against success criteria. If the agent hasn't produced a specific \"completion promise\" (e.g., `<promise>DONE</promise>`), the hook blocks exit and re-injects the original prompt.\n\nThis creates a self-referential loop: the agent confronts its previous work, analyzes why the task remains incomplete, and attempts a new approach.\n\n### 2. External Verification (Generator/Judge Separation)\n\nThe agent is not considered finished when it *believes* it's done—only when external verification confirms success:\n\n| Evaluation Type | Agent Logic | External Tooling |\n|-----------------|-------------|------------------|\n| Self-Assessment | \"I believe this is correct\" | None (Subjective) |\n| External Verification | \"I will run docker build\" | Docker Engine (Objective) |\n| Exit Decision | LLM decides to stop | System stops because tests pass |\n\nThis is the architectural enforcement of Generator/Judge separation from [Adversarial Code Review](/patterns/adversarial-code-review), but mechanized.\n\n### 3. Git as Persistent Memory\n\nContext windows rot, but Git history persists. Each iteration commits changes, so subsequent iterations \"see\" modifications from previous attempts. The codebase becomes the source of truth, not the conversation.\n\nGit also enables easy rollback if an iteration degrades quality.\n\n### 4. Context Rotation and Progress Files\n\n**Context rot:** Accumulation of error logs and irrelevant history degrades LLM reasoning.\n\n**Solution:** At 60-80% context capacity, trigger forced rotation to fresh context. Essential state carries over via structured progress files:\n\n- Summary of tasks completed\n- Failed approaches (to avoid repeating)\n- Architectural decisions to maintain\n- Files intentionally modified\n\nThis is the functional equivalent of `free()` for LLM memory—applied [Context Engineering](/concepts/context-engineering).\n\n### 5. Convergence Through Iteration\n\nThe probability of successful completion P(C) is a function of iterations n:\n\n```\nP(C) = 1 - (1 - p_success)^n\n```\n\nAs n increases (often up to 50 iterations), probability of handling complex bugs approaches 1.\n\n## OODA Loop Mapping\n\nThe Ralph Loop is [OODA](/concepts/ooda-loop) mechanized:\n\n| OODA Phase | Ralph Loop Implementation |\n|------------|--------------------------|\n| **Observe** | Read codebase state, error logs, failed builds |\n| **Orient** | Marshal context, interpret errors, read progress file |\n| **Decide** | Formulate specific plan for next iteration |\n| **Act** | Modify files, run tests, commit changes |\n\nThe cycle repeats until external verification passes.\n\n## Relationship to Other Patterns\n\n**[Context Gates](/patterns/context-gates)** — Context rotation + progress files = state filtering between iterations. Ralph Loops are Context Gates applied to the iteration boundary.\n\n**[Adversarial Code Review](/patterns/adversarial-code-review)** — Ralph architecturally enforces Generator/Judge separation. External tooling is the \"Judge\" that prevents self-assessment failure.\n\n**[The Spec](/patterns/the-spec)** — Completion promises require machine-verifiable success criteria. Well-structured Specs with Gherkin scenarios are ideal Ralph inputs.\n\n**[Workflow as Code](/practices/workflow-as-code)** — The practice for implementing Ralph Loops using typed step abstractions rather than prompt-based orchestration. Provides deterministic control flow with the agent invoked only for probabilistic tasks.\n\n## Anti-Patterns\n\n| Anti-Pattern | Description | Failure Mode |\n|--------------|-------------|--------------|\n| **Vague Prompts** | \"Improve this codebase\" without specific criteria | Divergence; endless superficial changes |\n| **No External Verification** | Relying on agent self-assessment | Self-Assessment Trap; hallucinates success |\n| **No Iteration Caps** | Running without max iterations limit | Infinite loops; runaway API costs |\n| **No Sandbox Isolation** | Agent has access to sensitive host files | Security breach; SSH keys, cookies exposed |\n| **No Context Rotation** | Letting context window fill without rotation | Context rot; degraded reasoning |\n| **No Progress Files** | Fresh iterations re-discover completed work | Wasted tokens; repeated mistakes |\n\n### Unbounded Generation\n\nRunning Ralph Loop without scope constraints produces volume without value. Each iteration may \"fix\" the immediate error while introducing architectural drift. Over time, the codebase becomes:\n\n- **Internally inconsistent**: Different modules make different assumptions\n- **Unmaintainable**: No human understands the full system\n- **Expensive to verify**: Review time exceeds generation time\n\n### Missing Architectural Verification\n\nRalph Loop's default exit criteria (tests pass, compilation succeeds) don't verify architectural coherence. A loop that only checks \"does it work?\" will happily generate code that violates design patterns, duplicates logic, or introduces subtle inconsistencies.\n\n**Mitigation**: Combine Ralph Loop with [Constitutional Review](/patterns/constitutional-review) to verify outputs against architectural principles, not just functional requirements.\n\n## Guardrails\n\n| Risk | Mitigation |\n|------|------------|\n| Infinite Looping | Hard iteration caps (20-50 iterations) |\n| Context Rot | Periodic rotation at 60-80% capacity |\n| Security Breach | Sandbox isolation (Docker, WSL) |\n| Token Waste | Exact completion promise requirements |\n| Logic Drift | Frequent Git commits each iteration |\n| Cost Overrun | API cost tracking per session |",
    "tags": ["Agent Architecture", "Automation", "Iteration", "Verification"],
    "references": [
      {
        "type": "website",
        "title": "Understanding the Ralph Loop",
        "url": "https://ghuntley.com/ralph/",
        "author": "Geoffrey Huntley",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Original formulation of the Ralph Loop concept and philosophy."
      },
      {
        "type": "paper",
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
        "url": "https://arxiv.org/abs/2310.01798",
        "author": "Jie Huang et al.",
        "published": "2023-10-03T00:00:00.000Z",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Research demonstrating LLM self-correction limitations without external feedback."
      },
      {
        "type": "website",
        "title": "Dev Workflows as Code",
        "url": "https://medium.com/nick-tune-tech-strategy-blog/dev-workflows-as-code-fab70d44b6ab",
        "author": "Nick Tune",
        "published": "2026-01-16T00:00:00.000Z",
        "accessed": "2026-01-18T00:00:00.000Z",
        "annotation": "Proposes composable step abstractions for deterministic loops."
      },
      {
        "type": "website",
        "title": "The Mythical LLM: Why Rumors of the Death of Software are Premature",
        "author": "Dan Cripe",
        "published": "2026-01-20T00:00:00.000Z",
        "url": "https://www.dancripe.com/ai-coding-enterprise-saas-reality-check/",
        "accessed": "2026-01-24T00:00:00.000Z",
        "annotation": "Critique of unbounded Ralph Loop usage: 'Who the hell wants to maintain 100,000,000 lines of crappy code?'"
      }
    ]
  },
  {
    "slug": "spec-reversing",
    "collection": "patterns",
    "title": "Spec Reversing",
    "description": "Using frontier models to derive specifications from existing code to bootstrap the Agentic SDLC in brownfield projects.",
    "status": "Experimental",
    "content": "## The Void: Missing Truth\n\nThe Agentic SDLC relies on the **Spec** as the source of truth. However, most real-world projects are \"brownfield\"—they have code but no up-to-date documentation. This creates a \"Void\" where agents have no context to ground their work, leading to regression loops and hallucinated requirements.\n\nSpec Reversing bridges this gap by treating the current codebase as the *de facto* truth—but only temporarily.\n\n## The Pattern\n\nSpec Reversing is a bootstrapping workflow. Instead of writing a spec from scratch, we use a frontier model (like Claude 3.5 Sonnet or GPT-4o) to \"read\" the code and \"write\" the missing spec.\n\nThe workflow follows this loop:\n\n1.  **Select Scope**: Identify the specific file or component you are about to modify.\n2.  **Reverse**: Feed the code to a frontier model in `Architect` or `Planning` mode.\n    *   *Prompt*: \"Reverse engineer a functional specification from this code. Capture the intent, logic, and edge cases.\"\n3.  **Review**: A human (you) reviews the generated spec.\n    *   *Critique*: \"Is this actually what we want? Or just what the code currently does?\"\n    *   *Correct*: Fix any bugs in the *logic* (in the spec) before touching the code.\n4.  **Commit**: Save this as a new Spec file (e.g., `specs/feature-name.md`).\n5.  **Execute**: Now create your PBI based on this new Spec.\n\n## When to Use\n\n*   **Before a PBI**: Never start a PBI without a Spec. If one doesn't exist, reverse it first.\n*   **Legacy Refactoring**: When touching \"ancient\" code that no one understands.\n*   **Drift Detection**: When you suspect the current documentation is lying. Reverse the code and compare it to the old docs.\n\n## Directives\n\n*   **Don't Trust the Code Blindly**: The code might contain bugs. The reversed spec will document those bugs as features. It is your job during the **Review** phase to decide if those are intended.\n*   **Keep it High-Level**: Don't just narrate the code (\"variable x is assigned 5\"). Describe the *behavior* (\"The retry limit is set to 5\").\n*   **One File at a Time**: Don't try to reverse the entire repository. Reverse only the Spec you need for the Task at hand.\n\n## Benefits\n\n*   **Stops Context Amnesia**: Creates a permanent memory of how the system works.\n*   **Enables Agent Autonomy**: Agents can now reason about the system *before* acting.\n*   **Safe Refactoring**: You have a baseline \"contract\" to test against.",
    "tags": ["Brownfield", "Specification", "Bootstrapping", "Context Engineering"],
    "references": []
  },
  {
    "slug": "the-adr",
    "collection": "patterns",
    "title": "The ADR",
    "description": "A structural pattern for capturing architectural decisions with context, rationale, and consequences in an immutable record.",
    "status": "Live",
    "content": "## Definition\n\nThe **ADR** (Architecture Decision Record) is a lightweight document pattern for capturing significant architectural decisions. Each ADR records exactly one decision: what was decided, why it was decided, and what consequences follow.\n\nUnlike [The Spec](/patterns/the-spec) which defines the current state of a feature and evolves with the code, an ADR is **immutable**—it captures a snapshot of thinking at a specific moment. When circumstances change, a new ADR supersedes the old one, preserving the decision history.\n\n## The Problem: Decision Amnesia\n\nArchitectural knowledge decays rapidly. Six months after a technology choice, teams ask:\n\n- \"Why did we choose PostgreSQL over MongoDB?\"\n- \"Who decided we'd use microservices here?\"\n- \"What alternatives were considered for the auth system?\"\n\nWithout explicit decision records, this context lives only in:\n- Email threads (unsearchable, often deleted)\n- Slack conversations (ephemeral, noisy)\n- Tribal knowledge (leaves when people leave)\n\nFor agentic development, this creates a severe problem. An agent refactoring authentication code has no visibility into why Supabase Auth was chosen over Firebase Auth—it may inadvertently violate the constraints that drove the original decision.\n\n## The Solution: Immutable Decision Records\n\nADRs solve decision amnesia by making architectural decisions **first-class artifacts** in the codebase. Each decision is documented at the moment it's made, with full context preserved.\n\n```\ndocs/adrs/\n├── ADR-001-use-postgresql.md\n├── ADR-002-supabase-auth.md\n├── ADR-003-event-driven-messaging.md\n└── ADR-004-svelte-over-react.md        # Supersedes ADR-001 (hypothetical)\n```\n\nThe key insight: **decisions are immutable, but their status changes**. ADR-001 might be \"Accepted\" for two years, then become \"Superseded by ADR-010\" when the team migrates databases.\n\n## Anatomy\n\nAn ADR consists of six sections, each serving a distinct purpose:\n\n### 1. Title\n\nA short, descriptive name with a unique identifier.\n\n**Format:** `ADR-NNN: Decision Summary`\n\n**Examples:**\n- ADR-001: Use PostgreSQL for Primary Database\n- ADR-007: Adopt Event-Driven Architecture for Order Processing\n- ADR-012: Choose Svelte 5 over React for Interactive Components\n\n### 2. Status\n\nThe lifecycle state of the decision:\n\n| Status | Meaning |\n|--------|---------|\n| **Proposed** | Under discussion, not yet decided |\n| **Accepted** | Decision made and in effect |\n| **Deprecated** | No longer recommended but not replaced |\n| **Superseded** | Replaced by a newer ADR (link to successor) |\n\n**Example:** `Status: Superseded by ADR-015`\n\n### 3. Context\n\nThe forces and constraints that shaped the decision. This is the *why*—without it, the decision appears arbitrary.\n\n**Include:**\n- Business requirements driving the need\n- Technical constraints (existing systems, team skills)\n- Timeline pressures\n- Non-functional requirements (scale, security, compliance)\n\n**Example:**\n> We need real-time collaboration features. The existing polling-based approach creates unacceptable latency (>2s) and server load. The team has experience with PostgreSQL but not MongoDB. We have 3 weeks before the feature deadline.\n\n### 4. Decision\n\nWhat was decided. State it clearly and unambiguously.\n\n**Format:** \"We will [do X]\" or \"We decided to [do X]\"\n\n**Example:**\n> We will use Supabase Realtime (built on PostgreSQL logical replication) for real-time collaboration features.\n\n### 5. Consequences\n\nThe outcomes of this decision—positive, negative, and neutral. **Honesty here is critical.** A decision that hides its downsides will be revisited with confusion.\n\n**Structure:**\n- **Positive:** Benefits and capabilities gained\n- **Negative:** Trade-offs and limitations accepted\n- **Neutral:** Changes that are neither good nor bad\n\n**Example:**\n> **Positive:** Leverages existing PostgreSQL expertise. Real-time updates with <100ms latency. No new database to manage.\n>\n> **Negative:** Tied to Supabase SaaS (vendor lock-in). Less flexible query patterns than dedicated real-time databases. Learning curve for PostgreSQL triggers.\n>\n> **Neutral:** Requires migration of subscription logic from polling to channels.\n\n### 6. Alternatives Considered\n\nWhat other options were evaluated and why they were rejected. This prevents future teams from re-evaluating the same options without understanding the original analysis.\n\n**Format:** List each alternative with rejection rationale.\n\n**Example:**\n> - **Firebase Realtime Database:** Rejected—would require a second database system and doesn't integrate with existing PostgreSQL data.\n> - **Custom WebSocket implementation:** Rejected—significant development effort and maintenance burden for real-time infrastructure.\n> - **Pusher:** Rejected—adds external dependency and per-message costs at scale.\n\n## State vs The Spec\n\nThe ADR complements [The Spec](/patterns/the-spec) but serves a different purpose:\n\n| Dimension | The Spec | The ADR |\n|-----------|----------|---------|\n| **Purpose** | Define how it works now | Record why we decided |\n| **Mutability** | Living (updated with code) | Immutable (superseded, not edited) |\n| **Scope** | Feature-level behavior | Architectural choice |\n| **Audience** | Implementers | Archaeologists, reviewers |\n\nA feature Spec might say \"Authentication uses Supabase Auth with Magic Link.\" The ADR explains *why* Supabase Auth was chosen over Firebase Auth.\n\n## Adversarial Decision Review\n\nThe [Adversarial Code Review](/patterns/adversarial-code-review) pattern validates code against specs. ADRs need a different review approach—**Adversarial Decision Review**—that evaluates the decision quality itself.\n\n### Critic Agent Prompt\n\n```\nYou are reviewing an Architecture Decision Record.\n\nEvaluate:\n1. **Context Completeness** — Are the forces and constraints clearly articulated? \n   Could someone unfamiliar with the project understand WHY this decision was needed?\n\n2. **Alternatives Rigor** — Were reasonable alternatives considered? \n   Is each rejection rationale specific (not \"too complex\" without explanation)?\n\n3. **Consequence Honesty** — Are negative outcomes acknowledged?\n   Beware ADRs with only positive consequences—every decision has trade-offs.\n\n4. **Reversibility Clarity** — Is it clear how to undo this decision if needed?\n   What would trigger reconsideration?\n\n5. **Scope Discipline** — Does this ADR decide exactly one thing?\n   Multiple decisions should be separate ADRs.\n\nOutput: ACCEPT or list of concerns with suggested improvements.\n```\n\nThis pattern ensures ADRs maintain quality as high-value context for future decisions.\n\n## Relationship to Other Patterns\n\n**[The Spec](/patterns/the-spec)** — Specs define current feature state; ADRs explain the architectural choices that constrain specs. An ADR might mandate \"all API routes use REST,\" and feature specs implement within that constraint.\n\n**[Agent Constitution](/patterns/agent-constitution)** — ADRs can become constitutional rules. \"ADR-003: All database migrations must be backward-compatible\" may be promoted to an agent constitution constraint that the agent must not violate.\n\n**[Context Engineering](/concepts/context-engineering)** — ADRs are high-value context for agents. Including relevant ADRs in agent context helps prevent accidental violations of past architectural decisions.\n\n**[Request for Comments](/concepts/request-for-comments)** — RFCs are proposals that spawn ADRs. An RFC gathers feedback; acceptance creates one or more ADRs.\n\n**[ADR Authoring](/practices/adr-authoring)** — The practice that implements this pattern with templates, lifecycle guidance, and file organization.",
    "tags": ["Architecture", "Documentation", "Decision Making"],
    "references": [
      {
        "type": "website",
        "title": "Documenting Architecture Decisions",
        "url": "https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions",
        "author": "Michael Nygard",
        "published": "2011-11-15T00:00:00.000Z",
        "accessed": "2026-01-28T00:00:00.000Z",
        "annotation": "The original ADR proposal establishing the canonical format."
      },
      {
        "type": "website",
        "title": "Architectural Decision Records",
        "url": "https://adr.github.io/",
        "accessed": "2026-01-28T00:00:00.000Z",
        "annotation": "Community resources, tooling, and template variations."
      }
    ]
  },
  {
    "slug": "the-pbi",
    "collection": "patterns",
    "title": "The PBI",
    "description": "A transient execution unit that defines the delta (change) while pointing to permanent context (The Spec), optimized for agent consumption.",
    "status": "Live",
    "content": "## Definition\n\nThe Product Backlog Item (PBI) is the unit of execution in the ASDLC. While [The Spec](/patterns/the-spec) defines the **State** (how the system works), the PBI defines the **Delta** (the specific change to be made).\n\nIn an AI-native workflow, the PBI transforms from a \"User Story\" (negotiable conversation) into a **Prompt** (strict directive). The AI has flexibility in *how* code is written, but the PBI enforces strict boundaries on *what* is delivered.\n\n## The Problem: Ambiguous Work Items\n\nTraditional user stories (\"As a user, I want...\") are designed for human negotiation. They assume ongoing dialogue, implicit context, and shared understanding built over time.\n\nAgents don't negotiate. They execute. A vague story becomes a hallucinated implementation.\n\n**What fails without structured PBIs:**\n- Agents interpret scope liberally, touching unrelated code\n- No clear pointer to authoritative design decisions\n- Success criteria scattered across conversations\n- Merge conflicts from parallel agents hitting the same files\n\n## The Solution: Pointer, Not Container\n\nThe PBI acts as a **pointer** to permanent context, not a container for the full design. It defines the delta while referencing The Spec for the state.\n\n| Dimension   | The Spec                        | The PBI                          |\n| :---------- | :------------------------------ | :------------------------------- |\n| **Purpose** | Define the State (how it works) | Define the Delta (what changes)  |\n| **Lifespan**| Permanent (lives with the code) | Transient (closed after merge)   |\n| **Scope**   | Feature-level rules             | Task-level instructions          |\n| **Audience**| Architects, Agents (Reference)  | Agents, Developers (Execution)   |\n\n## Anatomy\n\nAn effective PBI consists of four parts:\n\n### 1. The Directive\n\nWhat to do, with explicit scope boundaries. Not a request—a constrained instruction.\n\n### 2. The Context Pointer\n\nReference to the permanent spec. Prevents the PBI from becoming a stale copy of design decisions that live elsewhere.\n\n### 3. The Verification Pointer\n\nLink to success criteria defined in the spec's Contract section. The agent knows exactly what \"done\" looks like.\n\n### 4. The Refinement Rule\n\nProtocol for when reality diverges from the spec. Does the agent stop? Update the spec? Flag for human review?\n\n## Bounded Agency\n\nBecause AI is probabilistic, it requires freedom to explore the \"How\" (implementation details, syntax choices). However, to prevent hallucination, we bound this freedom with non-negotiable constraints.\n\n**Negotiable (The Path):** Code structure, variable naming, internal logic flow, refactoring approaches.\n\n**Non-Negotiable (The Guardrails):** Steps defined in the PBI, outcome metrics in the Spec, documented anti-patterns, architectural boundaries.\n\nThe PBI is not a request for conversation—it's a constrained optimization problem.\n\n## Atomicity & Concurrency\n\nIn swarm execution (multiple agents working in parallel), each PBI must be:\n\n**Atomic:** The PBI delivers a complete, working increment. No partial states. If the agent stops mid-task, either the full change lands or nothing does.\n\n**Self-Testable:** Verification criteria must be executable without other pending PBIs completing first. If PBI-102 requires PBI-101's code to test, PBI-102 is not self-testable.\n\n**Isolated:** Changes target distinct files/modules. Two concurrent PBIs modifying the same file create merge conflicts and non-deterministic outcomes.\n\n### Dependency Declaration\n\nWhen a PBI requires another to complete first, the dependency is declared explicitly in the PBI structure—not discovered at merge time.\n\n## Relationship to Other Patterns\n\n**[The Spec](/patterns/the-spec)** — The permanent source of truth that PBIs reference. The Spec defines state; the PBI defines delta.\n\n**[PBI Authoring](/practices/pbi-authoring)** — The practice for writing effective PBIs, including templates and lifecycle.\n\nSee also:\n- [Spec-Driven Development](/concepts/spec-driven-development) — The overarching methodology\n- [Context Gates](/patterns/context-gates) — Validation checkpoints for PBI completion",
    "tags": ["Agile", "Product Backlog Item", "Spec-Driven Development", "Bounded Agency"],
    "references": []
  },
  {
    "slug": "the-spec",
    "collection": "patterns",
    "title": "Specs",
    "description": "Living documents that serve as the permanent source of truth for features, solving the context amnesia problem in agentic development.",
    "status": "Live",
    "content": "## Definition\n\nA **Spec** is the permanent source of truth for a feature. It defines *how* the system works (Design) and *how* we know it works (Quality).\n\nUnlike traditional tech specs or PRDs that are \"fire and forget,\" specs are **living documents**. They reside in the repository alongside the code and evolve with every change to the feature.\n\n## The Problem: Context Amnesia\n\nAgents do not have long-term memory. They cannot recall Jira tickets from six months ago or Slack conversations about architectural decisions. When an agent is tasked with modifying a feature, it needs immediate access to:\n\n- The architectural decisions that shaped the feature\n- The constraints that must not be violated\n- The quality criteria that define success\n\nWithout specs, agents reverse-engineer intent from code comments and commit messages—a process prone to hallucination and architectural drift.\n\nTraditional documentation fails because:\n- **Wikis decay** — separate systems fall out of sync with code\n- **Tickets disappear** — issue trackers capture deltas (changes), not state (current rules)\n- **Comments lie** — code comments describe implementation, not architectural intent\n- **Memory fails** — tribal knowledge evaporates when team members leave\n\nSpecs solve this by making documentation a **first-class citizen** in the codebase, subject to the same version control and review processes as the code itself.\n\n## State vs Delta\n\nThis is the core distinction that makes agentic development work at scale.\n\n| Dimension | The Spec | The PBI |\n|-----------|----------|---------|\n| **Purpose** | Define the State (how it works) | Define the Delta (what changes) |\n| **Lifespan** | Permanent (lives with the code) | Transient (closed after merge) |\n| **Scope** | Feature-level rules | Task-level instructions |\n| **Audience** | Architects, Agents (Reference) | Agents, Developers (Execution) |\n\nThe Spec defines the **current state** of the system:\n- \"All notifications must deliver within 100ms\"\n- \"API must handle 1000 req/sec\"\n\nThe PBI defines the **change**:\n- \"Add SMS fallback to notification system\"\n- \"Optimize database query for search endpoint\"\n\nThe PBI *references* the Spec for context and *updates* the Spec when it changes contracts.\n\n### Why Separation Matters\n\n```\nSprint 1: PBI-101 \"Build notification system\"\n  → Creates /plans/notifications/spec.md\n  → Spec defines: \"Deliver within 100ms via WebSocket\"\n\nSprint 3: PBI-203 \"Add SMS fallback\"\n  → Updates spec.md with new transport rules\n  → PBI-203 is closed, but the spec persists\n\nSprint 8: PBI-420 \"Refactor notification queue\"\n  → Agent reads spec.md, sees all rules still apply\n  → Refactoring preserves all documented contracts\n```\n\nWithout this separation, the agent in Sprint 8 has no visibility into decisions made in Sprint 1.\n\n## The Assembly Model\n\nSpecs serve as the context source for Feature Assembly. Multiple PBIs reference the same spec, and the spec's contracts are verified at quality gates.\n\n```mermaid\nflowchart LR\n  A[/spec.md/]\n\n  B[\\pbi-101.md\\]\n  C[\\pbi-203.md\\]\n  D[\\pbi-420.md\\]\n\n  B1[[FEATURE ASSEMBLY]]\n  C1[[FEATURE ASSEMBLY]]\n  D1[[FEATURE ASSEMBLY]]\n\n  E{GATE}\n\n  F[[MIGRATION]]\n\n  A --> B\n  A --> C\n  A --> D\n\n  B --> B1\n  C --> C1\n  D --> D1\n\n  B1 --> E\n  C1 --> E\n  D1 --> E\n\n  A --> |Context|E\n\n  E --> F\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/the-spec-fig-1.svg\" alt=\"Mermaid Diagram\" />\n  \n</figure>\n\n## Anatomy\n\nEvery spec consists of two parts:\n\n### Blueprint (Design)\nDefines **implementation constraints** that prevent agents from hallucinating invalid architectures.\n\n- **Context** — Why does this feature exist?\n- **Architecture** — API contracts, schemas, dependency directions\n- **Anti-Patterns** — What agents must NOT do\n\n### Contract (Quality)\nDefines **verification rules** that exist independently of any specific task.\n\n- **Definition of Done** — Observable success criteria\n- **Regression Guardrails** — Invariants that must never break\n- **Scenarios** — [Gherkin](/concepts/gherkin)-style behavioral specifications\n\nThe Contract section implements [Behavior-Driven Development](/concepts/behavior-driven-development) principles: scenarios define *what* behavior is expected without dictating *how* to implement it. This allows agents to interpret intent dynamically while providing clear verification criteria.\n\nFor detailed structure, examples, and templates, see the [Living Specs Practice Guide](/practices/living-specs).\n\n## Relationship to Other Patterns\n\n**[The PBI](/patterns/the-pbi)** — PBIs are the transient execution units (Delta) that reference specs for context. When a PBI changes contracts, it updates the spec in the same commit.\n\n**[Feature Assembly](/practices/feature-assembly)** — Specs define the acceptance criteria verified during assembly. The diagram above shows this flow.\n\n**[Experience Modeling](/patterns/experience-modeling)** — Experience models capture user journeys; specs capture the technical contracts that implement those journeys.\n\n**[Context Engineering](/concepts/context-engineering)** — Specs are structured context assets optimized for agent consumption, with predictable sections (Blueprint, Contract) for efficient extraction.\n\n**[Behavior-Driven Development](/concepts/behavior-driven-development)** — BDD provides the methodology for the Contract section. [Gherkin](/concepts/gherkin) scenarios serve as \"specifications of behavior\" that guide agent reasoning and define acceptance criteria.\n\n## Iterative Spec Refinement\n\nKent Beck critiques spec-driven approaches that assume \"you aren't going to learn anything during implementation.\" This is valid—specs are not waterfall artifacts.\n\n**The refinement cycle:**\n\n1. **Initial Spec** — Capture known constraints (API contracts, quality targets, anti-patterns)\n2. **Implementation Discovery** — Agent or human encounters edge cases, performance issues, or missing requirements\n3. **Spec Update** — New constraints committed alongside the code that revealed them\n4. **Verification** — Gate validates implementation against updated spec\n5. **Repeat**\n\nThis is the [Learning Loop](/concepts/learning-loop) applied to specs: the spec doesn't prevent learning—it captures learnings so agents can act on them in future sessions.\n\n> \"Large Language Models give us great leverage—but they only work if we focus on learning and understanding.\"\n> — Unmesh Joshi, via Martin Fowler\n\n\n## Industry Validation\n\nThe Spec pattern has emerged independently across the industry under different names. Notably, Rasmus Widing's **Product Requirement Prompt (PRP)** methodology defines the same structure: Goal + Why + Success Criteria + Context + Implementation Blueprint + Validation Loop.\n\nHis core principles—\"Plan before you prompt,\" \"Context is everything,\" \"Scope to what the model can reliably do\"—mirror ASDLC's Spec-Driven Development philosophy.\n\nSee [Product Requirement Prompts](/concepts/product-requirement-prompt) for the full mapping and [Industry Alignment](/resources/industry-alignment) for convergent frameworks.\n\nSee also:\n- [Living Specs Practice Guide](/practices/living-specs) — Implementation instructions, templates, and best practices\n- [Behavior-Driven Development](/concepts/behavior-driven-development) — The methodology behind Contract scenarios\n- [Gherkin](/concepts/gherkin) — Syntax guidance for writing behavioral specifications",
    "tags": [
      "Documentation",
      "Living Documentation",
      "Spec-Driven Development",
      "Context Engineering"
    ],
    "references": [
      {
        "type": "website",
        "title": "Living Documentation",
        "url": "https://martinfowler.com/bliki/LivingDocumentation.html",
        "author": "Martin Fowler",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Martin Fowler's definition of Living Documentation, the foundation for keeping documentation synchronized with code."
      },
      {
        "type": "repository",
        "title": "PRPs: Agentic Engineering",
        "url": "https://github.com/Wirasm/PRPs-agentic-eng",
        "author": "Rasmus Widing",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Practitioner-validated implementation of spec-driven agentic development, demonstrating convergent evolution with ASDLC principles."
      },
      {
        "type": "website",
        "title": "Martin Fowler Fragment: January 8, 2026",
        "url": "https://martinfowler.com/fragments/2026-01-08.html",
        "author": "Martin Fowler",
        "published": "2022-12-22T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Commentary on Anthropic research and Kent Beck's critique of spec-driven approaches."
      },
      {
        "type": "website",
        "title": "Kent Beck on Spec-Driven Development",
        "url": "https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/",
        "author": "Kent Beck",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Critique that specs must accommodate learning during implementation—addressed by iterative refinement."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Research validating the cold start problem and context transfer challenges in AI-assisted development."
      },
      {
        "type": "website",
        "title": "Calm Coding: The Workflow That Makes Vibecoding Survivable",
        "url": "https://oc2sf.com/blog/calm-coding-vibecoding-survivable.html",
        "author": "Jan (OC2SF)",
        "published": "2026-02-01T00:00:00.000Z",
        "accessed": "2026-02-04T00:00:00.000Z",
        "annotation": "Mandates a 'Written Contract' (Spec) before any code is generated. Validates ASDLC's core thesis: 'You can't debug what was never designed.'"
      }
    ]
  },
  {
    "slug": "adr-authoring",
    "collection": "practices",
    "title": "ADR Authoring",
    "description": "Step-by-step guide for creating, organizing, and maintaining Architecture Decision Records in your codebase.",
    "status": "Live",
    "content": "## Definition\n\n**ADR Authoring** is the practice of writing, organizing, and maintaining Architecture Decision Records throughout a project's lifecycle. This practice implements [The ADR](/patterns/the-adr) pattern with concrete templates, file organization conventions, and lifecycle management.\n\nFollowing this practice produces a searchable, version-controlled archive of architectural decisions that serves both humans and agents.\n\n## When to Use\n\n**Write an ADR when:**\n\n- Making a technology choice (database, framework, language)\n- Choosing between architectural approaches (monolith vs microservices, REST vs GraphQL)\n- Accepting a significant trade-off (performance vs simplicity, vendor lock-in vs flexibility)\n- Establishing a constraint that will affect future work (\"all APIs must be versioned\")\n- Reverting or superseding a previous decision\n\n**Skip an ADR when:**\n\n- The decision is easily reversible (naming convention for a single file)\n- The decision is already documented in a feature spec\n- No reasonable alternatives exist (using HTTP for web requests)\n- The decision is entirely tactical, not architectural\n\n## Process\n\n### Step 1: Check for Existing ADRs\n\nBefore writing a new ADR, search for existing decisions in the same domain.\n\n```bash\n# Search for related ADRs\ngrep -r \"database\" docs/adrs/\n```\n\nIf a relevant ADR exists, you may need to **supersede** it rather than create a fresh decision.\n\n### Step 2: Choose an ID and Title\n\nAssign the next sequential ID and write a clear, descriptive title.\n\n**Format:** `ADR-NNN-short-descriptive-title.md`\n\n**Filename conventions:**\n- Use lowercase with hyphens\n- Keep it scannable (3-7 words after the ID)\n- Start with the domain if helpful: `ADR-015-auth-use-supabase.md`\n\n### Step 3: Document the Context\n\nThis is the most important section. Capture the forces that make this decision necessary:\n\n- What triggered this decision?\n- What constraints exist?\n- What are the non-negotiable requirements?\n- What is the timeline pressure?\n\n> [!TIP]\n> Write context as if explaining to someone joining the team next month. They should understand *why* this decision was needed, not just what was chosen.\n\n### Step 4: State the Decision\n\nWrite a clear, unambiguous statement of what was decided.\n\n**Good:** \"We will use PostgreSQL as the primary database for all transactional data.\"\n\n**Bad:** \"We decided to maybe consider PostgreSQL or something similar.\"\n\n### Step 5: Document Consequences\n\nList outcomes honestly—positive, negative, and neutral.\n\n**Positive:** What capabilities or benefits does this enable?\n\n**Negative:** What trade-offs are we accepting? What doors does this close?\n\n**Neutral:** What changes but isn't inherently good or bad?\n\n> [!WARNING]\n> ADRs with no negative consequences are suspicious. Every significant decision has trade-offs. Hiding them leads to confusion when the downsides surface later.\n\n### Step 6: Record Alternatives Considered\n\nFor each seriously considered alternative, explain why it was rejected. Use specific, concrete reasons—not vague dismissals.\n\n**Good:** \"Firebase Realtime Database rejected—would require a second database system and doesn't integrate with existing PostgreSQL data models.\"\n\n**Bad:** \"Firebase rejected—too complex.\"\n\n### Step 7: Set Status and Commit\n\nSet status to `Proposed` for review, or `Accepted` if the decision is final. Commit the ADR alongside related code changes when possible.\n\n```bash\ngit add docs/adrs/ADR-015-auth-use-supabase.md\ngit commit -m \"docs: add ADR-015 for Supabase Auth decision\"\n```\n\n## Template\n\n```markdown\n# ADR-NNN: {Title}\n\n**Status:** Proposed | Accepted | Deprecated | Superseded by ADR-XXX\n\n**Date:** YYYY-MM-DD\n\n## Context\n\n{What forces are at play? What problem needs solving? What constraints exist?}\n\n## Decision\n\n{What was decided? State clearly and unambiguously.}\n\n## Consequences\n\n**Positive:**\n- {Benefit 1}\n- {Benefit 2}\n\n**Negative:**\n- {Trade-off 1}\n- {Trade-off 2}\n\n**Neutral:**\n- {Change 1}\n\n## Alternatives Considered\n\n### {Alternative 1}\n\n{Description and rejection rationale}\n\n### {Alternative 2}\n\n{Description and rejection rationale}\n```\n\n## File Organization\n\nRecommended directory structure:\n\n```\ndocs/\n└── adrs/\n    ├── README.md              # Index and search tips\n    ├── ADR-001-use-postgres.md\n    ├── ADR-002-event-driven.md\n    ├── ADR-003-supabase-auth.md\n    └── ...\n```\n\nThe `README.md` should include:\n- Quick reference of active ADRs\n- Instructions for creating new ADRs\n- Search tips (grep patterns for common queries)\n\n## Lifecycle Management\n\n### Status Transitions\n\n```mermaid\nstateDiagram-v2\n    [*] --> Proposed\n    Proposed --> Accepted : Approved\n    Proposed --> Rejected : Rejected\n    Accepted --> Deprecated : No longer recommended\n    Accepted --> Superseded : Replaced by new ADR\n    Deprecated --> [*]\n    Superseded --> [*]\n    Rejected --> [*]\n```\n\n### Superseding an ADR\n\nWhen a decision is replaced:\n\n1. Create the new ADR with the updated decision\n2. Update the old ADR's status: `**Status:** Superseded by [ADR-NNN](./ADR-NNN-title.md)`\n3. Do **not** delete or modify the content of the superseded ADR\n\nThis preserves the archaeological record of how thinking evolved.\n\n## Common Mistakes\n\n### The Novel\n\n**Problem:** ADR is 10+ pages, covering multiple decisions.\n\n**Solution:** Split into multiple ADRs. Each ADR should decide exactly one thing.\n\n### The Hidden Trade-off\n\n**Problem:** Consequences section lists only positives.\n\n**Solution:** Force yourself to list at least one negative consequence. If you can't find any, you haven't thought hard enough.\n\n### The Vague Alternative\n\n**Problem:** Alternatives listed but rejection rationale is \"too complex\" or \"not a good fit.\"\n\n**Solution:** Be specific. What exactly made it complex? What would adopting it have required?\n\n### The Orphaned Decision\n\n**Problem:** ADR created but never reviewed or status remains \"Proposed\" indefinitely.\n\n**Solution:** Include ADR review in your PR/code review process. ADRs should move to \"Accepted\" or \"Rejected\" within one sprint.\n\n### The Missing Context\n\n**Problem:** Decision makes no sense without knowing the constraints that existed at the time.\n\n**Solution:** Write context as if for a new team member. Include timeline, budget, team skills, existing systems.\n\n## Agentic Integration\n\nADRs serve as high-value context for agents:\n\n**Include in agent context** when:\n- Working on code in the ADR's domain\n- Making decisions that might conflict with existing ADRs\n- Refactoring systems covered by architectural decisions\n\n**Agent-friendly practices:**\n- Keep ADRs in a consistent location (`docs/adrs/`)\n- Use clear, searchable titles\n- Tag ADRs with domain keywords in a frontmatter section (optional)\n\nAn agent working on authentication should be provided `ADR-003-supabase-auth.md` as context to avoid accidentally violating the architectural constraints.\n\n## Related Patterns\n\nThis practice implements:\n\n- **[The ADR](/patterns/the-adr)** — The structural pattern this practice executes\n\nSee also:\n\n- **[Living Specs](/practices/living-specs)** — For feature documentation that evolves (unlike immutable ADRs)\n- **[Request for Comments](/concepts/request-for-comments)** — For proposals that spawn multiple ADRs\n- **[Context Engineering](/concepts/context-engineering)** — ADRs as context sources for agents",
    "tags": ["Architecture", "Documentation", "Decision Making", "ADR"],
    "references": [
      {
        "type": "website",
        "title": "Documenting Architecture Decisions",
        "url": "https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions",
        "author": "Michael Nygard",
        "published": "2011-11-15T00:00:00.000Z",
        "accessed": "2026-01-28T00:00:00.000Z",
        "annotation": "The original ADR proposal with template and philosophy."
      },
      {
        "type": "website",
        "title": "ADR Tools",
        "url": "https://github.com/npryce/adr-tools",
        "accessed": "2026-01-28T00:00:00.000Z",
        "annotation": "Command-line tools for managing ADRs."
      }
    ]
  },
  {
    "slug": "adversarial-code-review",
    "collection": "practices",
    "title": "Adversarial Code Review",
    "description": "Executing automated verification using a Critic Agent to validate implementation artifacts against Spec contracts.",
    "status": "Live",
    "content": "## Definition\n\n**Adversarial Code Review** is the practice of automating code validation by employing a specialized **Critic Agent** to review claimed implementations against established [Spec](/patterns/the-spec) contracts and the [Agent Constitution](/patterns/agent-constitution).\n\nBy separating the \"Builder\" role from the \"Critic\" role, this practice ensures that verification remains objective and rigorous, catching architectural drifts, security vulnerabilities, and logic errors that might pass standard unit tests.\n\n## When to Use\n\nUse this practice to implement a high-reasoning verification gate before human review.\n\n**Use this practice when:**\n- A feature implementation is ready for review.\n- The project maintains clear [Specs](/patterns/the-spec) or [CLAUDE.md](/practices/agents-md-spec) constitutions.\n- You are using [Model Routing](/patterns/model-routing) to separate implementation and verification roles.\n- The risk of \"echo-chamber\" self-validation is high.\n\n**Skip this practice when:**\n- Performing trivial documentation fixes or small refactors with no logic changes.\n- Validating exploratory \"vibe coding\" prototypes where specs are not yet defined.\n\n## Process\n\n### Step 1: Fetch Issue Context\nRetrieve the source of truth for the work being reviewed. This typically involves getting details from a project management tool (like Linear) to understand the title, description, and acceptance criteria.\n\n### Step 2: Gather Implementation Artifacts\nIdentify what has changed. Check the `git status` for uncommitted changes or review recent commits associated with the issue ID. Prepare the diff or the set of modified files for the Critic Agent.\n\n### Step 3: Load Contracts\nIdentify the \"laws\" the implementation must follow. This includes:\n- Relevant functional specs in the `specs/` or `docs/` directory.\n- The project's **Constitution** (often `CLAUDE.md`), which contains architectural constraints and coding standards.\n\n### Step 4: Adversarial Review\nDeploy the **Critic Agent** with an adversarial persona. Instruct the agent to be skeptical by design and to prioritize rejecting violations over being \"helpful.\" Compare the code strictly against the loaded contracts.\n\n### Step 5: Identify Violations & Verdict\nAnalyze the Critic's output. If violations are found, categorize them by impact and provide specific remediation paths. If no violations are found against the contracts, issue a PASS verdict.\n\n## Templates\n\n### Critic Agent Prompt\nUse this template to configure a session or subagent for adversarial review.\n\n```markdown\n# Adversarial Code Review\n\nYou are a rigorous **Critic Agent** performing adversarial code review per ASDLC.io patterns.\n\nYour role is skeptical by design: reject code that violates the Spec or Constitution, even if it \"works.\" Favor false positives over false negatives.\n\n## Task\nReview the implementation claimed for: {issue_id_or_description}\n\n## Workflow\n1. **Fetch Context**: Review specs/{spec_name}.md and {constitution_file}.\n2. **Review Artifacts**: Analyze the provided code diff/files.\n3. **Compare Strictly**: Check against Spec contracts, Security (RLS/Auth), Type safety, and Design system tokens.\n4. **Identify Violations**: For each issue, cite the clause violated, the impact, and the remediation path.\n\n## Output Format\n\n### If No Violations Found:\n## Verdict: PASS\n[Summary of what was reviewed and why it passes]\n\n### If Violations Found:\n## Verdict: NOT READY TO MERGE\n\n### Acceptance Criteria Check\n| Criterion | Status | Notes |\n|-----------|--------|-------|\n| {criterion} | {status} | {notes} |\n\n### Violations Found\n**1. [Category]: [Brief description]**\n- **Violated**: [Spec section or rule]\n- **Impact**: [Why this matters]\n- **Remediation**: [How to fix]\n```\n\n## Common Mistakes\n\n### Using the Same Session\n**Problem:** Allowing the Builder Agent to review its own work within the same chat history.\n**Solution:** Always start a fresh session or use a distinct subagent with a high-reasoning model for the review.\n\n### Vague Violation Reports\n**Problem:** The Critic flags an issue but doesn't explain *why* it's a violation or how to fix it.\n**Solution:** Enforce a structured output format that requires citing specific spec clauses and providing remediation steps.\n\n## Related Patterns\n\nThis practice implements:\n- **[Adversarial Code Review](/patterns/adversarial-code-review)** — The core architectural pattern of separated verification roles.\n- **[The Spec](/patterns/the-spec)** — The source of truth used for validation.\n- **[Agent Constitution](/patterns/agent-constitution)** — The set of behavioral and technical constraints enforced during review.",
    "tags": ["Code Review", "Quality Gates", "Multi-Agent", "Verification"],
    "references": []
  },
  {
    "slug": "adversarial-requirement-review",
    "collection": "practices",
    "title": "Adversarial Requirement Review",
    "description": "A verification practice where a Critic Agent challenges the problem statement and assumptions before any specification or code is written.",
    "status": "Experimental",
    "content": "# Adversarial Requirement Review\n\n## Definition\n\n**Adversarial Requirement Review** is a verification practice where a **Thought Partner** agent (acting as an adversarial critic) challenges the problem statement, underlying assumptions, and strategy *before* any specification is written or implementation begins.\n\nThis shifts the \"adversarial\" concept left—from reviewing code ([Adversarial Code Review](/practices/adversarial-code-review)) to reviewing the *intent* itself.\n\n## When to Use\n\n**Use this practice when:**\n- Validating a new feature idea before writing a Spec.\n- You have a PBI but suspect the \"Why\" is weak.\n- Stakeholders ask for a specific solution (\"We need a dashboard\") without explaining the problem.\n\n**Skip this practice when:**\n- Fixing bugs (unless the bug reveals a flawed requirement).\n- Implementing purely technical tasks where the \"Why\" is established (e.g., library upgrades).\n\n## The Problem: The Backwards Approach\n\nIn traditional development, and accelerated by AI, we often start with \"How do we build X?\" rather than \"Is X the right problem to solve?\".\n\n**The Backwards Problem:**\n1. Stakeholder has an idea (\"We need a weekly email report\").\n2. Engineer/AI jumps to implementation (\"I'll set up a cron job...\").\n3. Feature ships quickly.\n4. Feature fails because the underlying problem was misunderstood (e.g., users needed real-time data, not weekly snapshots).\n\nAI exacerbates this by making implementation so cheap that we skip validation. We build the wrong thing faster than ever.\n\n## The Solution: Thought Partner vs. Leader\n\nTo break this cycle, we separate the roles:\n\n- **Human (Thought Leader):** Provides context, judgment, and final decisions.\n- **AI (Thought Partner/Critic):** Provides challenges, alternative angles, and stress-testing questioning.\n\nThe goal is not for the AI to *solve* the problem, but to *sharpen* the problem definition.\n\n## The Workflow\n\nThis pattern consists of three distinct phases of challenge.\n\n### 1. The Problem Sharpener\n**Goal:** Clarify the problem statement and remove implied solutions.\n\n**Prompt Pattern:**\n> \"I'm going to describe a problem I'm trying to solve. I want you to act as a **Thought Partner** - not to solve it, but to help me understand it better.\n> \n> After I describe the problem, **interview me one question at a time** to:\n> - Clarify who exactly is affected and when\n> - Surface barriers I might be glossing over\n> - Identify assumptions I'm making without realizing it\n> - Challenge whether I've framed the problem correctly\n> \n> Don't suggest solutions. Help me see the problem more clearly.\n> \n> Here's the problem: [describe your problem]\"\n\n### 2. The Assumption Surfacer\n**Goal:** Identify risky beliefs that must be true for the strategy to succeed.\n\n**Prompt Pattern:**\n> \"I'm considering this product strategy: [describe what you're building and why].\n> \n> What assumptions am I making that **must be true** for this to work?\n> \n> Focus on:\n> - **Behavior:** Will people actually change their behavior to use it? (Desire != Action)\n> - **Value:** Is it worth building? Does the value justify the cost?\n> - **Alternatives:** What am I deprioritizing, and what is the cost of leaving that unsolved?\n> \n> List 5-7 assumptions, starting with the ones most likely to be wrong.\"\n\n### 3. The Pre-Build Stress Test\n**Goal:** Final pressure test before committing to a Spec or PBI.\n\n**Prompt Pattern:**\n> \"Before I commit to building this, I want to pressure-test the idea.\n> \n> Context: [describe what you're planning to build and the problem it solves]\n> \n> Act as a **skeptical but constructive advisor**. Interview me one question at a time to find weaknesses in my thinking. Push back where my reasoning seems thin. Help me discover what I don't know before I invest in building.\"\n\n## Integration with ASDLC\n\nThis practice operates at the **beginning of the second diamond** (The Solution Space), acting as the bridge between \"Insight\" and \"Specification\".\n\n- **Diamond 1 (Discover/Define):** Ends with a Problem Graph or validated Insight.\n- **Diamond 2 (Develop/Deliver):** Starts with this practice to validate the strategy *before* writing the Spec.\n\nIt ensures that we don't proceed to **[Spec-Driven Development](/concepts/spec-driven-development)** with a flawed premise.\n\n**Output:**\nThe output of this review is a validated **Problem Statement** and **Strategy**, which then becomes the \"Context\" section of your **[Spec](/patterns/the-spec)**.\n\n## Related Practices\n\n- **[Adversarial Code Review](/practices/adversarial-code-review)**: The downstream equivalent. While Requirement Review verifies the *Why*, Code Review verifies the *How*.\n- **[Spec-Driven Development](/concepts/spec-driven-development)**: This practice ensures the Spec is worth writing.\n\n## Related Patterns\n\n- **[Agentic Double Diamond](/patterns/agentic-double-diamond)** — The broader design framework where this practice serves as the gate to the second diamond.\n- **[Product Thinking](/concepts/product-thinking)**: The mindset that this practice operationalizes.\n\n## References\n\n1. **Before I Ask AI to Build, I Ask It to Challenge**\n   Author: Daniel Donbavand\n   Published: 2026-02-12\n   URL: https://danieldonbavand.com/2026/02/12/before-i-ask-ai-to-build-i-ask-it-to-challenge/\n   *Source of the \"Problem Sharpener,\" \"Assumption Surfacer,\" and \"Pre-Build Stress Test\" prompts.*",
    "tags": ["practice", "verification", "product-thinking", "requirements"],
    "references": []
  },
  {
    "slug": "agent-personas",
    "collection": "practices",
    "title": "Agent Personas",
    "description": "A guide on how to add multiple personas to an AGENTS.md file, with examples.",
    "status": "Live",
    "content": "## Definition\n\nDefining clear personas for your agents is crucial for ensuring they understand their role, trigger constraints, and goals. This guide demonstrates how to structure multiple personas within your `AGENTS.md` file.\n\nPersonas are a context engineering practice—they scope agent work by defining boundaries and focus, not by role-playing. When combined with [Model Routing](/patterns/model-routing), personas can also specify which computational tool (LLM) to use for each type of work.\n\nFor the full specification of the `AGENTS.md` file, see the [AGENTS.md Specification](./agents-md-spec).\n\n## When to Use\n\n**Use this practice when:**\n- Your `AGENTS.md` is becoming a monolith of conflicting instructions\n- You have distinct workflows (e.g., Coding vs. Writing vs. Architecting)\n- You need to support specialized sub-agents with narrow scopes\n- You are hitting context window limits with a single generic instruction set\n\n**Skip this practice when:**\n- Variable roles are handled purely by \"Model Routing\" (manual model selection)\n- The project is simple enough for a single \"General Developer\" persona\n\n## How to Add Multiple Personas\n\nYou can define multiple personas by specifying triggers, goals, and guidelines for each. This allows different agents (or the same agent in different contexts) to adopt specific behaviors suited for the task at hand.\n\n### Example: Our Internal Personas\n\nBelow are the personas we use, serving as a template for your own `AGENTS.md`.\n\n```markdown\n### 1.1. Lead Developer / Astro Architect (@Lead)\n**Trigger:** When asked about system design, specs, or planning.\n* **Goal**: Specify feature requirements, architecture, and required changes. Analyze the project state and plan next steps.\n* **Guidelines**\n  - **Schema Design:** When creating new content types, immediately define the Zod schema in `src/content/config.ts`.\n  - **Routing:** Use Astro's file-based routing. For dynamic docs, use `[...slug].astro` and `getStaticPaths()`.\n  - **SEO:** Ensure canonical URLs and Open Graph tags are generated for every new page.\n  - **Dev Performace:** Focus on tangible, deliverable outcomes.\n  - **Spec driven development:** Always produce clear, concise specifications before handing off to implementation agents.\n  - **Planned iterations:** Break down large tasks into manageable PBIs with clear acceptance criteria.\n\n### 1.2. Designer / User Experience Lead (@Designer)\n**Trigger:** When asked about Design system UI/UX, design systems, or visual consistency.\n* **Goal**: Ensure the design system can be effectively utilized by agents and humans alike.\n* **Guidelines**\n  - **Design Tokens:** Tokens must be set in `src/styles/tokens.css`. No hardcoded colors or fonts.\n  - **Component Consistency:** All components must adhere to the design system documented in `src/pages/resources/design-system.astro`. \n  - **Accessibility:** Ensure all components meet WCAG 2.1 AA standards.\n  - **Documentation:** Update the Design System page with any new components or styles introduced.\n  - **Experience Modeling Allowed:** Design system components are protected by a commit rule: use \\[EM] tag to override the rule.\n  \n### 1.3. Content Engineer / Technical Writer (@Content)\n**Trigger:** When asked to create or update documentation, articles, or knowledge base entries.\n* **Goal**: Produce high-quality, structured content that adheres to the project's schema and style guidelines.\n* **Guidelines**\n  - **Content Structure:** Follow the established folder structure in `src/content/` for concepts\n  \n### 1.4. Developer / Implementation Agent (@Dev)\n**Trigger:** When assigned implementation tasks or bug fixes.\n* **Goal**: Implement features, fix bugs, and ensure the codebase remains healthy and maintainable.\n* **Guidelines**\n  - **Expect PBIs:** Always work from a defined Product Backlog Item (PBI) with clear acceptance criteria, if available.\n  - **Type Safety:** Use TypeScript strictly. No `any` types allowed.\n  - **Component Imports:** Explicitly import all components used in `.astro` files.\n  - **Testing:** Ensure all changes pass `pnpm check` and `pnpm lint`\n  - **Document progress:** Update the relevant PBI in `docs/backlog/` with status and notes.md after completing tasks.\n```\n\n## Model Routing and Personas\n\nPersonas define **what work to do** and **how to scope it**. [Model Routing](/patterns/model-routing) is a separate practice that defines **which computational tool to use**.\n\n### Current State (December 2025)\n\nAI-assisted IDEs (Cursor, Windsurf, Claude Code) do **not** automatically select models based on persona definitions. Model selection is manual.\n\n### Best Practice: Keep Them Separate\n\n**Don't add model profiles to `AGENTS.md`** - It adds noise to the context window without providing automation value.\n\nInstead:\n1. **Keep personas focused** on triggers, goals, and guidelines\n2. **Use Model Routing separately** - Manually select models based on the task characteristics\n3. **Reference the pattern** when deciding which model to use\n\n### Matching Personas to Model Profiles\n\nWhen you invoke a persona, choose your model based on the work type:\n\n| Persona Type | Typical Work | Recommended Profile |\n|---|---|---|\n| Lead / Architect | System design, specs, architectural decisions | High Reasoning |\n| Developer / Implementation | Code generation, refactoring, tests | High Throughput |\n| Documentation Analyst | Legacy code analysis, comprehensive docs | Massive Context |\n\nThe workflow:\n1. **Identify the persona** needed for your task\n2. **Select the appropriate model** manually in your IDE\n3. **Invoke the persona** with your prompt\n\nThis keeps `AGENTS.md` lean and focused on scoping agent work, while model selection remains a deliberate engineering decision.",
    "tags": ["agents", "personas", "guide"],
    "references": []
  },
  {
    "slug": "agents-md-spec",
    "collection": "practices",
    "title": "AGENTS.md Specification",
    "description": "The definitive guide to the AGENTS.md file, including philosophy, anatomy, and implementation strategy.",
    "status": "Live",
    "content": "## DEFINITION\n\n`AGENTS.md` is an open format for guiding coding agents, acting as a \"README for agents.\" It provides a dedicated, predictable place for context and instructions—such as build steps, tests, and conventions—that help AI coding agents work effectively on a project.\n\nWe align with the [agents.md specification](https://agents.md), treating this file as the authoritative source of truth for agentic behavior within the ASDLC.\n\n## When to Use\n\n**Use this practice when:**\n- Establishing a new repository for AI-assisted development\n- Onboarding new AI tools (Cursor, Windsurf, Claude) to an existing project\n- You need to standardize agent behavior across a team\n- AI agents are hallucinating dependencies or breaking architectural rules\n\n**Skip this practice when:**\n- The project is a temporary script or throwaway prototype\n- You are not using any agentic tools or LLM assistants\n\n## CORE PHILOSOPHY\n\n**1. A README for Agents**\n\nJust as `README.md` is for humans, `AGENTS.md` is for agents. It complements existing documentation by containing the detailed context—build commands, strict style guides, and test instructions—that agents need but might clutter a human-facing README.\n\n**2. Context is Code**\n\nIn the ASDLC, we treat `AGENTS.md` with the same rigor as production software:\n\n- **Version Controlled**: Tracked via git and PRs.\n- **Falsifiable**: Contains clear success criteria for agent actions.\n- **Optimized**: Structured to maximize signal-to-noise ratio for LLM context windows, preventing \"Lost in the Middle\" issues.\n\n**3. The Context Anchor (Long-Term Memory)**\n\nAGENTS.md solves the \"Context Amnesia\" problem. Agents are stateless—each new session starts with blank context. Without grounding, the agent reverts to generic training weights, forgetting project-specific patterns and lessons learned.\n\nThe `AGENTS.md` file acts as persistent \"standing orders\" for the agent across different sessions. By documenting your research tools, coding styles, architectural decisions, and accumulated lessons here, you prevent session-to-session drift.\n\nThis transforms `AGENTS.md` from a simple configuration file into the project's **institutional memory** for AI collaboration.\n\n## Format Philosophy\n\nThe structures in this specification (YAML maps, XML standards, tiered boundaries) are optimized for large teams and complex codebases. For smaller projects:\n\n- A simple markdown list may suffice\n- Focus on the *concepts* (persona, boundaries, commands) rather than exact syntax\n- Iterate on what produces best adherence from your specific model\n\nThe goal is signal density, not format compliance. Overly rigid specs create adoption friction. Let teams scale complexity to their needs.\n\n## TOOL-SPECIFIC CONSIDERATIONS\n\nDifferent AI coding tools look for different filenames. While `AGENTS.md` is the emerging standard, some tools require specific naming:\n\n| Tool | Expected Filename | Notes |\n| :--- | :--- | :--- |\n| **Cursor** | `.cursorrules` | Also reads `AGENTS.md` |\n| **Windsurf** | `.windsurfrules` | Also reads `AGENTS.md` |\n| **Claude Code** | `CLAUDE.md` | Does not read `AGENTS.md`; case-sensitive |\n| **Codex** | `AGENTS.md` | Native support |\n| **Zed** | `.rules` | Priority-based; reads `AGENTS.md` at lower priority |\n| **VS Code / Copilot** | `AGENTS.md` | Requires `chat.useAgentsMdFile` setting enabled |\n\n### Zed Priority Order\n\nZed uses the first matching file from this list:\n1. `.rules`\n2. `.cursorrules`\n3. `.windsurfrules`\n4. `.clinerules`\n5. `.github/copilot-instructions.md`\n6. `AGENT.md`\n7. `AGENTS.md`\n8. `CLAUDE.md`\n9. `GEMINI.md`\n\n### VS Code Configuration\n\nVS Code requires explicit opt-in for `AGENTS.md` support:\n- Enable `chat.useAgentsMdFile` setting to use `AGENTS.md`\n- Enable `chat.useNestedAgentsMdFiles` for subfolder-specific instructions\n\n### Recommendation\n\nCreate a symlink to support Claude Code without duplicating content:\n\n```bash\nln -s AGENTS.md CLAUDE.md\n```\n\nThis ensures Claude Code users get the same guidance while maintaining a single source of truth. Note that Claude Code also supports `CLAUDE.local.md` for personal preferences that shouldn't be version-controlled.\n\n## ECOSYSTEM TOOLS\n\nAs AGENTS.md adoption grows, tools emerge to bridge compatibility gaps between different coding assistants and enforce standards across heterogeneous environments.\n\n### Ruler\n\n[Ruler](https://github.com/intellectronica/ruler) is a meta-tool that synthesizes agent instructions from multiple sources (AGENTS.md, .cursorrules, project conventions) and injects them into coding assistants that may not natively support the AGENTS.md standard.\n\n**Key capabilities:**\n- **Cross-platform normalization**: Translates AGENTS.md into assistant-specific formats\n- **Multi-source aggregation**: Combines rules from various config files into unified context\n- **Dynamic injection**: Ensures consistent agent behavior across tools like Cursor, Windsurf, and Claude Code\n\n**Use case:** Teams using multiple coding assistants (e.g., some developers on Cursor, others on Claude Code) can maintain a single source of truth in AGENTS.md while Ruler handles distribution to tool-specific formats.\n\nThis demonstrates ecosystem maturity: when third-party tools emerge to solve interoperability problems, the standard has achieved meaningful adoption.\n\n## ASDLC IMPLEMENTATION STRATEGY\n\nWhile the [agents.md](https://agents.md) standard provides the format, the ASDLC recommends a structured implementation to ensure reliability. We present our `AGENTS.md` format not just as a list of tips, but as a segmented database of rules. This is *one* valid implementation strategy, particularly suited for rigorous engineering environments.\n\n### 1. Identity Anchoring (The Persona)\n\nEstablishes the specific expertise required to prune the model's search space. Without this, the model reverts to the \"average\" developer found in its training data. For detailed examples of defining multiple personas, see [Agent Personas](./agent-personas).\n\nBad: \"You are a coding assistant.\"\n\nGood: \"You are a Principal Systems Engineer specializing in Go 1.22, gRPC, and high-throughput concurrency patterns. You favor composition over inheritance.\"\n\n### 2. Contextual Alignment (The Mission)\n\nA concise, high-level summary of the project’s purpose and business domain. This is often formatted as a blockquote at the top of the file to \"set the stage\" for the agent's session.\n\n- **Why:** LLMs are stateless. A 50-token description differentiates a \"User\" in a banking app (high security/compliance) from a \"User\" in a casual game (low friction), reducing the need for corrective follow-up prompts.\n    \n- **Format:** Focus on the \"What\" and \"Why,\" not the narrative history.\n    \n\n**Example:**\n\n> **Project:** \"ZenTask\" - A minimalist productivity app. **Core Philosophy:** Local-first data architecture; offline support is mandatory.\n\n### 3. Operational Grounding (The Tech Stack)\n\nExplicitly defines the software environment to prevent \"Library Hallucination.\" This section must be exhaustive regarding key dependencies and restrictive regarding alternatives.\n\n- Directive: \"Runtime: Node.js v20 (LTS) exclusively.\"\n- Directive: \"Styling: Tailwind CSS only. Do not use CSS Modules or Emotion.\"\n- Directive: \"State: Zustand only. Do not use Redux.\"\n\n### 4. Behavioral Boundaries (Context Gates)\n\nReplaces vague \"Guardrails\" with a \"Three-Tiered Boundary\" system, forming the [Agent Constitution](/patterns/agent-constitution). As the models are probabilistic, absolute prohibitions are unrealistic. Instead, this system categorizes rules by severity and required action. These rules are aimed to reducing the likelihood of critical errors. Note that you should always complement\nthe _constitution_ with explicit and deterministic _quality gates_ enforced by tests, linters, and CI/CD pipelines.\n\n**Tier 1 (Constitutive - ALWAYS): Non-negotiable standards.**\n\nExample: \"Always add JSDoc to exported functions.\"\n\n**Tier 2 (Procedural - ASK): High-risk operations requiring Human-in-the-Loop.**\n\nExample: \"Ask before running database migrations or deleting files.\"\n\n**Tier 3 (Hard Constraints - NEVER): Safety limits.**\n\nExample: \"Never commit secrets, API keys, or .env files.\"\n\n### 5. Semantic Directory Mapping\n\nWhen documenting the codebase structure in AGENTS.md, prefer Annotated YAML over ASCII trees.\n\n- **Use Valid Syntax:** Ensure the block allows an LLM to parse the structure as a dictionary.\n- **Annotate Key Files:** do not just list files; map them to a brief string describing their responsibility. This acts as a 'map legend' for the Agent, allowing it to route coding tasks to the correct file without needing to scan the file content first.\n- **Omit Noise:** Only include directories and files relevant to the Agent's operation or the architectural scope.\n\n**Example:**\n\n```yaml\ndirectory_map:\n  src:\n    # Core Application Logic\n    main.py: \"Application entry point; initializes the Agent Orchestrator\"\n    \n    agents:\n      # Individual Agent definitions\n      base_agent.py: \"Abstract base class defining the 'step()' and 'memory' interfaces\"\n    \n    utils:\n      # Shared libraries\n      llm_client.py: \"Wrapper for OpenAI/Anthropic APIs with retry logic\"\n```\n\n### 6. The Command Registry\n\nA lookup table mapping intent to execution. Agents often default to standard commands (npm test) which may fail in custom environments (make test-unit). The Registry forces specific tool usage.\n\n| Intent | Command | Notes |\n| :--- | :--- | :--- |\n| **Build** | pnpm build | Outputs to `dist/` |\n| **Test** | pnpm test:unit | Flags: --watch=false |\n| **Lint** | pnpm lint --fix | Self-correction enabled |\n\n\n### 7. Implementation notes\n\nXML-Tagging for Semantic Parsing\n\nTo maximize adherence, use pseudo-XML tags to encapsulate rules. This creates a \"schema\" that the model can parse more strictly than bullet points.\n\n```xml\n<coding_standard name=\"React Hooks\">\n  <instruction>\n    Use functional components and Hooks. Avoid Class components.\n    Ensure extensive use of custom hooks for logic reuse.\n  </instruction>\n  <anti_pattern>\n    class MyComponent extends React.Component {... }\n  </anti_pattern>\n  <preferred_pattern>\n    const MyComponent = () => {... }\n  </preferred_pattern>\n</coding_standard>\n```\n\n## REFERENCE TEMPLATE\n\n`Filename: AGENTS.md`\n\n```md\n# AGENTS.md - Context & Rules for AI Agents\n\n> **Project Mission:** High-throughput gRPC service for processing real-time financial transactions.\n> **Core Constraints:** Zero-trust security model, ACID compliance required for all writes.\n\n## 1. Identity & Persona\n- **Role:** Senior Systems Engineer\n- **Specialization:** High-throughput distributed systems in Go.\n- **Objective:** Write performant, thread-safe, and maintainable code.\n\n## 2. Tech Stack (Ground Truth)\n- **Language:** Go 1.22 (Use `iter` package for loops)\n- **Transport:** gRPC (Protobuf v3)\n- **Database:** PostgreSQL 15 with `pgx` driver (No ORM allowed)\n- **Infra:** Kubernetes, Helm, Docker\n\n## 3. Operational Boundaries (CRITICAL)\n- **NEVER** commit secrets, tokens, or `.env` files.\n- **NEVER** modify `api/proto` without running `buf generate`.\n- **ALWAYS** handle errors; never use `_` to ignore errors.\n- **ASK** before adding external dependencies.\n\n## 4. Command Registry\n| Action | Command | Note |\n| :--- | :--- | :--- |\n| **Build** | `make build` | Outputs to `./bin` |\n| **Test** | `make test` | Runs with `-race` detector |\n| **Lint** | `golangci-lint run` | Must pass before commit |\n| **Gen** | `make proto` | Regenerates gRPC stubs |\n\n## 5. Development Map\n```yaml\ndirectory_map:\n  api:\n    proto: \"Protocol Buffers definitions (Source of Truth)\"\n  cmd:\n    server: \"Main entry point, dependency injection wire-up\"\n  internal:\n    biz: \"Business logic and domain entities (Pure Go)\"\n    data: \"Data access layer (Postgres + pgx)\"\n```\n\n## 6. Coding Standards\n```xml\n<rule_set name=\"Concurrency\">\n  <instruction>Use `errgroup` for managing goroutines. Avoid bare `go` routines.</instruction>\n  <example>\n    <bad>go func() {... }()</bad>\n    <good>g.Go(func() error {... })</good>\n  </example>\n</rule_set>\n```\n\n## 7. Context References\n- **Database Schema:** Read `@database/schema.sql`\n- **API Contracts:** Read `@api/v1/service.proto`\n```",
    "tags": ["governance", "agents", "specification"],
    "references": [
      {
        "type": "video",
        "title": "Beyond Vibe-Coding: Learn Effective AI-Assisted Coding in 4 minutes",
        "url": "https://www.youtube.com/watch?v=HR5f2TDC65E",
        "publisher": "Vanishing Gradients",
        "annotation": "Source material for the Context Anchor concept. Explains how persistent context files ground AI agents across sessions."
      },
      {
        "type": "website",
        "title": "AGENTS.md outperforms skills in our agent evals",
        "url": "https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals",
        "accessed": "2026-01-30T00:00:00.000Z",
        "publisher": "Vercel",
        "annotation": "Empirical validation of passive context approach. Compressed 8KB AGENTS.md achieved 100% pass rate on Next.js 16 API evals vs 79% for skills with explicit instructions. Demonstrates value of 'retrieval-led reasoning' over training data."
      }
    ]
  },
  {
    "slug": "constitutional-review-implementation",
    "collection": "practices",
    "title": "Constitutional Review Implementation",
    "description": "Step-by-step guide for implementing Constitutional Review to validate code against both Spec and Constitution contracts.",
    "status": "Experimental",
    "content": "## Definition\n\n**Constitutional Review Implementation** is the operational practice of configuring and executing [Constitutional Review](/patterns/constitutional-review) to validate code against both functional requirements (the Spec) and architectural values (the Constitution).\n\nThis practice extends [Adversarial Code Review](/patterns/adversarial-code-review) by adding constitutional constraints to the Critic Agent's validation criteria.\n\n## When to Use\n\n**Use this practice when:**\n\n- Your project has documented architectural principles in an [Agent Constitution](/patterns/agent-constitution)\n- Code passes tests but you've experienced architectural violations in production\n- You want to enforce non-functional requirements (performance, security, data access patterns)\n- Your team needs to prevent \"regression to mediocrity\" (LLMs generating internet-average code)\n\n**Skip this practice when:**\n\n- You don't have an Agent Constitution documented (implement [AGENTS.md Specification](/practices/agents-md-spec) first)\n- Your project is a prototype without architectural constraints\n- The overhead of dual-contract validation exceeds the benefit (very small projects)\n\n## Prerequisites\n\nBefore implementing Constitutional Review, ensure you have:\n\n1. **[Agent Constitution](/patterns/agent-constitution)** documented (typically `AGENTS.md`)\n2. **[The Spec](/patterns/the-spec)** for the feature being reviewed\n3. **Critic Agent session** separate from the Builder Agent (fresh context)\n4. **Architectural constraints** clearly defined in the Constitution\n\nIf architectural constraints aren't documented, start with [AGENTS.md Specification](/practices/agents-md-spec).\n\n## Process\n\n### Step 1: Document Architectural Constraints in Constitution\n\nEnsure your Agent Constitution includes **non-functional constraints** that are:\n- **Specific** (not \"be performant\" but \"push filtering to database layer\")\n- **Testable** (can be objectively verified)\n- **Scoped** (applies to specific categories: Data Access, Performance, Security)\n\n**Example Structure**:\n\n```markdown\n## Architectural Constraints\n\n### Data Access\n- All filtering operations MUST be pushed to the database layer\n- Never use `findAll()` or `LoadAll()` followed by in-memory filtering\n- Queries must handle 10k+ records without memory issues\n\n### Performance\n- API responses < 200ms at p99\n- Database queries must use indexes for common filters\n- No N+1 query patterns\n\n### Security\n- User IDs never logged (use hashed identifiers)\n- All inputs validated against Zod schemas before processing\n- Authentication tokens expire within 24 hours\n- No hardcoded secrets (use environment variables)\n\n### Error Handling\n- Never fail silently (all errors logged with context)\n- User-facing errors never expose stack traces\n- Database errors map to generic \"Service unavailable\" messages\n```\n\n### Step 2: Configure Critic Agent Prompt\n\nExtend the standard [Adversarial Code Review](/patterns/adversarial-code-review) prompt to include constitutional validation.\n\n**System Prompt Template**:\n\n```\nYou are a rigorous Code Reviewer validating implementation against TWO sources of truth:\n\n1. The Spec (/plans/{feature-name}/spec.md)\n   - Functional requirements (what should it do?)\n   - API contracts (what are the inputs/outputs?)\n   - Data schemas (what is the structure?)\n\n2. The Constitution (AGENTS.md)\n   - Architectural patterns (e.g., \"push filtering to DB\")\n   - Performance constraints (e.g., \"queries handle 10k+ records\")\n   - Security rules (e.g., \"never log user IDs\")\n   - Error handling policies (e.g., \"never fail silently\")\n\nYOUR JOB:\nIdentify where code satisfies the Spec (functional) but violates the Constitution (architectural).\n\nCOMMON CONSTITUTIONAL VIOLATIONS TO CHECK:\n- LoadAll().Filter() pattern (data access violation)\n- Hardcoded secrets (security violation)\n- Missing error logging (error handling violation)\n- N+1 query patterns (performance violation)\n- User IDs in logs (security violation)\n\nOUTPUT FORMAT:\nFor each violation:\n1. Type: Constitutional Violation - [Category]\n2. Location: File path and line number\n3. Issue: What constitutional principle is violated\n4. Impact: Why this matters at scale (performance, security, maintainability)\n5. Remediation Path: Ordered steps to fix (prefer standard patterns, escalate if needed)\n6. Test Requirements: What tests would prevent regression\n\nIf no violations found, output: PASS - Constitutional Review\n```\n\n### Step 3: Execute Constitutional Review Workflow\n\nFollow this sequence to ensure proper validation:\n\n```\n┌─────────────┐\n│   Builder   │ → Implements Spec\n└──────┬──────┘\n       ↓\n┌─────────────────┐\n│  Quality Gates  │ → Tests, types, linting (deterministic)\n└──────┬──────────┘\n       ↓ (pass)\n┌──────────────────┐\n│ Spec Compliance  │ → Does it meet functional requirements?\n│     Review       │    (Adversarial Code Review)\n└──────┬───────────┘\n       ↓ (pass)\n┌──────────────────┐\n│ Constitutional   │ → Does it follow architectural principles?\n│     Review       │    (This practice)\n└──────┬───────────┘\n       ↓ (pass)\n┌─────────────────┐\n│ Acceptance Gate │ → Human strategic review (is it the right thing?)\n└─────────────────┘\n```\n\n**Execution Steps:**\n\n1. **Builder completes implementation** — Code written, tests pass\n2. **Quality Gates pass** — Compilation, linting, unit tests all green\n3. **Spec Compliance Review** — Critic validates functional requirements met\n4. **⭐ Constitutional Review** — Critic validates architectural principles followed:\n   - Open **new Critic Agent session** (fresh context, no Builder bias)\n   - Provide **Constitution** (AGENTS.md)\n   - Provide **Spec** (feature spec file)\n   - Provide **Code Diff** (changed files only)\n   - Use **Constitutional Review prompt** (from Step 2)\n   - Critic outputs violations or PASS\n5. **If violations found** → Return to Builder with remediation path\n6. **If PASS** → Proceed to Acceptance Gate (human review)\n\n### Step 4: Process Violation Reports\n\nWhen the Critic identifies constitutional violations, the output will follow this format:\n\n```\nVIOLATION: Constitutional - Data Access Pattern\n\nLocation: src/audit/AuditService.cs Line 23\n\nIssue: Loads all records into memory before filtering\nConstitution Violation: \"All filtering operations MUST be pushed to database layer\"\n\nImpact: \n- Works fine with small datasets (< 1k records)\n- Breaks at scale (10k+ records cause memory issues)\n- Creates N+1 query patterns in related queries\n- Violates performance SLA (API responses > 200ms)\n\nRemediation Path:\n1. Push filter to database query:\n   repository.FindWhere(x => x.Date > startDate)\n2. If ORM doesn't support this pattern, use raw SQL:\n   SELECT * FROM audit_logs WHERE date > @startDate\n3. Add performance test with 10k+ mock records to prevent regression\n4. Document the constraint in repository interface comments\n\nTest Requirements:\n- Add test: \"GetLogs with 10k records completes in < 200ms\"\n- Add test: \"GetLogs does not load entire table into memory\"\n  (mock repository, verify FindWhere called, not LoadAll)\n```\n\n**Processing steps:**\n\n1. **Return to Builder Agent** with full violation report\n2. **Builder implements remediation** following the ordered path\n3. **Re-run Constitutional Review** after fixes\n4. **Iterate until PASS** (typically 1-2 cycles)\n\n### Step 5: Update Constitution Based on Violations\n\nIf the Critic struggles to validate or produces unclear violations:\n\n**Indicators Constitution needs update:**\n- Critic says \"unclear whether this violates principles\"\n- False positives (valid code flagged as violation)\n- Violations lack specific remediation paths\n\n**Action**: Refine constitutional constraints:\n\n```markdown\n## Before (vague)\n### Performance\n- Code should be fast\n\n## After (specific)\n### Performance\n- API responses < 200ms at p99\n- Database queries must use indexes for common filters\n- Push filtering to database layer (never LoadAll().Filter())\n```\n\n## Examples\n\n### Example 1: Data Access Violation\n\n**Code Submitted**:\n```typescript\nasync function getActiveUsers() {\n  const users = await db.users.findAll();\n  return users.filter(u => u.status === 'active');\n}\n```\n\n**Constitutional Review Output**:\n```\nVIOLATION: Constitutional - Data Access Pattern\n\nLocation: src/users/service.ts Line 12\n\nIssue: Loads all users then filters in-memory\nConstitution Violation: \"Push filtering to database layer\"\n\nImpact: Works with 100 users, breaks at 10k+\n\nRemediation:\n1. db.users.findWhere({ status: 'active' })\n2. Add test with 10k users to verify performance\n\nTest Requirements:\n- Performance test: 10k users returns in < 200ms\n- Verify db.users.findWhere called (not findAll)\n```\n\n### Example 2: Security Violation\n\n**Code Submitted**:\n```typescript\nlogger.info(`User ${userId} logged in from ${ipAddress}`);\n```\n\n**Constitutional Review Output**:\n```\nVIOLATION: Constitutional - Security\n\nLocation: src/auth/logger.ts Line 45\n\nIssue: Logs user ID directly\nConstitution Violation: \"Never log user IDs (use hashed identifiers)\"\n\nImpact: GDPR compliance risk, audit log exposure\n\nRemediation:\n1. Hash user ID: logger.info(`User ${hashUserId(userId)} logged in...`)\n2. Implement hashUserId utility (SHA-256 with salt)\n3. Update all logging to use hashed IDs\n\nTest Requirements:\n- Verify logs do not contain raw user IDs\n- Verify hashed IDs are consistent (same user = same hash)\n```\n\n## Implementation Constraints\n\n**Requires Clear Constitutional Principles** — Vague constraints produce vague critiques. \"Be performant\" is not actionable. \"API responses < 200ms at p99\" is.\n\n**Not Fully Automated (Yet)** — As of January 2026, requires manual orchestration. You must manually:\n- Start new Critic Agent session\n- Provide Constitution + Spec + Code Diff\n- Interpret violation reports\n\n**Model Capability Variance** — Not all reasoning models perform equally at constitutional review. Recommended:\n- **High Reasoning models** for Critic (DeepSeek R1, Gemini 2.0 Flash Thinking, Claude 3.7 Sonnet)\n- Avoid throughput-optimized models (they skip architectural analysis)\n\n**False Positives Possible** — Architectural rules have exceptions. The Critic may flag valid code that violates general principles for good reasons. Human review in Acceptance Gate remains essential.\n\n**Context Window Limits** — Large diffs may exceed context windows. Solutions:\n- Review changed files only (not entire codebase)\n- Split large PRs into smaller, focused changes\n- Use Summary Gates to compress Spec to relevant sections\n\n## Troubleshooting\n\n### Issue: Critic approves code that violates Constitution\n\n**Cause**: Constitutional constraints not specific enough in AGENTS.md\n\n**Solution**: \n1. Review violation that slipped through\n2. Add specific constraint to Constitution:\n   ```markdown\n   ### Data Access\n   - ❌ Before: \"Queries should be efficient\"\n   - ✅ After: \"Never use LoadAll().Filter() - push filtering to database\"\n   ```\n3. Re-run Constitutional Review with updated Constitution\n\n### Issue: Critic flags valid code as violation\n\n**Cause**: Constitutional rule is too strict or lacks exceptions\n\n**Solution**:\n1. Document exception in Constitution:\n   ```markdown\n   ### Data Access\n   - Push filtering to database layer\n   - Exception: In-memory filtering allowed for cached reference data (< 100 records)\n   ```\n2. Update Critic prompt to recognize exceptions\n3. Proceed to Acceptance Gate (human validates exception is legitimate)\n\n### Issue: Constitutional Review takes too long\n\n**Cause**: Large code diffs or complex Constitution\n\n**Solution**:\n1. **Break up PRs** — Smaller, focused changes\n2. **Parallelize reviews** — Review multiple files concurrently\n3. **Use Summary Gates** — Compress Spec to relevant sections only\n4. **Cache Constitution** — Reuse constitutional context across reviews\n\n## Future Automation Potential\n\nThis practice is currently manual but has clear automation paths:\n\n**CI/CD Integration** — Automated constitutional review on PR creation:\n```yaml\n# .github/workflows/constitutional-review.yml\non: pull_request\njobs:\n  constitutional-review:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run Constitutional Review\n        run: |\n          constitutional-review-agent \\\n            --constitution AGENTS.md \\\n            --spec plans/${FEATURE}/spec.md \\\n            --diff ${{ github.event.pull_request.diff_url }}\n```\n\n**IDE Integration** — Real-time constitutional feedback:\n- Inline warnings when typing code that violates Constitution\n- Suggestions appear as you code (like linting)\n\n**Living Constitution** — Automatic updates:\n- Track approved exceptions to constitutional rules\n- Suggest Constitution updates when patterns emerge\n\n**Violation Analytics** — Dashboard tracking:\n- Which constitutional principles violated most often\n- Identify gaps in agent training\n- Measure constitutional compliance over time\n\nSee also:\n- [Constitutional Review](/patterns/constitutional-review) — The pattern this practice implements\n- [Adversarial Code Review](/patterns/adversarial-code-review) — The base review pattern\n- [Agent Constitution](/patterns/agent-constitution) — Source of architectural truth\n- [The Spec](/patterns/the-spec) — Source of functional truth\n- [AGENTS.md Specification](/practices/agents-md-spec) — How to document the Constitution\n- [Feature Assembly](/practices/feature-assembly) — The full workflow where this practice fits\n\n### External Validation\n- [A Method for AI-Assisted Pull Request Reviews](https://lassala.net/2026/01/05/a-method-for-ai-assisted-pull-request-reviews-aligning-code-with-business-value/) (Carlos Lassala, January 2026) — Production implementation showing this practice in action",
    "tags": ["Code Review", "Implementation", "Critic Agent", "Workflow", "Quality Gates"],
    "references": []
  },
  {
    "slug": "living-specs",
    "collection": "practices",
    "title": "Living Specs",
    "description": "Practical guide to creating and maintaining specs that evolve alongside your codebase.",
    "status": "Experimental",
    "content": "## Overview\n\nThis guide provides practical instructions for implementing the [Specs](/patterns/the-spec) pattern. While the pattern describes *what* specs are and *why* they matter, this guide focuses on *how* to create and maintain them.\n\n## When to Create a Spec\n\nCreate a spec when starting work that involves:\n\n**Feature Domains** — New functionality that introduces architectural patterns, API contracts, or data models that other parts of the system depend on.\n\n**User-Facing Workflows** — Features with defined user journeys and acceptance criteria that need preservation for future reference.\n\n**Cross-Team Dependencies** — Any feature that other teams will integrate with, requiring clear contract definitions.\n\n**Don't create specs for:** Simple bug fixes, trivial UI changes, configuration updates, or dependency bumps.\n\n## Spec granularity\n\nA spec should be detailed enough to serve as a contract for the feature, but not so detailed that it becomes a maintenance burden.\n\nSome spec features, like gherkin scenarios, are not always necessary if the feature is simple or well-understood. \n\n## When to Update a Spec\n\nUpdate an existing spec when:\n\n- API contracts change (new endpoints, modified payloads, deprecated routes)\n- Data schemas evolve (migrations, new fields, constraint changes)\n- Quality targets shift (performance, security, accessibility requirements)\n- Anti-patterns are discovered (during review or post-mortems)\n- Architecture decisions are made (any ADR should update relevant specs)\n\n**Golden Rule:** If code behavior changes, the spec MUST be updated in the same commit.\n\n## File Structure\n\nOrganize specs by **feature domain**, not by sprint or ticket number.\n\n```\n/project-root\n├── ARCHITECTURE.md           # Global system rules\n├── plans/                    # Feature-level specs\n│   ├── user-authentication/\n│   │   └── spec.md\n│   ├── payment-processing/\n│   │   └── spec.md\n│   └── notifications/\n│       └── spec.md\n└── src/                      # Implementation code\n```\n\n**Conventions:**\n- Directory name: kebab-case, matches the feature's conceptual name\n- File name: always `spec.md`\n- Location: `/plans/{feature-domain}/spec.md`\n- Scope: one spec per independently evolvable feature\n\n## Maintenance Protocol\n\n### Same-Commit Rule\n\nIf code changes behavior, update the spec in the same commit. Add \"Spec updated\" to your PR checklist.\n\n```\ngit commit -m \"feat(notifications): add SMS fallback\n\n- Implements SMS delivery when WebSocket fails\n- Updates /plans/notifications/spec.md with new transport layer\"\n```\n\n### Deprecation Over Deletion\n\nMark outdated sections as deprecated rather than removing them. This preserves historical context.\n\n```markdown\n### Architecture\n\n**[DEPRECATED 2024-12-01]**\n~~WebSocket transport via Socket.io library~~\nReplaced by native WebSocket API to reduce bundle size.\n\n**Current:**\nNative WebSocket connection via `/api/ws/notifications`\n```\n\n### Bidirectional Linking\n\nLink code to specs and specs to code:\n\n```typescript\n// Notification delivery must meet 100ms latency requirement\n// See: /plans/notifications/spec.md#contract\n```\n\n```markdown\n### Data Schema\nImplemented in `src/types/Notification.ts` using Zod validation.\n```\n\n## Template\n\n```markdown\n# Feature: [Feature Name]\n\n## Blueprint\n\n### Context\n[Why does this feature exist? What business problem does it solve?]\n\n### Architecture\n- **API Contracts:** `[METHOD] /api/v1/[endpoint]` - [Description]\n- **Data Models:** See `[file path]`\n- **Dependencies:** [What this depends on / what depends on this]\n\n### Anti-Patterns\n- [What agents must avoid, with rationale]\n\n## Contract\n\n### Definition of Done\n- [ ] [Observable success criterion]\n\n### Regression Guardrails\n- [Critical invariant that must never break]\n\n### Scenarios\n**Scenario: [Name]**\n- Given: [Precondition]\n- When: [Action]\n- Then: [Expected outcome]\n```\n\n## Anti-Patterns\n\n### The Stale Spec\n**Problem:** Spec created during planning, never updated as the feature evolves.\n\n**Solution:** Make spec updates mandatory in Definition of Done. Add PR checklist item.\n\n### The Spec in Slack\n**Problem:** Design decisions discussed in chat but never committed to the repo.\n\n**Solution:** After consensus, immediately update `spec.md` with a commit linking to the discussion.\n\n### The Monolithic Spec\n**Problem:** A single 5000-line spec tries to document the entire application.\n\n**Solution:** Split into feature-domain specs. Use `ARCHITECTURE.md` only for global cross-cutting concerns.\n\n### The Spec-as-Tutorial\n**Problem:** Spec reads like a beginner's guide, full of basic programming concepts.\n\n**Solution:** Assume engineering competence. Document constraints and decisions, not general knowledge.\n\n### The Copy-Paste Code\n**Problem:** Spec duplicates large chunks of implementation code.\n\n**Solution:** Reference canonical sources with file paths. Only include minimal examples to illustrate patterns.\n\nSee also:\n- [Specs Pattern](/patterns/the-spec) — Conceptual foundation\n- [The PBI](/patterns/the-pbi) — Execution units that reference specs",
    "tags": ["Documentation", "Spec-Driven Development", "Living Documentation"],
    "references": [
      {
        "type": "website",
        "title": "Living Documentation",
        "url": "https://martinfowler.com/bliki/LivingDocumentation.html",
        "author": "Martin Fowler",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Martin Fowler's definition of Living Documentation, the foundation for keeping documentation synchronized with code."
      }
    ]
  },
  {
    "slug": "micro-commits",
    "collection": "practices",
    "title": "Micro-Commits",
    "description": "Ultra-granular commit practice for agentic workflows, treating version control as reversible save points.",
    "status": "Live",
    "content": "## Definition\n\n**Micro-Commits** is the practice of committing code changes at much higher frequency than traditional development workflows. Each discrete task—often a single function, test, or file—receives its own commit.\n\nWhen working with LLM-generated code, commits become \"save points in a game\": Checkpoints that enable instant rollback when probabilistic outputs introduce bugs or architectural drift.\n\n## When to Use\n\n**Use this practice when:**\n- Working with LLMs to generate code (preventing \"vibe convergence\")\n- Refactoring complex logic where regression risk is high\n- Conducting experimental \"spikes\" that might need total rollback\n- Trying to isolate specific AI changes for audit or debugging\n\n**Skip this practice when:**\n- Making trivial documentation fixes (typos)\n- The work is entirely manual and low-risk\n\n## The Problem: Coarse-Grained Commits in Agentic Workflows\n\nTraditional commit practices optimize for human readability and PR review: \"logical units of work\" that span multiple files and implement complete features.\n\nThis fails in agentic workflows because:\n\n**LLM outputs are probabilistic** — A model might generate correct code for 3 files and introduce subtle bugs in the 4th. Bundling all 4 files into one commit makes rollback destructive.\n\n**Regression to mediocrity** — Without checkpoints, it's difficult to identify where LLM output drifted from the [Spec](/patterns/the-spec) contracts.\n\n**Context loss** — Large commits obscure the sequence of decisions. When debugging, you need to know \"what changed, when, and why.\"\n\n**No emergency exit** — If an LLM generates a tangled mess across 10 files, your only option is manual surgery or discarding hours of work.\n\n## The Solution: Commit After Every Task\n\nMake a commit immediately after:\n- Completing a [PBI](/patterns/the-pbi) subtask\n- Generating a single function or module\n- Making a file pass linting/compilation\n- Adding one test\n- Any LLM-assisted edit that produces working code\n\nThis creates a breadcrumb trail of working states.\n\n## The Practice\n\n### 4.1. Atomic Tasks → Atomic Commits\n\nBreak work into small, testable chunks. Each chunk maps to one commit.\n\n**Example PBI:** \"Add OAuth login flow\"\n\n**Commit sequence:**\n```\n1. feat: add OAuth config schema\n2. feat: implement token exchange endpoint\n3. feat: add session storage for OAuth tokens\n4. test: add OAuth flow integration test\n5. refactor: extract OAuth error handling\n```\n\nThis aligns with atomic [PBIs](/patterns/the-pbi): small, bounded execution units.\n\n### 4.2. Commit Messages as Execution Log\n\nCommit messages document the sequence of LLM-assisted changes. They serve as:\n- **Context for debugging** — \"The bug appeared after commit 7.\"\n- **Briefing material for AI** — Feed recent commits to an LLM to explain current state.\n- **Audit trail** — Track architectural decisions embedded in code changes.\n\n**Format:**\n```\ntype(scope): brief description\n\n- Detail 1\n- Detail 2\n```\n\n**Example:**\n```\nfeat(auth): implement OAuth token validation\n\n- Add JWT verification middleware\n- Extract claims from token payload\n- Return 401 on expired tokens\n```\n\n### 4.3. Branches and Worktrees for Isolation\n\nUse branches or git worktrees to isolate LLM experiments:\n\n**Branches** — Separate experimental work from stable code. Merge only after validation.\n\n**Worktrees** — Run parallel LLM sessions on the same repository without context conflicts. Each worktree is an independent working directory.\n\n**Example workflow:**\n```bash\n# Create worktree for LLM experiment\ngit worktree add ../project-experiment experiment-oauth\n\n# Work in worktree, commit frequently\ncd ../project-experiment\n# ... LLM generates code ...\ngit commit -m \"feat: add OAuth callback handler\"\n\n# If successful, merge into main\ngit checkout main\ngit merge experiment-oauth\n\n# If failed, discard worktree\ngit worktree remove ../project-experiment\n```\n\nThis prevents contaminating the main branch with failed LLM output.\n\n### 4.4. Rollback as First-Class Operation\n\nWhen LLM output introduces bugs:\n\n**Identify the bad commit** — Review recent history to find where issues appeared.\n\n**Rollback to last known good state:**\n```bash\n# Soft reset (keeps changes as uncommitted)\ngit reset --soft HEAD~1\n\n# Hard reset (discards changes entirely)\ngit reset --hard HEAD~1\n```\n\n**Selective revert:**\n```bash\n# Revert specific commit without losing subsequent work\ngit revert <commit-hash>\n```\n\nThis is only safe because micro-commits isolate changes.\n\n### 5. Tidy History for Comprehension\n\nGranular commits create noisy history. Before merging to main, optionally squash related commits into logical units:\n\n```bash\n# Interactive rebase to squash last 5 commits\ngit rebase -i HEAD~5\n```\n\nThis preserves detailed history during development while creating clean history for long-term maintenance.\n\n**Trade-off:** Squashing removes granular rollback points. Only squash after validation passes [Quality Gates](/patterns/context-gates).\n\n## Relationship to The PBI\n\n[PBIs](/patterns/the-pbi) define **what to build**. Micro-Commits define **how to track progress**.\n\n**Atomic PBIs** (small, bounded tasks) naturally produce micro-commits. Each PBI generates 1-5 commits depending on complexity.\n\n**Example mapping:**\n- **PBI:** \"Implement retry logic with exponential backoff\"\n- **Commits:**\n  1. `feat: add retry wrapper function`\n  2. `feat: implement exponential backoff calculation`\n  3. `test: add retry logic unit tests`\n  4. `docs: update retry behavior in spec`\n\nThis makes PBI progress traceable and reversible.\n\nSee also:\n- [The PBI](/patterns/the-pbi) — Atomic execution units that map to commit sequences\n- [Context Gates](/patterns/context-gates) — Validation checkpoints that rely on granular commits\n- [Agentic SDLC](/concepts/agentic-sdlc) — The cybernetic loop where micro-commits enable rapid iteration",
    "tags": ["Version Control", "Git", "Safety", "Rollback"],
    "references": [
      {
        "type": "website",
        "title": "My LLM Coding Workflow Going into 2026",
        "url": "https://addyo.substack.com/p/my-llm-coding-workflow-going-into",
        "author": "Addy Osmani",
        "published": "2026-01-01T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Addy Osmani's practical guide emphasizing commits as 'save points in a game', validating the micro-commit approach for LLM workflows."
      }
    ]
  },
  {
    "slug": "pbi-authoring",
    "collection": "practices",
    "title": "PBI Authoring",
    "description": "How to write Product Backlog Items that agents can read, execute, and verify—with templates and lifecycle guidance.",
    "status": "Live",
    "content": "## Definition\n\n**PBI Authoring** is the practice of writing Product Backlog Items optimized for agent execution. This includes structuring the four-part anatomy, ensuring machine accessibility, and managing the PBI lifecycle from planning through closure.\n\nFollowing this practice produces PBIs that agents can programmatically access, unambiguously interpret, and verifiably complete.\n\n## When to Use\n\n**Use this practice when:**\n- Creating work items for agent execution\n- Planning a sprint with AI-assisted development\n- Converting legacy user stories to agent-ready format\n- Setting up a new project's backlog structure\n\n**Skip this practice when:**\n- Work is purely exploratory with no defined outcome\n- The task is a one-off command (use direct prompting instead)\n- Human-only execution with no agent involvement\n\n## Process\n\n### Step 1: Ensure Accessibility\n\n**Invisibility is a bug.** If an agent cannot read the PBI, the workflow is broken.\n\nA PBI locked inside a web UI without API or MCP integration is useless to an AI developer. The agent must programmatically access the work item without requiring human copy-paste.\n\n**Valid access methods:**\n\n| Method | Description |\n|--------|-------------|\n| **MCP Integration** | Agent connected to Issue Tracker (Linear, Jira, GitHub) via MCP |\n| **Repo-Based** | PBI exists as a markdown file (e.g., `tasks/PBI-123.md`) |\n| **API Access** | Tracker exposes REST/GraphQL API the agent can query |\n\n**If your tracker has no API access:** Mirror PBIs as markdown files during sprint planning, or implement MCP integration.\n\n### Step 2: Write the Directive\n\nState what to do with explicit scope boundaries. Be imperative, not conversational.\n\n**Good:**\n```\nImplement the API Layer for user notification preferences.\nScope: Only touch the `src/api/notifications` folder.\n```\n\n**Bad:**\n```\nAs a user, I want to manage my notification preferences so that I can control what emails I receive.\n```\n\nThe second example requires interpretation. The first is executable.\n\n> [!TIP]\n> **Prompt for the Plan.** Even if your tool handles planning automatically, explicitly instruct the agent to output its plan for review. This forces the Specify → Plan → Execute loop.\n>\n> **Example Directive:** \"Analyze the Spec, propose a step-by-step plan including which files you will touch, and wait for my approval before editing files.\"\n\n### Step 3: Add Context Pointers\n\nReference the permanent spec—don't copy design decisions into the PBI.\n\n```\nReference: `plans/notifications/spec.md` Part A for the schema.\nSee the \"Architecture\" section for endpoint contracts.\n```\n\n**Why pointers, not copies:** Specs evolve. A copied schema in a PBI becomes stale the moment the spec updates. Pointers ensure the agent always reads the authoritative source.\n\n### Step 4: Define Verification Criteria\n\nLink to success criteria in the spec, or define inline checkboxes.\n\n```\nMust pass \"Scenario 3: Preference Update\" defined in \n`plans/notifications/spec.md` Part B (Contract).\n```\n\nOr inline:\n```\n- [ ] POST /preferences returns 201 on valid input\n- [ ] Invalid payload returns 400 with error schema\n- [ ] Unit test coverage > 80%\n```\n\n### Step 5: Declare Dependencies\n\nExplicitly state what blocks this PBI and what it blocks.\n\n```\n## Dependencies\n- Blocked by: PBI-101 (creates the base schema)\n- Must merge before: PBI-103 (extends this endpoint)\n```\n\n**Anti-Pattern:** Implicit dependencies discovered at merge time. Identify during planning; either sequence the work or refactor into independent units.\n\n### Step 6: Set the Refinement Rule\n\nDefine what happens when reality diverges from the spec.\n\n```\nIf implementation requires changing the Architecture, you MUST \nupdate `spec.md` in the same PR with a changelog entry.\n```\n\nOptions to specify:\n- **Update spec in same PR** — Agent has authority to evolve the design\n- **Flag for human review** — Agent stops and requests guidance\n- **Proceed with deviation log** — Agent continues but documents the gap\n\n## Template\n\n```markdown\n# PBI-XXX: [Brief Imperative Title]\n\n## Directive\n[What to build/change in 1-2 sentences]\n\n**Scope:**\n- [Explicit file/folder boundaries]\n- [What NOT to touch]\n\n## Dependencies\n- Blocked by: [PBI-YYY if any, or \"None\"]\n- Must merge before: [PBI-ZZZ if any, or \"None\"]\n\n## Context\nRead: `[path/to/spec.md]`\n- [Specific section to reference]\n\n## Verification\n- [ ] [Criterion 1: Functional requirement]\n- [ ] [Criterion 2: Performance/quality requirement]\n- [ ] [Criterion 3: Test coverage requirement]\n\n## Refinement Protocol\n[What to do if the spec needs updating during implementation]\n```\n\n## PBI Lifecycle\n\n| Phase | Actor | Action |\n|-------|-------|--------|\n| **Planning** | Human | Creates PBI with 4-part structure |\n| **Assignment** | Human/System | PBI assigned to Agent or Developer |\n| **Execution** | Agent | Reads Spec, implements Delta |\n| **Review** | Human | Verifies against Spec's Contract section |\n| **Merge** | Human/System | Code merged, Spec updated if needed |\n| **Closure** | System | PBI archived, linked to commit/PR |\n\n## Common Mistakes\n\n### The User Story Hangover\n\n**Problem:** PBI written as \"As a user, I want...\" with no explicit scope or verification.\n\n**Solution:** Rewrite in imperative form with scope boundaries and checkable criteria.\n\n### The Spec Copy\n\n**Problem:** PBI contains copied design decisions that drift from the spec.\n\n**Solution:** Use pointers to spec sections, never copy content that lives elsewhere.\n\n### The Hidden Dependency\n\n**Problem:** Two PBIs touch the same files; discovered at merge time.\n\n**Solution:** During planning, map file ownership. If overlap exists, sequence the PBIs or refactor scope.\n\n### The Untestable Increment\n\n**Problem:** PBI verification requires another PBI to complete first.\n\n**Solution:** Ensure each PBI is self-testable. If not possible, merge into a single PBI or create test fixtures.\n\n## Related Patterns\n\nThis practice implements:\n\n- **[The PBI](/patterns/the-pbi)** — The structural pattern this practice executes\n\nSee also:\n\n- **[The Spec](/patterns/the-spec)** — The permanent context PBIs reference\n- **[Living Specs](/practices/living-specs)** — How to maintain the specs PBIs point to",
    "tags": ["Agile", "Product Backlog Item", "Workflow", "Agent Execution"],
    "references": [
      {
        "type": "video",
        "title": "Beyond Vibe-Coding: Learn Effective AI-Assisted Coding in 4 minutes",
        "url": "https://www.youtube.com/watch?v=HR5f2TDC65E",
        "publisher": "Vanishing Gradients",
        "annotation": "Source material for the 'Prompt for the Plan' tip. Explains the Specify → Plan → Execute workflow."
      }
    ]
  },
  {
    "slug": "workflow-as-code",
    "collection": "practices",
    "title": "Workflow as Code",
    "description": "Define agentic workflows in deterministic code rather than prompts to ensure reliability, type safety, and testable orchestration.",
    "status": "Experimental",
    "content": "## Definition\n\n**Workflow as Code** is the practice of defining agentic workflows using deterministic programming languages (like TypeScript or Python) rather than natural language prompts.\n\nIt treats the **Agent** as a function call within a larger, strongly-typed system, rather than treating the **System** as a tool available to a chatty agent.\n\n## When to Use\n\n**Use this practice when:**\n- Building repetitive production processes (CI/CD, release workflows)\n- Implementing complex branching logic with multiple decision points\n- Operating high-reliability pipelines where failure consequences are significant\n- Orchestrating multi-step agent tasks that require verification checkpoints\n\n**Skip this practice when:**\n- Exploratory tasks with undefined outcomes\n- Simple, linear command sequences\n- Ad-hoc queries or one-off investigations\n- Low-stakes prototyping where speed matters more than reliability\n\n## Why It Matters\n\nWhen complex workflows are driven entirely by an LLM loop (\"Here is a goal, figure it out\"), the system suffers from **Context Pollution**. As the agent accumulates history—observations, tool outputs, internal monologue—its attention degrades.\n\nNick Tune describes this as the agent becoming \"tipsy wobbling from side-to-side\": it loses focus on strict process adherence because its context window is overflowing with implementation details.\n\n## Process\n\n### Step 1: Identify Deterministic vs Probabilistic Tasks\n\nAudit your workflow. Separate mechanical tasks (running builds, conditional logic, file operations) from intelligence tasks (code review, summarization, decision-making under ambiguity).\n\n**Deterministic (Code):**\n- Run build/test commands\n- Parse structured output\n- Branch on conditions\n- Read/write files\n- Make API calls\n\n**Probabilistic (Agent):**\n- Review code against spec\n- Summarize findings\n- Generate implementation\n- Assess quality\n\n### Step 2: Define Typed Step Abstraction\n\nCreate a common interface for workflow steps:\n\n```typescript\nexport type WorkflowContext = {\n  workDir: string;\n  spec: string;\n  history: StepResult[];\n};\n\nexport type StepResult =\n  | { type: 'success'; data: unknown }\n  | { type: 'failure'; reason: string; recoverable: boolean };\n\nexport type Step = (ctx: WorkflowContext) => Promise<StepResult>;\n```\n\nThis enables:\n- **Composition**: Reassemble steps into new workflows\n- **Type Safety**: Validate data passing between steps\n- **Testability**: Unit test orchestration without invoking an LLM\n\n### Step 3: Implement the Orchestration Shell\n\nWrite the control flow in code. The LLM only appears where intelligence is required:\n\n```typescript\nasync function runDevWorkflow(ctx: WorkflowContext) {\n  // Deterministic: Run build\n  const buildResult = await runBuild(ctx);\n  if (buildResult.type === 'failure') {\n    return handleBuildError(buildResult);\n  }\n\n  // Probabilistic: Agent reviews the diff\n  const reviewResult = await runAgentReview({\n    diff: await git.getDiff(),\n    spec: ctx.spec\n  });\n\n  // Deterministic: Act on structured result\n  if (reviewResult.verdict === 'PASS') {\n    await git.commit();\n    await github.createPR();\n  }\n}\n```\n\n### Step 4: Implement Opaque Commands\n\nFrom the agent's perspective, workflow steps should be \"Black Boxes.\" The agent invokes a high-level command and acts on the structured result—it doesn't need to know implementation details.\n\n**Define the interface:**\n```typescript\ntype VerifyWorkResult = {\n  status: 'passed' | 'failed';\n  errors?: { file: string; line: number; message: string }[];\n};\n\nasync function verifyWork(ctx: WorkflowContext): Promise<VerifyWorkResult> {\n  // Implementation hidden from agent\n  const lint = await runLint(ctx.workDir);\n  const types = await runTypeCheck(ctx.workDir);\n  const tests = await runTests(ctx.workDir);\n  \n  return aggregateResults([lint, types, tests]);\n}\n```\n\nThis reduces token usage and prevents the agent from hallucinating incorrect shell commands.\n\n### Step 5: Add Enforcement Hooks\n\nAgents will sometimes try to bypass the workflow. Implement hard boundaries using client-side hooks:\n\n```bash\n# .claude/hooks/pre-tool-use.sh\nif [[ \"$TOOL\" == \"Bash\" && \"$COMMAND\" =~ \"git push\" ]]; then\n  echo \"Blocked: Use 'submit-pr' tool which runs verification first.\"\n  exit 1\nfi\n```\n\nThis shifts enforcement from \"Instructions in the System Prompt\" (which can be ignored) to \"Code in the Environment\" (which cannot).\n\n## Template\n\nMinimal workflow orchestrator structure:\n\n```typescript\n// workflows/dev-workflow.ts\nimport type { Step, WorkflowContext, StepResult } from './types';\n\nconst steps: Step[] = [\n  runBuild,\n  runLint,\n  runAgentReview,  // Only probabilistic step\n  commitChanges,\n  createPR,\n];\n\nexport async function execute(ctx: WorkflowContext): Promise<StepResult> {\n  for (const step of steps) {\n    const result = await step(ctx);\n    if (result.type === 'failure' && !result.recoverable) {\n      return result;\n    }\n    ctx.history.push(result);\n  }\n  return { type: 'success', data: ctx.history };\n}\n```\n\n## Common Mistakes\n\n### The God Prompt\n\n**Problem:** Entire workflow described in a single system prompt, expecting the agent to \"figure it out.\"\n\n**Solution:** Extract deterministic logic into code. The agent should only handle tasks requiring intelligence.\n\n### Leaky Abstractions\n\n**Problem:** Agent sees raw command output (500 lines of test failures) instead of structured results.\n\n**Solution:** Parse outputs into typed results before passing to the agent. Summarize, don't dump.\n\n### Missing Enforcement\n\n**Problem:** Workflow relies on the agent \"following instructions\" without hard boundaries.\n\n**Solution:** Implement hooks that block unauthorized actions. Trust code, not compliance.\n\n### Over-Agentification\n\n**Problem:** Using an LLM to run `npm install` or parse JSON—tasks with zero ambiguity.\n\n**Solution:** Reserve agent calls for genuinely probabilistic tasks. Everything else is code.\n\n## Related Patterns\n\n- **[Ralph Loop](/patterns/ralph-loop)** — Implements the \"Loop\" part of the workflow using code-based persistence\n- **[Context Gates](/patterns/context-gates)** — Architectural checkpoints that Workflow as Code enforces programmatically\n- **[Model Routing](/patterns/model-routing)** — Assigning different models to different steps within the code-based workflow",
    "tags": ["Orchestration", "Determinism", "TypeScript", "Automation"],
    "references": [
      {
        "type": "website",
        "title": "Dev Workflows as Code",
        "url": "https://medium.com/nick-tune-tech-strategy-blog/dev-workflows-as-code-fab70d44b6ab",
        "author": "Nick Tune",
        "published": "2026-01-16T00:00:00.000Z",
        "accessed": "2026-01-18T00:00:00.000Z",
        "annotation": "Foundational article describing the shift from prompt-based to code-based orchestration."
      }
    ]
  }
]
