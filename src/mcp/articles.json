[
  {
    "slug": "event-modeling",
    "collection": "concepts",
    "title": "Event Modeling",
    "description": "A system blueprinting method that centers on events as the primary source of truth, serving as a rigorous bridge between visual design and technical implementation.",
    "status": "Experimental",
    "content": "## Definition\n\n**Event Modeling** is a method for designing information systems by mapping what happens over time. It creates a linear blueprint that serves as the single source of truth for Product, Design, and Engineering.\n\nUnlike static diagrams (like ERDs or UML) that focus on structure, Event Modeling focuses on the **narrative of the system**. It visualizes the system as a film strip, showing exactly how a user’s action impacts the system state and what information is displayed back to them.\n\n### Core Components\nAn Event Model is composed of four distinct elements:\n\n* **Commands (Blue)**: The intent or action initiated by the user (e.g., \"Submit Order\").\n* **Events (Orange)**: A fact recorded by the system that cannot be changed (e.g., \"OrderPlaced\"). This is the single source of truth.\n* **Views (Green)**: Information displayed to the user, derived from previous events (e.g., \"Order Confirmation Screen\").\n* **Processes**: The logic or automation that reacts to events to trigger other commands or update views.\n\n## Why It Matters for AI\n\nIn modern software development, ambiguity is the enemy. While human engineers can infer intent from a loose visual mockup, AI models require explicit instructions.\n\nEvent Modeling forces implicit business rules to become explicit. By defining the exact data payload of every *Command* and the resulting state change of every *Event*, we provide AI agents with a deterministic roadmap. This ensures the generated code handles edge cases and data consistency correctly, rather than just \"looking right\" on the frontend.\n\n## Relationship to Requirements\n\nEvent Modeling acts as a bridge between **Visual Design** (what it looks like) and **Technical Architecture** (how it works).\n\nIt does not replace functional requirements; rather, it validates them. A feature is only considered \"defined\" when there is a complete path mapped from the user's view, through the command, to the stored event, and back to the view. This \"closed loop\" guarantees that every pixel on the screen is backed by real data.\n\n## References\n\n* [EventModeling.org](https://eventmodeling.org/) - The official home of the methodology.",
    "tags": []
  },
  {
    "slug": "levels-of-autonomy",
    "collection": "concepts",
    "title": "Levels of Autonomy",
    "description": "SAE-inspired taxonomy for AI agent autonomy in software development, from L1 (assistive) to L5 (full), standardized at L3 conditional autonomy.",
    "status": "Live",
    "content": "## Definition\n\nThe **Levels of Autonomy** scale categorizes AI systems based on their operational independence.\nUnlike general-purpose taxonomies, the Autonomous System Development Life Cycle (ASDLC) utilizes\nthis scale to establish the Safe Operating Area for software agents. \n\nWe define these levels to explicitly identify where the **Context Gate** (the boundary of human\noversight) must be placed. In ASDLC, autonomy is not a measure of intelligence; it is a measure\nof optimal risk.\n\n## The ASDLC Standard: L3\n\nASDLC standardizes practices for **Level 3 (Conditional Autonomy)** in software engineering. While \nthe industry frequently promotes **Level 5 (Full Autonomy)** as the ultimate goal, we believe this \nperspective may be counterproductive at the moment. Therefore, we intentionally establish Level 3 a\ns the sensible default.\n\n> ## Level 4 Autonomy Risks\n> \n> At Level 4 (L4), agents are advanced enough to operate for days without human intervention; \n> however, they lack the strategic foresight needed to maintain system integrity. This results in \n> a phenomenon known as Silent Drift—where the codebase continues to function technically but \n> gradually deteriorates into an unmanageable state.\n> \n> It's important to note that there are ways to mitigate this risk, such as implementing Advanced\n> Context Gate strategies and utilizing emerging tools to monitor architectural health for drift.\n> However, these solutions will need to be tested and validated over time.\n\n## The Scale\n\n| Level | Designation | Description | Human Role | Failure Mode |\n| :--- | :--- | :--- | :--- | :--- |\n| **L1** | Assistive | Autocomplete, Chatbots. Zero state retention. | Driver. Hands on wheel 100% of time. | Distraction / Minor Syntax Errors |\n| **L2** | Task-Based | \"Fix this function.\" Single-file context. | Reviewer. Checks output before commit. | Logic bugs within a single file. |\n| **L3** | Conditional | **ASDLC Standard.** \"Implement this feature.\" Multi-file orchestration. | Instructor. Defines constraints & intervenes on \"drift.\" | Regression to the Mean (Mediocrity). |\n| **L4** | High | \"Manage this backlog.\" Self-directed planning. | Auditor. Post-hoc analysis. | Silent Failure. Strategic drift over time. |\n| **L5** | Full | \"Run this company.\" | Consumer. Passive beneficiary. | Existential alignment drift. |\n\n## Analogy: The Self-Driving Standard (SAE)\n\nWe map software autonomy directly to the SAE J3016 automotive standard to clarify the \"Human-in-the-Loop\" requirements.\n\n| ASDLC Level | SAE Equivalent | The \"Steering Wheel\" Metaphor |\n| :--- | :--- | :--- |\n| **L1** | L1 (Driver Assist) | **Hands On, Feet On.** AI nudges the wheel (Lane Keep) or gas (Cruise), but Human drives. |\n| **L2** | L2 (Partial) | **Hands On (mostly).** AI handles steering and speed in bursts, but Human monitors constantly. |\n| **L3** | L3 (Conditional) | **Hands Off, Eyes On.** AI executes the maneuver (The Drive). Human is the Instructor ready to grab the wheel immediately. |\n| **L4** | L4 (High) | **Mind Off.** Sleeping in the back seat within a geo-fenced area. Dangerous if the \"fence\" (Context) breaks. |\n| **L5** | L5 (Full) | **No Steering Wheel.** The vehicle has no manual controls. |",
    "tags": []
  },
  {
    "slug": "spec-driven-development",
    "collection": "concepts",
    "title": "Spec-Driven Development (SDD)",
    "description": "Methodology where machine-readable specs define contracts before code, inverting traditional workflows to prevent agent hallucination and drift.",
    "status": "Experimental",
    "content": "## Definition\n\nSpec-Driven Development (SDD) means defining the \"What\" and the \"Why\" first. This is done in precise, machine-readable formats. Only then does an AI agent address the \"How.\"\n\nSDD inverts control. Instead of code serving as the source of documentation, the Spec becomes the authority. Code must then fulfill this Spec.\n\nIn the era of Agentic tools, SDD addresses the issue of \"vibe coding,\" in which LLMs generate code from vague prompts. By grounding workflows in schemas, state machines, and contracts, SDD converts probabilistic outputs into reliable engineering artifacts.\n\n> [!WARNING]\n> **The Figma Trap**\n> A beautiful mockup is not a specification; it is a suggestion. Mockups typically demonstrate the \"happy path\" but hide the edge cases, error states, and data consistency strictures where production bugs live.\n>\n> **Never** treat a visual design as a complete technical requirement. AI agents given only mockups will hallucinate implementations for the missing logical layers.\n\n## SDD in the ASDLC\n\nIn the Agentic Software Development Life Cycle (ASDLC), we suggest using SDD as a comprehensive lifecycle-constraint system rather than merely for code generation. Specifications generate code, model the domain, and enforce architectural governance.\n\nOur methodology separates context to prevent misalignment:\n\n1. The Agent Constitution (Global Context): A permanent set of rules, such as \"Use safe databases\" or \"Store all info in one place.\" This makes sure everything created by the system meets company guidelines.\n2. The Docs (Reference Context): Long-term, stable documentation like Product Requirements Documents (PRDs) or earlier Specs that explain the overall goals and constraints.\n3. The Spec (Local Context): A short-term, targeted set of instructions for a task, used to validate immediate results.\n\n## Accessibility: Specs as Source Code\n\nIn ASDLC, Product Requirements Documents (PRDs), tasks, and Specs are treated like Source Code. The AI must be able to read Specs directly, not just see them in pictures or separate documents.\n\nTo make this happen, we either use the Model Context Protocol (MCP) or put the specs right next to the code:\n\n* Repository Colocation: Storing specs as Markdown or YAML files directly in the git repository (e.g., .specs/feature-login.md). This ensures that the version history matches the code history.\n* MCP Integration: Use MCP to connect to external tools (e.g., Linear or Jira) so the AI can read tasks as clear instructions.\n\n## Context Gates\n\nASDLC formalizes the classic SDD step-by-step process with strict context gates. Spec updates and handoffs should occur strictly through HITL (human-in-the-loop) gates.\n\n```mermaid\nflowchart LR\n %% caption: Context Gating of Spec-Driven Development\n A(SYSTEM CONTEXT)\n A.1(Documentation)\n A.2(User Guidance)\n B[Spec]\n C{GATE}\n D[[Agentic Development]]\n E{GATE}\n F[[...]]\n\n A --> B\n A.1 --> B\n A.2 --> B\n\n B --> C\n\n C --> D\n\n D --> E\n\n\n E --> |REQUEST FOR REFINEMENT| B\n E --> F\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/spec-driven-development-fig-1.svg\" alt=\"Context Gating of Spec-Driven Development\" />\n  <figcaption>Context Gating of Spec-Driven Development</figcaption>\n</figure>",
    "tags": []
  },
  {
    "slug": "agent-constitution",
    "collection": "patterns",
    "title": "Agent Constitution",
    "description": "Prime directives that align agent behavior with system goals before action, acting as driver training vs. context gates as brakes.",
    "status": "Experimental",
    "content": "## The Prime Directives\n\nAn Agent Constitution is a set of \"Prime Directives\" injected into an agent's context window to align its intent and behavior with high-level principles. Unlike specific task instructions, the constitution provides the ethical and operational boundaries within which the agent must operate. It serves as the foundational \"superego\" for the agent, ensuring that even in ambiguous situations, its actions remain consistent with the system's overarching goals and safety guidelines.\n\n## Driver Training vs. Brakes\n\nTo understand the role of an Agent Constitution, consider the analogy of a car. Context Gates and other restrictive mechanisms act as the \"brakes\"—they stop the agent from doing something wrong after it has already attempted or formulated an action. The Agent Constitution, however, is the \"driver training.\" It shapes the agent's decision-making process *before* it even considers an action. By internalizing these rules, the agent is less likely to need the \"brakes\" because it is steering itself correctly from the start.\n\n## Implementation\n\nFor the standard implementation of this pattern, see the [AGENTS.md Specification](/practices/agents-md-spec). The specification details how to formally document and inject these directives into your agentic workflows.",
    "tags": []
  },
  {
    "slug": "model-routing",
    "collection": "patterns",
    "title": "Model Routing",
    "description": "Strategic assignment of LLM models to SDLC phases based on reasoning capability versus execution speed.",
    "status": "Live",
    "content": "## Definition\n\n**Model Routing** is the strategic assignment of different Large Language Models (LLMs) to different phases of the software development lifecycle based on their capability profile.\n\nDifferent computational tasks have different performance characteristics. Model Routing matches model capabilities to task requirements: **reasoning depth** during design phases and **speed with large context windows** during implementation phases.\n\nThis is a tool selection strategy, not a delegation strategy. Engineers remain accountable for output quality while selecting the appropriate computational tool for each phase.\n\n## The Problem: Single-Model Inefficiency\n\nUsing one model for all phases creates a mismatch between computational capability and task requirements.\n\nHigh-speed models struggle with architectural decisions requiring deep constraint satisfaction. Reasoning models are too slow for high-volume implementation tasks. Models with massive context windows are expensive when you only need to process small, focused changes.\n\nEach model class optimizes for different performance characteristics. Using the wrong one wastes either quality (insufficient reasoning) or resources (excessive capability for simple tasks).\n\n## The Solution: Capability-Based Assignment\n\nWe categorize models into three capability profiles aligned with [Agentic SDLC](/concepts/agentic-sdlc) phases:\n\n| Capability Profile | Optimization | Primary Use Cases | Model Examples |\n|---|---|---|---|\n| **High Reasoning** | Deep logic, high latency, \"System 2\" thinking | Writing [Specs](/patterns/the-spec), architectural decisions, logic debugging, security analysis | Gemini 3 Deep Think, DeepSeek V3.2, OpenAI o3-pro |\n| **High Throughput** | Speed, low latency, real-time execution | Code generation, refactoring, unit tests, UI implementation | Gemini 3 Flash, Llama 4 Scout, Claude Haiku 4.5 |\n| **Massive Context** | Repository-scale context (500k-5M tokens) | Documentation analysis, codebase navigation, legacy system understanding | Gemini 3 Pro (5M tokens), Claude 4.5 Sonnet (500k), GPT-5 (RAG-native) |\n\n*Model examples current as of December 27, 2025. The LLM landscape evolves rapidly—validate capabilities and availability before implementation.*\n\n## Relationship to Levels of Autonomy\n\n[Levels of Autonomy](/concepts/levels-of-autonomy) define human oversight requirements. Model Routing complements this by matching computational capability to task characteristics:\n\n- **Complex architectural decisions** (L3 with high uncertainty) → High Reasoning models\n- **Well-specified implementation tasks** (L3 with clear contracts) → High Throughput models\n- **Exploratory analysis** (L2 with discovery focus) → Massive Context models\n\nThis ensures that the computational tool's capability profile matches the task's computational requirements and the degree of human verification needed.\n\n## References\n\n### Framework Patterns\n- [Agent Personas](/practices/agent-personas) — Context engineering practice for scoping agent work, extended by model routing\n- [The Spec](/patterns/the-spec) — The artifact produced by High Reasoning models in the planning phase\n- [Context Engineering](/concepts/context-engineering) — The practice of structuring context for optimal LLM performance\n\n### External Resources\n- [My LLM Coding Workflow Going into 2026](https://addyo.substack.com/p/my-llm-coding-workflow-going-into) — Addy Osmani's workflow guide emphasizing pragmatic model selection and mid-task model switching patterns",
    "tags": ["LLM Selection", "Context Engineering", "ASDLC", "Agent Architecture"]
  },
  {
    "slug": "the-spec",
    "collection": "patterns",
    "title": "Specs",
    "description": "Living documents that serve as the permanent source of truth for features, solving the context amnesia problem in agentic development.",
    "status": "Live",
    "content": "## Definition\n\nA **Spec** is the permanent source of truth for a feature. It defines *how* the system works (Design) and *how* we know it works (Quality).\n\nUnlike traditional tech specs or PRDs that are \"fire and forget,\" specs are **living documents**. They reside in the repository alongside the code and evolve with every change to the feature.\n\n## The Problem: Context Amnesia\n\nAgents do not have long-term memory. They cannot recall Jira tickets from six months ago or Slack conversations about architectural decisions. When an agent is tasked with modifying a feature, it needs immediate access to:\n\n- The architectural decisions that shaped the feature\n- The constraints that must not be violated\n- The quality criteria that define success\n\nWithout specs, agents reverse-engineer intent from code comments and commit messages—a process prone to hallucination and architectural drift.\n\nTraditional documentation fails because:\n- **Wikis decay** — separate systems fall out of sync with code\n- **Tickets disappear** — issue trackers capture deltas (changes), not state (current rules)\n- **Comments lie** — code comments describe implementation, not architectural intent\n- **Memory fails** — tribal knowledge evaporates when team members leave\n\nSpecs solve this by making documentation a **first-class citizen** in the codebase, subject to the same version control and review processes as the code itself.\n\n## State vs Delta\n\nThis is the core distinction that makes agentic development work at scale.\n\n| Dimension | The Spec | The PBI |\n|-----------|----------|---------|\n| **Purpose** | Define the State (how it works) | Define the Delta (what changes) |\n| **Lifespan** | Permanent (lives with the code) | Transient (closed after merge) |\n| **Scope** | Feature-level rules | Task-level instructions |\n| **Audience** | Architects, Agents (Reference) | Agents, Developers (Execution) |\n\nThe Spec defines the **current state** of the system:\n- \"All notifications must deliver within 100ms\"\n- \"API must handle 1000 req/sec\"\n\nThe PBI defines the **change**:\n- \"Add SMS fallback to notification system\"\n- \"Optimize database query for search endpoint\"\n\nThe PBI *references* the Spec for context and *updates* the Spec when it changes contracts.\n\n### Why Separation Matters\n\n```\nSprint 1: PBI-101 \"Build notification system\"\n  → Creates /plans/notifications/spec.md\n  → Spec defines: \"Deliver within 100ms via WebSocket\"\n\nSprint 3: PBI-203 \"Add SMS fallback\"\n  → Updates spec.md with new transport rules\n  → PBI-203 is closed, but the spec persists\n\nSprint 8: PBI-420 \"Refactor notification queue\"\n  → Agent reads spec.md, sees all rules still apply\n  → Refactoring preserves all documented contracts\n```\n\nWithout this separation, the agent in Sprint 8 has no visibility into decisions made in Sprint 1.\n\n## The Assembly Model\n\nSpecs serve as the context source for Feature Assembly. Multiple PBIs reference the same spec, and the spec's contracts are verified at quality gates.\n\n```mermaid\nflowchart LR\n  A[/spec.md/]\n\n  B[\\pbi-101.md\\]\n  C[\\pbi-203.md\\]\n  D[\\pbi-420.md\\]\n\n  B1[[FEATURE ASSEMBLY]]\n  C1[[FEATURE ASSEMBLY]]\n  D1[[FEATURE ASSEMBLY]]\n\n  E{GATE}\n\n  F[[MIGRATION]]\n\n  A --> B\n  A --> C\n  A --> D\n\n  B --> B1\n  C --> C1\n  D --> D1\n\n  B1 --> E\n  C1 --> E\n  D1 --> E\n\n  A --> |Context|E\n\n  E --> F\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/the-spec-fig-1.svg\" alt=\"Mermaid Diagram\" />\n  \n</figure>\n\n## Anatomy\n\nEvery spec consists of two parts:\n\n### Blueprint (Design)\nDefines **implementation constraints** that prevent agents from hallucinating invalid architectures.\n\n- **Context** — Why does this feature exist?\n- **Architecture** — API contracts, schemas, dependency directions\n- **Anti-Patterns** — What agents must NOT do\n\n### Contract (Quality)\nDefines **verification rules** that exist independently of any specific task.\n\n- **Definition of Done** — Observable success criteria\n- **Regression Guardrails** — Invariants that must never break\n- **Scenarios** — Gherkin-style journeys for E2E tests\n\nFor detailed structure, examples, and templates, see the [Living Specs Practice Guide](/practices/living-specs).\n\n## Relationship to Other Patterns\n\n**[The PBI](/patterns/the-pbi)** — PBIs are the transient execution units (Delta) that reference specs for context. When a PBI changes contracts, it updates the spec in the same commit.\n\n**[Feature Assembly](/practices/feature-assembly)** — Specs define the acceptance criteria verified during assembly. The diagram above shows this flow.\n\n**[Experience Modeling](/patterns/experience-modeling)** — Experience models capture user journeys; specs capture the technical contracts that implement those journeys.\n\n**[Context Engineering](/concepts/context-engineering)** — Specs are structured context assets optimized for agent consumption, with predictable sections (Blueprint, Contract) for efficient extraction.\n\n## References\n\n- [Living Specs Practice Guide](/practices/living-specs) — Implementation instructions, templates, and best practices\n- [Living Documentation (Martin Fowler)](https://martinfowler.com/bliki/LivingDocumentation.html)",
    "tags": [
      "Documentation",
      "Living Documentation",
      "Spec-Driven Development",
      "Context Engineering"
    ]
  },
  {
    "slug": "agent-personas",
    "collection": "practices",
    "title": "Agent Personas",
    "description": "A guide on how to add multiple personas to an AGENTS.md file, with examples.",
    "status": "Live",
    "content": "## Overview\n\nDefining clear personas for your agents is crucial for ensuring they understand their role, trigger constraints, and goals. This guide demonstrates how to structure multiple personas within your `AGENTS.md` file.\n\nPersonas are a context engineering practice—they scope agent work by defining boundaries and focus, not by role-playing. When combined with [Model Routing](/patterns/model-routing), personas can also specify which computational tool (LLM) to use for each type of work.\n\nFor the full specification of the `AGENTS.md` file, see the [AGENTS.md Specification](./agents-md-spec).\n\n## How to Add Multiple Personas\n\nYou can define multiple personas by specifying triggers, goals, and guidelines for each. This allows different agents (or the same agent in different contexts) to adopt specific behaviors suited for the task at hand.\n\n### Example: Our Internal Personas\n\nBelow are the personas we use, serving as a template for your own `AGENTS.md`.\n\n```markdown\n### 1.1. Lead Developer / Astro Architect (@Lead)\n**Trigger:** When asked about system design, specs, or planning.\n* **Goal**: Specify feature requirements, architecture, and required changes. Analyze the project state and plan next steps.\n* **Guidelines**\n  - **Schema Design:** When creating new content types, immediately define the Zod schema in `src/content/config.ts`.\n  - **Routing:** Use Astro's file-based routing. For dynamic docs, use `[...slug].astro` and `getStaticPaths()`.\n  - **SEO:** Ensure canonical URLs and Open Graph tags are generated for every new page.\n  - **Dev Performace:** Focus on tangible, deliverable outcomes.\n  - **Spec driven development:** Always produce clear, concise specifications before handing off to implementation agents.\n  - **Planned iterations:** Break down large tasks into manageable PBIs with clear acceptance criteria.\n\n### 1.2. Designer / User Experience Lead (@Designer)\n**Trigger:** When asked about Design system UI/UX, design systems, or visual consistency.\n* **Goal**: Ensure the design system can be effectively utilized by agents and humans alike.\n* **Guidelines**\n  - **Design Tokens:** Tokens must be set in `src/styles/tokens.css`. No hardcoded colors or fonts.\n  - **Component Consistency:** All components must adhere to the design system documented in `src/pages/resources/design-system.astro`. \n  - **Accessibility:** Ensure all components meet WCAG 2.1 AA standards.\n  - **Documentation:** Update the Design System page with any new components or styles introduced.\n  - **Experience Modeling Allowed:** Design system components are protected by a commit rule: use \\[EM] tag to override the rule.\n  \n### 1.3. Content Engineer / Technical Writer (@Content)\n**Trigger:** When asked to create or update documentation, articles, or knowledge base entries.\n* **Goal**: Produce high-quality, structured content that adheres to the project's schema and style guidelines.\n* **Guidelines**\n  - **Content Structure:** Follow the established folder structure in `src/content/` for concepts\n  \n### 1.4. Developer / Implementation Agent (@Dev)\n**Trigger:** When assigned implementation tasks or bug fixes.\n* **Goal**: Implement features, fix bugs, and ensure the codebase remains healthy and maintainable.\n* **Guidelines**\n  - **Expect PBIs:** Always work from a defined Product Backlog Item (PBI) with clear acceptance criteria, if available.\n  - **Type Safety:** Use TypeScript strictly. No `any` types allowed.\n  - **Component Imports:** Explicitly import all components used in `.astro` files.\n  - **Testing:** Ensure all changes pass `pnpm check` and `pnpm lint`\n  - **Document progress:** Update the relevant PBI in `docs/backlog/` with status and notes.md after completing tasks.\n```\n\n## Model Routing and Personas\n\nPersonas define **what work to do** and **how to scope it**. [Model Routing](/patterns/model-routing) is a separate practice that defines **which computational tool to use**.\n\n### Current State (December 2025)\n\nAI-assisted IDEs (Cursor, Windsurf, Claude Code) do **not** automatically select models based on persona definitions. Model selection is manual.\n\n### Best Practice: Keep Them Separate\n\n**Don't add model profiles to `AGENTS.md`** - It adds noise to the context window without providing automation value.\n\nInstead:\n1. **Keep personas focused** on triggers, goals, and guidelines\n2. **Use Model Routing separately** - Manually select models based on the task characteristics\n3. **Reference the pattern** when deciding which model to use\n\n### Matching Personas to Model Profiles\n\nWhen you invoke a persona, choose your model based on the work type:\n\n| Persona Type | Typical Work | Recommended Profile |\n|---|---|---|\n| Lead / Architect | System design, specs, architectural decisions | High Reasoning |\n| Developer / Implementation | Code generation, refactoring, tests | High Throughput |\n| Documentation Analyst | Legacy code analysis, comprehensive docs | Massive Context |\n\nThe workflow:\n1. **Identify the persona** needed for your task\n2. **Select the appropriate model** manually in your IDE\n3. **Invoke the persona** with your prompt\n\nThis keeps `AGENTS.md` lean and focused on scoping agent work, while model selection remains a deliberate engineering decision.",
    "tags": ["agents", "personas", "guide"]
  },
  {
    "slug": "agents-md-spec",
    "collection": "practices",
    "title": "AGENTS.md Specification",
    "description": "The definitive guide to the AGENTS.md file, including philosophy, anatomy, and implementation strategy.",
    "status": "Live",
    "content": "## DEFINITION\n\n`AGENTS.md` is an open format for guiding coding agents, acting as a \"README for agents.\" It provides a dedicated, predictable place for context and instructions—such as build steps, tests, and conventions—that help AI coding agents work effectively on a project.\n\nWe align with the [agents.md specification](https://agents.md), treating this file as the authoritative source of truth for agentic behavior within the ASDLC.\n\n## CORE PHILOSOPHY\n\n**1. A README for Agents**\n\nJust as `README.md` is for humans, `AGENTS.md` is for agents. It complements existing documentation by containing the detailed context—build commands, strict style guides, and test instructions—that agents need but might clutter a human-facing README.\n\n**2. Context is Code**\n\nIn the ASDLC, we treat `AGENTS.md` with the same rigor as production software:\n\n- **Version Controlled**: Tracked via git and PRs.\n- **Falsifiable**: Contains clear success criteria for agent actions.\n- **Optimized**: Structured to maximize signal-to-noise ratio for LLM context windows, preventing \"Lost in the Middle\" issues.\n\n## ASDLC IMPLEMENTATION STRATEGY\n\nWhile the [agents.md](https://agents.md) standard provides the format, the ASDLC defines a specific strict implementation to ensure reliability. We structure our `AGENTS.md` not just as a list of tips, but as a segmented database of rules.\n\n### 1. Identity Anchoring (The Persona)\n\nEstablishes the specific expertise required to prune the model's search space. Without this, the model reverts to the \"average\" developer found in its training data. For detailed examples of defining multiple personas, see [Agent Personas](./agent-personas).\n\nBad: \"You are a coding assistant.\"\n\nGood: \"You are a Principal Systems Engineer specializing in Go 1.22, gRPC, and high-throughput concurrency patterns. You favor composition over inheritance.\"\n\n### 2. Contextual Alignment (The Mission)\n\nA concise, high-level summary of the project’s purpose and business domain. This is often formatted as a blockquote at the top of the file to \"set the stage\" for the agent's session.\n\n- **Why:** LLMs are stateless. A 50-token description differentiates a \"User\" in a banking app (high security/compliance) from a \"User\" in a casual game (low friction), reducing the need for corrective follow-up prompts.\n    \n- **Format:** Focus on the \"What\" and \"Why,\" not the narrative history.\n    \n\n**Example:**\n\n> **Project:** \"ZenTask\" - A minimalist productivity app. **Core Philosophy:** Local-first data architecture; offline support is mandatory.\n\n### 3. Operational Grounding (The Tech Stack)\n\nExplicitly defines the software environment to prevent \"Library Hallucination.\" This section must be exhaustive regarding key dependencies and restrictive regarding alternatives.\n\n- Directive: \"Runtime: Node.js v20 (LTS) exclusively.\"\n- Directive: \"Styling: Tailwind CSS only. Do not use CSS Modules or Emotion.\"\n- Directive: \"State: Zustand only. Do not use Redux.\"\n\n### 4. Behavioral Boundaries (Context Gates)\n\nReplaces vague \"Guardrails\" with a \"Three-Tiered Boundary\" system, or _constitution_. As the models are probabilistic, absolute prohibitions are unrealistic. Instead, this system categorizes rules by severity and required action. These rules are aimed to reducing the likelihood of critical errors. Note that you should always complement\nthe _constitution_ with explicit and deterministic _quality gates_ enforced by tests, linters, and CI/CD pipelines.\n\n**Tier 1 (Constitutive - ALWAYS): Non-negotiable standards.**\n\nExample: \"Always add JSDoc to exported functions.\"\n\n**Tier 2 (Procedural - ASK): High-risk operations requiring Human-in-the-Loop.**\n\nExample: \"Ask before running database migrations or deleting files.\"\n\n**Tier 3 (Hard Constraints - NEVER): Safety limits.**\n\nExample: \"Never commit secrets, API keys, or .env files.\"\n\n### 5. Semantic Directory Mapping\n\nWhen documenting the codebase structure in AGENTS.md, prefer Annotated YAML over ASCII trees.\n\n- **Use Valid Syntax:** Ensure the block allows an LLM to parse the structure as a dictionary.\n- **Annotate Key Files:** do not just list files; map them to a brief string describing their responsibility. This acts as a 'map legend' for the Agent, allowing it to route coding tasks to the correct file without needing to scan the file content first.\n- **Omit Noise:** Only include directories and files relevant to the Agent's operation or the architectural scope.\n\n**Example:**\n\n```yaml\ndirectory_map:\n  src:\n    # Core Application Logic\n    main.py: \"Application entry point; initializes the Agent Orchestrator\"\n    \n    agents:\n      # Individual Agent definitions\n      base_agent.py: \"Abstract base class defining the 'step()' and 'memory' interfaces\"\n    \n    utils:\n      # Shared libraries\n      llm_client.py: \"Wrapper for OpenAI/Anthropic APIs with retry logic\"\n```\n\n### 6. The Command Registry\n\nA lookup table mapping intent to execution. Agents often default to standard commands (npm test) which may fail in custom environments (make test-unit). The Registry forces specific tool usage.\n\n| Intent | Command | Notes |\n| :--- | :--- | :--- |\n| **Build** | pnpm build | Outputs to `dist/` |\n| **Test** | pnpm test:unit | Flags: --watch=false |\n| **Lint** | pnpm lint --fix | Self-correction enabled |\n\n\n### 7. Implementation notes\n\nXML-Tagging for Semantic Parsing\n\nTo maximize adherence, use pseudo-XML tags to encapsulate rules. This creates a \"schema\" that the model can parse more strictly than bullet points.\n\n```xml\n<coding_standard name=\"React Hooks\">\n  <instruction>\n    Use functional components and Hooks. Avoid Class components.\n    Ensure extensive use of custom hooks for logic reuse.\n  </instruction>\n  <anti_pattern>\n    class MyComponent extends React.Component {... }\n  </anti_pattern>\n  <preferred_pattern>\n    const MyComponent = () => {... }\n  </preferred_pattern>\n</coding_standard>\n```\n\n## REFERENCE TEMPLATE\n\n`Filename: AGENTS.md`\n\n```md\n# AGENTS.md - Context & Rules for AI Agents\n\n> **Project Mission:** High-throughput gRPC service for processing real-time financial transactions.\n> **Core Constraints:** Zero-trust security model, ACID compliance required for all writes.\n\n## 1. Identity & Persona\n- **Role:** Senior Systems Engineer\n- **Specialization:** High-throughput distributed systems in Go.\n- **Objective:** Write performant, thread-safe, and maintainable code.\n\n## 2. Tech Stack (Ground Truth)\n- **Language:** Go 1.22 (Use `iter` package for loops)\n- **Transport:** gRPC (Protobuf v3)\n- **Database:** PostgreSQL 15 with `pgx` driver (No ORM allowed)\n- **Infra:** Kubernetes, Helm, Docker\n\n## 3. Operational Boundaries (CRITICAL)\n- **NEVER** commit secrets, tokens, or `.env` files.\n- **NEVER** modify `api/proto` without running `buf generate`.\n- **ALWAYS** handle errors; never use `_` to ignore errors.\n- **ASK** before adding external dependencies.\n\n## 4. Command Registry\n| Action | Command | Note |\n| :--- | :--- | :--- |\n| **Build** | `make build` | Outputs to `./bin` |\n| **Test** | `make test` | Runs with `-race` detector |\n| **Lint** | `golangci-lint run` | Must pass before commit |\n| **Gen** | `make proto` | Regenerates gRPC stubs |\n\n## 5. Development Map\n```yaml\ndirectory_map:\n  api:\n    proto: \"Protocol Buffers definitions (Source of Truth)\"\n  cmd:\n    server: \"Main entry point, dependency injection wire-up\"\n  internal:\n    biz: \"Business logic and domain entities (Pure Go)\"\n    data: \"Data access layer (Postgres + pgx)\"\n```\n\n## 6. Coding Standards\n```xml\n<rule_set name=\"Concurrency\">\n  <instruction>Use `errgroup` for managing goroutines. Avoid bare `go` routines.</instruction>\n  <example>\n    <bad>go func() {... }()</bad>\n    <good>g.Go(func() error {... })</good>\n  </example>\n</rule_set>\n```\n\n## 7. Context References\n- **Database Schema:** Read `@database/schema.sql`\n- **API Contracts:** Read `@api/v1/service.proto`\n```",
    "tags": ["governance", "agents", "specification"]
  },
  {
    "slug": "living-specs",
    "collection": "practices",
    "title": "Living Specs",
    "description": "Practical guide to creating and maintaining specs that evolve alongside your codebase.",
    "status": "Experimental",
    "content": "## Overview\n\nThis guide provides practical instructions for implementing the [Specs](/patterns/the-spec) pattern. While the pattern describes *what* specs are and *why* they matter, this guide focuses on *how* to create and maintain them.\n\n## When to Create a Spec\n\nCreate a spec when starting work that involves:\n\n**Feature Domains** — New functionality that introduces architectural patterns, API contracts, or data models that other parts of the system depend on.\n\n**User-Facing Workflows** — Features with defined user journeys and acceptance criteria that need preservation for future reference.\n\n**Cross-Team Dependencies** — Any feature that other teams will integrate with, requiring clear contract definitions.\n\n**Don't create specs for:** Simple bug fixes, trivial UI changes, configuration updates, or dependency bumps.\n\n## Spec granularity\n\nA spec should be detailed enough to serve as a contract for the feature, but not so detailed that it becomes a maintenance burden.\n\nSome spec features, like gherkin scenarios, are not always necessary if the feature is simple or well-understood. \n\n## When to Update a Spec\n\nUpdate an existing spec when:\n\n- API contracts change (new endpoints, modified payloads, deprecated routes)\n- Data schemas evolve (migrations, new fields, constraint changes)\n- Quality targets shift (performance, security, accessibility requirements)\n- Anti-patterns are discovered (during review or post-mortems)\n- Architecture decisions are made (any ADR should update relevant specs)\n\n**Golden Rule:** If code behavior changes, the spec MUST be updated in the same commit.\n\n## File Structure\n\nOrganize specs by **feature domain**, not by sprint or ticket number.\n\n```\n/project-root\n├── ARCHITECTURE.md           # Global system rules\n├── plans/                    # Feature-level specs\n│   ├── user-authentication/\n│   │   └── spec.md\n│   ├── payment-processing/\n│   │   └── spec.md\n│   └── notifications/\n│       └── spec.md\n└── src/                      # Implementation code\n```\n\n**Conventions:**\n- Directory name: kebab-case, matches the feature's conceptual name\n- File name: always `spec.md`\n- Location: `/plans/{feature-domain}/spec.md`\n- Scope: one spec per independently evolvable feature\n\n## Maintenance Protocol\n\n### Same-Commit Rule\n\nIf code changes behavior, update the spec in the same commit. Add \"Spec updated\" to your PR checklist.\n\n```\ngit commit -m \"feat(notifications): add SMS fallback\n\n- Implements SMS delivery when WebSocket fails\n- Updates /plans/notifications/spec.md with new transport layer\"\n```\n\n### Deprecation Over Deletion\n\nMark outdated sections as deprecated rather than removing them. This preserves historical context.\n\n```markdown\n### Architecture\n\n**[DEPRECATED 2024-12-01]**\n~~WebSocket transport via Socket.io library~~\nReplaced by native WebSocket API to reduce bundle size.\n\n**Current:**\nNative WebSocket connection via `/api/ws/notifications`\n```\n\n### Bidirectional Linking\n\nLink code to specs and specs to code:\n\n```typescript\n// Notification delivery must meet 100ms latency requirement\n// See: /plans/notifications/spec.md#contract\n```\n\n```markdown\n### Data Schema\nImplemented in `src/types/Notification.ts` using Zod validation.\n```\n\n## Template\n\n```markdown\n# Feature: [Feature Name]\n\n## Blueprint\n\n### Context\n[Why does this feature exist? What business problem does it solve?]\n\n### Architecture\n- **API Contracts:** `[METHOD] /api/v1/[endpoint]` - [Description]\n- **Data Models:** See `[file path]`\n- **Dependencies:** [What this depends on / what depends on this]\n\n### Anti-Patterns\n- [What agents must avoid, with rationale]\n\n## Contract\n\n### Definition of Done\n- [ ] [Observable success criterion]\n\n### Regression Guardrails\n- [Critical invariant that must never break]\n\n### Scenarios\n**Scenario: [Name]**\n- Given: [Precondition]\n- When: [Action]\n- Then: [Expected outcome]\n```\n\n## Anti-Patterns\n\n### The Stale Spec\n**Problem:** Spec created during planning, never updated as the feature evolves.\n\n**Solution:** Make spec updates mandatory in Definition of Done. Add PR checklist item.\n\n### The Spec in Slack\n**Problem:** Design decisions discussed in chat but never committed to the repo.\n\n**Solution:** After consensus, immediately update `spec.md` with a commit linking to the discussion.\n\n### The Monolithic Spec\n**Problem:** A single 5000-line spec tries to document the entire application.\n\n**Solution:** Split into feature-domain specs. Use `ARCHITECTURE.md` only for global cross-cutting concerns.\n\n### The Spec-as-Tutorial\n**Problem:** Spec reads like a beginner's guide, full of basic programming concepts.\n\n**Solution:** Assume engineering competence. Document constraints and decisions, not general knowledge.\n\n### The Copy-Paste Code\n**Problem:** Spec duplicates large chunks of implementation code.\n\n**Solution:** Reference canonical sources with file paths. Only include minimal examples to illustrate patterns.\n\n## References\n\n- [Specs Pattern](/patterns/the-spec) — Conceptual foundation\n- [The PBI](/patterns/the-pbi) — Execution units that reference specs\n- [Living Documentation (Martin Fowler)](https://martinfowler.com/bliki/LivingDocumentation.html)",
    "tags": ["Documentation", "Spec-Driven Development", "Living Documentation"]
  },
  {
    "slug": "micro-commits",
    "collection": "practices",
    "title": "Micro-Commits",
    "description": "Ultra-granular commit practice for agentic workflows, treating version control as reversible save points.",
    "status": "Live",
    "content": "## Definition\n\n**Micro-Commits** is the practice of committing code changes at much higher frequency than traditional development workflows. Each discrete task—often a single function, test, or file—receives its own commit.\n\nWhen working with LLM-generated code, commits become \"save points in a game\": checkpoints that enable instant rollback when probabilistic outputs introduce bugs or architectural drift.\n\n## The Problem: Coarse-Grained Commits in Agentic Workflows\n\nTraditional commit practices optimize for human readability and PR review: \"logical units of work\" that span multiple files and implement complete features.\n\nThis fails in agentic workflows because:\n\n**LLM outputs are probabilistic** — A model might generate correct code for 3 files and introduce subtle bugs in the 4th. Bundling all 4 files into one commit makes rollback destructive.\n\n**Regression to mediocrity** — Without checkpoints, it's difficult to identify where LLM output drifted from the [Spec](/patterns/the-spec) contracts.\n\n**Context loss** — Large commits obscure the sequence of decisions. When debugging, you need to know \"what changed, when, and why.\"\n\n**No emergency exit** — If an LLM generates a tangled mess across 10 files, your only option is manual surgery or discarding hours of work.\n\n## The Solution: Commit After Every Task\n\nMake a commit immediately after:\n- Completing a [PBI](/patterns/the-pbi) subtask\n- Generating a single function or module\n- Making a file pass linting/compilation\n- Adding one test\n- Any LLM-assisted edit that produces working code\n\nThis creates a breadcrumb trail of working states.\n\n## The Practice\n\n### 4.1. Atomic Tasks → Atomic Commits\n\nBreak work into small, testable chunks. Each chunk maps to one commit.\n\n**Example PBI:** \"Add OAuth login flow\"\n\n**Commit sequence:**\n```\n1. feat: add OAuth config schema\n2. feat: implement token exchange endpoint\n3. feat: add session storage for OAuth tokens\n4. test: add OAuth flow integration test\n5. refactor: extract OAuth error handling\n```\n\nThis aligns with atomic [PBIs](/patterns/the-pbi): small, bounded execution units.\n\n### 4.2. Commit Messages as Execution Log\n\nCommit messages document the sequence of LLM-assisted changes. They serve as:\n- **Context for debugging** — \"The bug appeared after commit 7.\"\n- **Briefing material for AI** — Feed recent commits to an LLM to explain current state.\n- **Audit trail** — Track architectural decisions embedded in code changes.\n\n**Format:**\n```\ntype(scope): brief description\n\n- Detail 1\n- Detail 2\n```\n\n**Example:**\n```\nfeat(auth): implement OAuth token validation\n\n- Add JWT verification middleware\n- Extract claims from token payload\n- Return 401 on expired tokens\n```\n\n### 4.3. Branches and Worktrees for Isolation\n\nUse branches or git worktrees to isolate LLM experiments:\n\n**Branches** — Separate experimental work from stable code. Merge only after validation.\n\n**Worktrees** — Run parallel LLM sessions on the same repository without context conflicts. Each worktree is an independent working directory.\n\n**Example workflow:**\n```bash\n# Create worktree for LLM experiment\ngit worktree add ../project-experiment experiment-oauth\n\n# Work in worktree, commit frequently\ncd ../project-experiment\n# ... LLM generates code ...\ngit commit -m \"feat: add OAuth callback handler\"\n\n# If successful, merge into main\ngit checkout main\ngit merge experiment-oauth\n\n# If failed, discard worktree\ngit worktree remove ../project-experiment\n```\n\nThis prevents contaminating the main branch with failed LLM output.\n\n### 4.4. Rollback as First-Class Operation\n\nWhen LLM output introduces bugs:\n\n**Identify the bad commit** — Review recent history to find where issues appeared.\n\n**Rollback to last known good state:**\n```bash\n# Soft reset (keeps changes as uncommitted)\ngit reset --soft HEAD~1\n\n# Hard reset (discards changes entirely)\ngit reset --hard HEAD~1\n```\n\n**Selective revert:**\n```bash\n# Revert specific commit without losing subsequent work\ngit revert <commit-hash>\n```\n\nThis is only safe because micro-commits isolate changes.\n\n### 5. Tidy History for Comprehension\n\nGranular commits create noisy history. Before merging to main, optionally squash related commits into logical units:\n\n```bash\n# Interactive rebase to squash last 5 commits\ngit rebase -i HEAD~5\n```\n\nThis preserves detailed history during development while creating clean history for long-term maintenance.\n\n**Trade-off:** Squashing removes granular rollback points. Only squash after validation passes [Quality Gates](/concepts/context-gates).\n\n## Relationship to The PBI\n\n[PBIs](/patterns/the-pbi) define **what to build**. Micro-Commits define **how to track progress**.\n\n**Atomic PBIs** (small, bounded tasks) naturally produce micro-commits. Each PBI generates 1-5 commits depending on complexity.\n\n**Example mapping:**\n- **PBI:** \"Implement retry logic with exponential backoff\"\n- **Commits:**\n  1. `feat: add retry wrapper function`\n  2. `feat: implement exponential backoff calculation`\n  3. `test: add retry logic unit tests`\n  4. `docs: update retry behavior in spec`\n\nThis makes PBI progress traceable and reversible.\n\n## References\n\n### Framework Patterns\n- [The PBI](/patterns/the-pbi) — Atomic execution units that map to commit sequences\n- [Context Gates](/concepts/context-gates) — Validation checkpoints that rely on granular commits\n- [Agentic SDLC](/concepts/agentic-sdlc) — The cybernetic loop where micro-commits enable rapid iteration\n\n### External Resources\n- [My LLM Coding Workflow Going into 2026](https://addyo.substack.com/p/my-llm-coding-workflow-going-into) — Addy Osmani's practical guide emphasizing commits as \"save points in a game\"",
    "tags": ["Version Control", "Git", "Safety", "Rollback"]
  }
]
