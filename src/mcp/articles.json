[
  {
    "slug": "4d-framework",
    "collection": "concepts",
    "title": "The 4D Framework (Anthropic)",
    "description": "A cognitive model codifying four essential competencies—Delegation, Description, Discernment, and Diligence—for effective generative AI use.",
    "status": "Live",
    "content": "## Definition\n\nThe **4D Framework** is a cognitive model for human-AI collaboration developed by [Anthropic](https://www.anthropic.com/research) in partnership with Dr. Joseph Feller and Rick Dakan as part of the *AI Fluency* curriculum.\n\nThe framework codifies four essential competencies for leveraging generative AI effectively and responsibly:\n\n1. **Delegation** — The Strategy\n2. **Description** — The Prompt\n3. **Discernment** — The Review\n4. **Diligence** — The Liability\n\nUnlike process models (e.g., Agile or Double Diamond) that dictate workflow timing, the 4D Framework specifies *how* to interact with AI systems. It positions the human not merely as a \"prompter,\" but as an **Editor-in-Chief**, accountable for strategic direction and risk management.\n\n## The Four Dimensions\n\n### Delegation (The Strategy)\n\nBefore engaging with the tool, the human operator must determine what, if anything, should be assigned to the AI. This is a strategic decision between **Automation** (offloading repetitive tasks) and **Augmentation** (leveraging AI as a thought partner).\n\n**Core Question:** \"Is this task 'boilerplate' with well-defined rules (High Delegation), or does it demand nuanced judgment, deep context, or ethical considerations (Low Delegation)?\"\n\n### Description (The Prompt)\n\nAI output quality is directly proportional to input quality. \"Description\" transcends prompt engineering hacks by emphasizing **Context Transfer**—delivering explicit goals, constraints, and data structures required for the task.\n\n**Core Question:** \"Have I specified the constraints, interface definitions, and success criteria needed for this task?\"\n\n### Discernment (The Review)\n\nThis marks the transition from **Creator** to **Editor**. The human must rigorously assess AI output for accuracy, hallucinations, bias, and overall quality. Failing to apply discernment is a leading cause of \"AI Technical Debt.\"\n\n**Core Question:** \"If I authored this output, would it meet code review standards? Does it introduce fictitious libraries or violate design tokens?\"\n\n### Diligence (The Liability)\n\nThe human user retains full accountability for outcomes. Diligence acknowledges that while AI accelerates execution, it never removes user responsibility for security, copyright, or ethical compliance.\n\n**Core Question:** \"Am I exposing PII in the context window? Am I deploying unvetted code to production?\"\n\n## Key Characteristics\n\n### The Editor-in-Chief Mental Model\n\nThe 4D Framework repositions the human from \"prompt writer\" to \"editorial director.\" Just as a newspaper editor doesn't write every article but maintains accountability for what gets published, the AI-fluent professional maintains responsibility for all AI-generated outputs.\n\n### Continuous Cycle\n\nThese four dimensions are not sequential steps but concurrent concerns. Every AI interaction requires simultaneous attention to all four:\n\n- What should I delegate?\n- How clearly have I described it?\n- How critically am I reviewing the output?\n- What risks am I accepting?\n\n## Anti-Patterns\n\n| Anti-Pattern | Description |\n|--------------|-------------|\n| **Over-Delegation** | Assigning strategic decisions or ethically sensitive tasks to AI |\n| **Vague Description** | Using natural language prompts without context, constraints, or examples |\n| **Blind Acceptance** | Copy-pasting AI output without verification |\n| **Liability Denial** | Assuming AI-generated content is inherently trustworthy or legally defensible |\n\n## ASDLC Usage\n\nApplied in: [AGENTS.md Specification](/practices/agents-md-spec), [Context Engineering](/concepts/context-engineering), [Context Gates](/concepts/context-gates)\n\nThe 4D dimensions map to ASDLC constructs: Delegation → agent autonomy levels, Description → context engineering, Discernment → context gates, Diligence → guardrail protocols.\n\n## References\n\n- [Anthropic AI Fluency Course](https://anthropic.skilljar.com/page/ai-fluency) — Original source of the framework",
    "tags": ["AI Fluency", "Human-AI Collaboration", "Cognitive Model", "Prompt Engineering"]
  },
  {
    "slug": "agentic-sdlc",
    "collection": "concepts",
    "title": "Agentic SDLC",
    "description": "Framework for industrializing software development where agents serve as the logistic layer while humans design, govern, and optimize the flow.",
    "status": "Live",
    "content": "## Definition\n\nThe Agentic Software Development Life Cycle (ASDLC) is a framework for industrializing software engineering. It represents the shift from craft-based development (individual artisans, manual tooling, implicit knowledge) to industrial-scale production (standardized processes, agent orchestration, deterministic protocols).\n\n> \"Agentic architecture is the conveyor belt for knowledge work.\" — [Ville Takanen](https://villetakanen.com/blog/scenario-26-industrialization-of-knowledge-work/)\n\nASDLC is not about \"AI coding assistants\" that make developers 10% faster. It's about building the **software factory**—systems where agents serve as the architecture of labor while humans design, govern, and optimize the flow.\n\n## The Industrial Thesis\n\n**Agents do not replace humans; they industrialize execution.**\n\nJust as robotic arms automate welding without replacing manufacturing expertise, agents automate high-friction parts of knowledge work (logistics, syntax, verification) while humans focus on intent, architecture, and governance.\n\nIn this model:\n- **Agents are the logistic layer** — Moving information, verifying specs, executing tests\n- **Context is the supply chain** — Just-in-Time delivery of requirements, schemas, and code\n- **Standardization is mandatory** — Schemas, typed interfaces, deterministic protocols replace \"vibes\"\n\n## The Cybernetic Model\n\nASDLC operates at [L3 Conditional Autonomy](/concepts/levels-of-autonomy)—a \"Fighter Jet\" model where the Agent acts as the Pilot executing maneuvers, and the Human acts as the Instructor-in-the-Cockpit.\n\n**Key Insight:** Compute is cheap, but novelty and correctness are expensive. Agents naturally drift toward the \"average\" solution (Regression to the Mean). The Instructor's role is not to write code, but to define failure boundaries (Determinism) and inject strategic intent (Steering) that guides agents out of mediocrity.\n\n## The Cybernetic Loop\n\nThe lifecycle replaces the linear CI/CD pipeline with a high-frequency feedback loop:\n\n**Mission Definition**: The Instructor defines the \"Objective Packet\" (Intent + Constraints). This is the core of Context Engineering.\n\n**Generation (The Maneuver)**: The Agent autonomously maps context—often using the Model Context Protocol (MCP) to fetch live data—and executes the task.\n\n**Verification (The Sim)**: Automated Gates check for technical correctness (deterministic), while the Agent's Constitution steers semantic intent (probabilistic).\n\n**Course Correction (HITL)**: The Instructor intervenes on technically correct but \"generic\" solutions to enforce architectural novelty.\n\n## Strategic Pillars\n\n### Factory Architecture (Orchestration)\nProjects structured with agents as connective tissue, moving from monolithic context windows to discrete, specialized stations (Planning, Spec-Definition, Implementation, Review).\n\n### Standardized Parts (Determinism)\nSchema-First Development where agents fulfill contracts, not guesses. `AGENTS.md`, `specs/`, and strict linting serve as the \"jigs\" and \"molds\" that constrain agent output.\n\n### Quality Control (Governance)\nAutomated, rigorous inspection through Probabilistic Unit Tests and Human-in-the-Loop (HITL) gates. Trust the _process_, not just the output.\n\n## ASDLC Usage\n\nFull project vision: [/docs/vision.md](../../docs/vision.md)\n\nApplied in: [Specs](/patterns/the-spec), [AGENTS.md Specification](/practices/agents-md-spec), [Context Gates](/concepts/context-gates), [Model Routing](/patterns/model-routing)",
    "tags": []
  },
  {
    "slug": "event-modeling",
    "collection": "concepts",
    "title": "Event Modeling",
    "description": "A system blueprinting method that centers on events as the primary source of truth, serving as a rigorous bridge between visual design and technical implementation.",
    "status": "Experimental",
    "content": "## Definition\n\n**Event Modeling** is a method for designing information systems by mapping what happens over time. It creates a linear blueprint that serves as the single source of truth for Product, Design, and Engineering.\n\nUnlike static diagrams (like ERDs or UML) that focus on structure, Event Modeling focuses on the **narrative of the system**. It visualizes the system as a film strip, showing exactly how a user’s action impacts the system state and what information is displayed back to them.\n\n### Core Components\nAn Event Model is composed of four distinct elements:\n\n* **Commands (Blue)**: The intent or action initiated by the user (e.g., \"Submit Order\").\n* **Events (Orange)**: A fact recorded by the system that cannot be changed (e.g., \"OrderPlaced\"). This is the single source of truth.\n* **Views (Green)**: Information displayed to the user, derived from previous events (e.g., \"Order Confirmation Screen\").\n* **Processes**: The logic or automation that reacts to events to trigger other commands or update views.\n\n## Why It Matters for AI\n\nIn modern software development, ambiguity is the enemy. While human engineers can infer intent from a loose visual mockup, AI models require explicit instructions.\n\nEvent Modeling forces implicit business rules to become explicit. By defining the exact data payload of every *Command* and the resulting state change of every *Event*, we provide AI agents with a deterministic roadmap. This ensures the generated code handles edge cases and data consistency correctly, rather than just \"looking right\" on the frontend.\n\n## Relationship to Requirements\n\nEvent Modeling acts as a bridge between **Visual Design** (what it looks like) and **Technical Architecture** (how it works).\n\nIt does not replace functional requirements; rather, it validates them. A feature is only considered \"defined\" when there is a complete path mapped from the user's view, through the command, to the stored event, and back to the view. This \"closed loop\" guarantees that every pixel on the screen is backed by real data.\n\n## References\n\n* [EventModeling.org](https://eventmodeling.org/) - The official home of the methodology.",
    "tags": []
  },
  {
    "slug": "levels-of-autonomy",
    "collection": "concepts",
    "title": "Levels of Autonomy",
    "description": "SAE-inspired taxonomy for AI agent autonomy in software development, from L1 (assistive) to L5 (full), standardized at L3 conditional autonomy.",
    "status": "Live",
    "content": "## Definition\n\nThe **Levels of Autonomy** scale categorizes AI systems based on their operational independence.\nUnlike general-purpose taxonomies, the Autonomous System Development Life Cycle (ASDLC) utilizes\nthis scale to establish the Safe Operating Area for software agents. \n\nWe define these levels to explicitly identify where the **Context Gate** (the boundary of human\noversight) must be placed. In ASDLC, autonomy is not a measure of intelligence; it is a measure\nof optimal risk.\n\n## The ASDLC Standard: L3\n\nASDLC standardizes practices for **Level 3 (Conditional Autonomy)** in software engineering. While \nthe industry frequently promotes **Level 5 (Full Autonomy)** as the ultimate goal, we believe this \nperspective may be counterproductive at the moment. Therefore, we intentionally establish Level 3 a\ns the sensible default.\n\n> ## Level 4 Autonomy Risks\n> \n> At Level 4 (L4), agents are advanced enough to operate for days without human intervention; \n> however, they lack the strategic foresight needed to maintain system integrity. This results in \n> a phenomenon known as Silent Drift—where the codebase continues to function technically but \n> gradually deteriorates into an unmanageable state.\n> \n> It's important to note that there are ways to mitigate this risk, such as implementing Advanced\n> Context Gate strategies and utilizing emerging tools to monitor architectural health for drift.\n> However, these solutions will need to be tested and validated over time.\n\n## The Scale\n\n| Level | Designation | Description | Human Role | Failure Mode |\n| :--- | :--- | :--- | :--- | :--- |\n| **L1** | Assistive | Autocomplete, Chatbots. Zero state retention. | Driver. Hands on wheel 100% of time. | Distraction / Minor Syntax Errors |\n| **L2** | Task-Based | \"Fix this function.\" Single-file context. | Reviewer. Checks output before commit. | Logic bugs within a single file. |\n| **L3** | Conditional | **ASDLC Standard.** \"Implement this feature.\" Multi-file orchestration. | Instructor. Defines constraints & intervenes on \"drift.\" | Regression to the Mean (Mediocrity). |\n| **L4** | High | \"Manage this backlog.\" Self-directed planning. | Auditor. Post-hoc analysis. | Silent Failure. Strategic drift over time. |\n| **L5** | Full | \"Run this company.\" | Consumer. Passive beneficiary. | Existential alignment drift. |\n\n## Analogy: The Self-Driving Standard (SAE)\n\nWe map software autonomy directly to the SAE J3016 automotive standard to clarify the \"Human-in-the-Loop\" requirements.\n\n| ASDLC Level | SAE Equivalent | The \"Steering Wheel\" Metaphor |\n| :--- | :--- | :--- |\n| **L1** | L1 (Driver Assist) | **Hands On, Feet On.** AI nudges the wheel (Lane Keep) or gas (Cruise), but Human drives. |\n| **L2** | L2 (Partial) | **Hands On (mostly).** AI handles steering and speed in bursts, but Human monitors constantly. |\n| **L3** | L3 (Conditional) | **Hands Off, Eyes On.** AI executes the maneuver (The Drive). Human is the Instructor ready to grab the wheel immediately. |\n| **L4** | L4 (High) | **Mind Off.** Sleeping in the back seat within a geo-fenced area. Dangerous if the \"fence\" (Context) breaks. |\n| **L5** | L5 (Full) | **No Steering Wheel.** The vehicle has no manual controls. |",
    "tags": []
  },
  {
    "slug": "spec-driven-development",
    "collection": "concepts",
    "title": "Spec-Driven Development (SDD)",
    "description": "Methodology where machine-readable specs define contracts before code, inverting traditional workflows to prevent agent hallucination and drift.",
    "status": "Experimental",
    "content": "## Definition\n\nSpec-Driven Development (SDD) means defining the \"What\" and the \"Why\" first. This is done in precise, machine-readable formats. Only then does an AI agent address the \"How.\"\n\nSDD inverts control. Instead of code serving as the source of documentation, the Spec becomes the authority. Code must then fulfill this Spec.\n\n> **Contrast:** For the anti-pattern SDD addresses, see [Vibe Coding](/concepts/vibe-coding).\n\nIn the era of Agentic tools, SDD addresses the issue of \"vibe coding,\" in which LLMs generate code from vague prompts. By grounding workflows in schemas, state machines, and contracts, SDD converts probabilistic outputs into reliable engineering artifacts.\n\n> [!WARNING]\n> **The Figma Trap**\n> A beautiful mockup is not a specification; it is a suggestion. Mockups typically demonstrate the \"happy path\" but hide the edge cases, error states, and data consistency strictures where production bugs live.\n>\n> **Never** treat a visual design as a complete technical requirement. AI agents given only mockups will hallucinate implementations for the missing logical layers.\n\n## SDD in the ASDLC\n\nIn the Agentic Software Development Life Cycle (ASDLC), we suggest using SDD as a comprehensive lifecycle-constraint system rather than merely for code generation. Specifications generate code, model the domain, and enforce architectural governance.\n\nOur methodology separates context to prevent misalignment:\n\n1. The Agent Constitution (Global Context): A permanent set of rules, such as \"Use safe databases\" or \"Store all info in one place.\" This makes sure everything created by the system meets company guidelines.\n2. The Docs (Reference Context): Long-term, stable documentation like Product Requirements Documents (PRDs) or earlier Specs that explain the overall goals and constraints.\n3. The Spec (Local Context): A short-term, targeted set of instructions for a task, used to validate immediate results.\n\n## Accessibility: Specs as Source Code\n\nIn ASDLC, Product Requirements Documents (PRDs), tasks, and Specs are treated like Source Code. The AI must be able to read Specs directly, not just see them in pictures or separate documents.\n\nTo make this happen, we either use the Model Context Protocol (MCP) or put the specs right next to the code:\n\n* Repository Colocation: Storing specs as Markdown or YAML files directly in the git repository (e.g., .specs/feature-login.md). This ensures that the version history matches the code history.\n* MCP Integration: Use MCP to connect to external tools (e.g., Linear or Jira) so the AI can read tasks as clear instructions.\n\n## Context Gates\n\nASDLC formalizes the classic SDD step-by-step process with strict context gates. Spec updates and handoffs should occur strictly through HITL (human-in-the-loop) gates.\n\n```mermaid\nflowchart LR\n %% caption: Context Gating of Spec-Driven Development\n A(SYSTEM CONTEXT)\n A.1(Documentation)\n A.2(User Guidance)\n B[Spec]\n C{GATE}\n D[[Agentic Development]]\n E{GATE}\n F[[...]]\n\n A --> B\n A.1 --> B\n A.2 --> B\n\n B --> C\n\n C --> D\n\n D --> E\n\n\n E --> |REQUEST FOR REFINEMENT| B\n E --> F\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/spec-driven-development-fig-1.svg\" alt=\"Context Gating of Spec-Driven Development\" />\n  <figcaption>Context Gating of Spec-Driven Development</figcaption>\n</figure>",
    "tags": []
  },
  {
    "slug": "vibe-coding",
    "collection": "concepts",
    "title": "Vibe Coding",
    "description": "Natural language code generation without formal specs—powerful for prototyping, problematic for production systems.",
    "status": "Experimental",
    "content": "## Definition\n\nVibe Coding is the practice of generating code directly from natural language prompts without formal specifications, schemas, or contracts. Coined by Andrej Karpathy, the term describes an AI-assisted development mode where engineers describe desired functionality conversationally (\"make this faster,\" \"add a login button\"), and the LLM produces implementation code.\n\nThis approach represents a fundamental shift: instead of writing specifications that constrain implementation, developers describe intent and trust the model to infer the details. The result is rapid iteration—code appears almost as fast as you can articulate what you want.\n\nWhile vibe coding accelerates prototyping and exploration, it inverts traditional software engineering rigor: the specification emerges *after* the code, if at all.\n\n## The Seduction of Speed\n\nThe productivity gains from vibe coding are undeniable:\n\n* **At Anthropic:** 80-90% of Claude Code's codebase is now written by Claude Code itself, with a 70% productivity increase per engineer since adoption.\n* **At Google:** Approximately 30% of code committed in 2024 was AI-generated.\n* **Industry-wide:** Engineers report 2-10x faster feature delivery for greenfield projects and prototypes.\n\nThis velocity is seductive. When a feature that previously took three days can be scaffolded in thirty minutes, the economic pressure to adopt vibe coding becomes overwhelming.\n\nThe feedback loop is immediate: describe the behavior, see the code, run it, iterate. For throwaway scripts, MVPs, and rapid exploration, this workflow is transformative.\n\n## The Failure Modes\n\nThe velocity advantage of vibe coding collapses when code must be maintained, extended, or integrated into production systems:\n\n### Technical Debt Accumulation\n\n**Forrester Research predicts that by 2026, 75% of technology leaders will face moderate-to-severe technical debt** directly attributable to AI-generated code. The mechanism is straightforward: code generated from vague prompts encodes vague assumptions.\n\nWhen specifications exist only in the prompt history (or the engineer's head), future maintainers inherit code without contracts. They must reverse-engineer intent from implementation—the exact problem formal specifications solve.\n\n### Copy-Paste Culture\n\n2024 marked the first year in industry history where **copy-pasted code exceeded refactored code**. This is a direct symptom of vibe coding: when generating fresh code is faster than understanding existing code, engineers default to regeneration over refactoring.\n\nThe result is systemic duplication. The same logic appears in fifteen places with fifteen slightly different implementations, none validated against a shared contract.\n\n### Silent Drift\n\nLLMs are probabilistic. When generating code from vibes, they make assumptions:\n- Error handling strategies (fail silently? throw? log?)\n- Data validation rules (what's a valid email?)\n- Concurrency models (locks? optimistic? eventual consistency?)\n\nThese assumptions are *never documented*. The code passes tests (if tests exist), but violates implicit architectural contracts. Over time, the system drifts toward inconsistency—different modules make different assumptions about the same concepts.\n\nBoris Cherny (Principal Engineer, Anthropic; creator of Claude Code) warns: **\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes.\"**\n\n> **\"Speed is seductive. Maintainability is survival.\"**  \n> — Boris Cherny, *The Peterman Podcast* (December 2025)\n\n### Regression to the Mean\n\nWithout deterministic constraints, LLMs trend toward generic solutions. Vibe coding produces code that works but lacks the specific optimizations, domain constraints, and architectural decisions that distinguish production systems from prototypes.\n\nThe model doesn't know that \"user IDs must never be logged\" or \"this cache must invalidate within 100ms.\" These constraints exist in specifications, not prompts.\n\n## Applications\n\nVibe coding is particularly effective in specific contexts:\n\n**Rapid Prototyping:** When validating product hypotheses, speed of iteration outweighs code quality. Vibe coding enables designers and product managers to generate functional prototypes without deep programming knowledge.\n\n**Throwaway Scripts:** One-off data migrations, analysis scripts, and temporary tooling benefit from vibe coding's velocity. Since the code has no maintenance burden, formal specifications are unnecessary overhead.\n\n**Learning and Exploration:** When experimenting with new APIs, frameworks, or architectural patterns, vibe coding provides immediate feedback. The goal is understanding, not production-ready code.\n\n**Greenfield MVPs:** Early-stage startups building minimum viable products often prioritize speed-to-market over maintainability. Vibe coding accelerates this phase, though technical debt must be managed during the transition to production.\n\n## ASDLC Usage\n\nIn ASDLC, vibe coding is recognized as a legitimate operational mode for bounded contexts (exploration, prototyping, throwaway code). However, for production systems, ASDLC mandates a transition to deterministic development.\n\n**The ASDLC position:**\n- Vibe coding is **steering** (probabilistic guidance via prompts)\n- Production requires **determinism** (schemas, tests, typed interfaces)\n- Both are necessary: prompts steer the agent; schemas enforce correctness\n\nApplied in:\n- [Spec-Driven Development](/concepts/spec-driven-development) — The production-grade alternative to vibe coding\n- [Context Gates](/concepts/context-gates) — Deterministic enforcement layer\n- [Levels of Autonomy](/concepts/levels-of-autonomy) — Human oversight model (L3: \"Hands Off, Eyes On\")\n\n## References\n\n- **Boris Cherny**, *The Peterman Podcast* (December 2025) — Claude Code creator's framework for disciplined AI-assisted development\n- **Forrester Research** (2024) — Technical debt predictions for AI-generated code\n- **Industry Data** — Google's 30% AI-generated code adoption, copy-paste vs refactor metrics\n\nSee also:\n- [Industry Alignment](/resources/industry-alignment) — External voices converging on ASDLC principles\n- [Spec-Driven Development](/concepts/spec-driven-development) — ASDLC's production-grade methodology\n- [Context Gates](/concepts/context-gates) — Deterministic enforcement layer",
    "tags": ["Disambiguation", "AI", "Code Quality", "Anti-Pattern"]
  },
  {
    "slug": "agent-constitution",
    "collection": "patterns",
    "title": "Agent Constitution",
    "description": "Prime directives that align agent behavior with system goals before action, acting as driver training vs. context gates as brakes.",
    "status": "Experimental",
    "content": "## The Prime Directives\n\nAn Agent Constitution is a set of \"Prime Directives\" injected into an agent's context window to align its intent and behavior with high-level principles. Unlike specific task instructions, the constitution provides the ethical and operational boundaries within which the agent must operate. It serves as the foundational \"superego\" for the agent, ensuring that even in ambiguous situations, its actions remain consistent with the system's overarching goals and safety guidelines.\n\n## Driver Training vs. Brakes\n\nTo understand the role of an Agent Constitution, consider the analogy of a car. Context Gates and other restrictive mechanisms act as the \"brakes\"—they stop the agent from doing something wrong after it has already attempted or formulated an action. The Agent Constitution, however, is the \"driver training.\" It shapes the agent's decision-making process *before* it even considers an action. By internalizing these rules, the agent is less likely to need the \"brakes\" because it is steering itself correctly from the start.\n\n## Implementation\n\nFor the standard implementation of this pattern, see the [AGENTS.md Specification](/practices/agents-md-spec). The specification details how to formally document and inject these directives into your agentic workflows.",
    "tags": []
  },
  {
    "slug": "model-routing",
    "collection": "patterns",
    "title": "Model Routing",
    "description": "Strategic assignment of LLM models to SDLC phases based on reasoning capability versus execution speed.",
    "status": "Live",
    "content": "## Definition\n\n**Model Routing** is the strategic assignment of different Large Language Models (LLMs) to different phases of the software development lifecycle based on their capability profile.\n\nDifferent computational tasks have different performance characteristics. Model Routing matches model capabilities to task requirements: **reasoning depth** during design phases and **speed with large context windows** during implementation phases.\n\nThis is a tool selection strategy, not a delegation strategy. Engineers remain accountable for output quality while selecting the appropriate computational tool for each phase.\n\n## The Problem: Single-Model Inefficiency\n\nUsing one model for all phases creates a mismatch between computational capability and task requirements.\n\nHigh-speed models struggle with architectural decisions requiring deep constraint satisfaction. Reasoning models are too slow for high-volume implementation tasks. Models with massive context windows are expensive when you only need to process small, focused changes.\n\nEach model class optimizes for different performance characteristics. Using the wrong one wastes either quality (insufficient reasoning) or resources (excessive capability for simple tasks).\n\n## The Solution: Capability-Based Assignment\n\nWe categorize models into three capability profiles aligned with [Agentic SDLC](/concepts/agentic-sdlc) phases:\n\n| Capability Profile | Optimization | Primary Use Cases | Model Examples |\n|---|---|---|---|\n| **High Reasoning** | Deep logic, high latency, \"System 2\" thinking | Writing [Specs](/patterns/the-spec), architectural decisions, logic debugging, security analysis | Gemini 3 Deep Think, DeepSeek V3.2, OpenAI o3-pro |\n| **High Throughput** | Speed, low latency, real-time execution | Code generation, refactoring, unit tests, UI implementation | Gemini 3 Flash, Llama 4 Scout, Claude Haiku 4.5 |\n| **Massive Context** | Repository-scale context (500k-5M tokens) | Documentation analysis, codebase navigation, legacy system understanding | Gemini 3 Pro (5M tokens), Claude 4.5 Sonnet (500k), GPT-5 (RAG-native) |\n\n*Model examples current as of December 27, 2025. The LLM landscape evolves rapidly—validate capabilities and availability before implementation.*\n\n## Relationship to Levels of Autonomy\n\n[Levels of Autonomy](/concepts/levels-of-autonomy) define human oversight requirements. Model Routing complements this by matching computational capability to task characteristics:\n\n- **Complex architectural decisions** (L3 with high uncertainty) → High Reasoning models\n- **Well-specified implementation tasks** (L3 with clear contracts) → High Throughput models\n- **Exploratory analysis** (L2 with discovery focus) → Massive Context models\n\nThis ensures that the computational tool's capability profile matches the task's computational requirements and the degree of human verification needed.\n\n## References\n\n### Framework Patterns\n- [Agent Personas](/practices/agent-personas) — Context engineering practice for scoping agent work, extended by model routing\n- [The Spec](/patterns/the-spec) — The artifact produced by High Reasoning models in the planning phase\n- [Context Engineering](/concepts/context-engineering) — The practice of structuring context for optimal LLM performance\n\n### External Resources\n- [My LLM Coding Workflow Going into 2026](https://addyo.substack.com/p/my-llm-coding-workflow-going-into) — Addy Osmani's workflow guide emphasizing pragmatic model selection and mid-task model switching patterns",
    "tags": ["LLM Selection", "Context Engineering", "ASDLC", "Agent Architecture"]
  },
  {
    "slug": "the-pbi",
    "collection": "patterns",
    "title": "The PBI",
    "description": "A transient execution unit that defines the delta (change) while pointing to permanent context (The Spec), optimized for agent consumption.",
    "status": "Live",
    "content": "## Definition\n\nThe Product Backlog Item (PBI) is the unit of execution in the ASDLC. While [The Spec](/patterns/the-spec) defines the **State** (how the system works), the PBI defines the **Delta** (the specific change to be made).\n\nIn an AI-native workflow, the PBI transforms from a \"User Story\" (negotiable conversation) into a **Prompt** (strict directive). The AI has flexibility in *how* code is written, but the PBI enforces strict boundaries on *what* is delivered.\n\n## The Problem: Ambiguous Work Items\n\nTraditional user stories (\"As a user, I want...\") are designed for human negotiation. They assume ongoing dialogue, implicit context, and shared understanding built over time.\n\nAgents don't negotiate. They execute. A vague story becomes a hallucinated implementation.\n\n**What fails without structured PBIs:**\n- Agents interpret scope liberally, touching unrelated code\n- No clear pointer to authoritative design decisions\n- Success criteria scattered across conversations\n- Merge conflicts from parallel agents hitting the same files\n\n## The Solution: Pointer, Not Container\n\nThe PBI acts as a **pointer** to permanent context, not a container for the full design. It defines the delta while referencing The Spec for the state.\n\n| Dimension   | The Spec                        | The PBI                          |\n| :---------- | :------------------------------ | :------------------------------- |\n| **Purpose** | Define the State (how it works) | Define the Delta (what changes)  |\n| **Lifespan**| Permanent (lives with the code) | Transient (closed after merge)   |\n| **Scope**   | Feature-level rules             | Task-level instructions          |\n| **Audience**| Architects, Agents (Reference)  | Agents, Developers (Execution)   |\n\n## Anatomy\n\nAn effective PBI consists of four parts:\n\n### 1. The Directive\n\nWhat to do, with explicit scope boundaries. Not a request—a constrained instruction.\n\n### 2. The Context Pointer\n\nReference to the permanent spec. Prevents the PBI from becoming a stale copy of design decisions that live elsewhere.\n\n### 3. The Verification Pointer\n\nLink to success criteria defined in the spec's Contract section. The agent knows exactly what \"done\" looks like.\n\n### 4. The Refinement Rule\n\nProtocol for when reality diverges from the spec. Does the agent stop? Update the spec? Flag for human review?\n\n## Bounded Agency\n\nBecause AI is probabilistic, it requires freedom to explore the \"How\" (implementation details, syntax choices). However, to prevent hallucination, we bound this freedom with non-negotiable constraints.\n\n**Negotiable (The Path):** Code structure, variable naming, internal logic flow, refactoring approaches.\n\n**Non-Negotiable (The Guardrails):** Steps defined in the PBI, outcome metrics in the Spec, documented anti-patterns, architectural boundaries.\n\nThe PBI is not a request for conversation—it's a constrained optimization problem.\n\n## Atomicity & Concurrency\n\nIn swarm execution (multiple agents working in parallel), each PBI must be:\n\n**Atomic:** The PBI delivers a complete, working increment. No partial states. If the agent stops mid-task, either the full change lands or nothing does.\n\n**Self-Testable:** Verification criteria must be executable without other pending PBIs completing first. If PBI-102 requires PBI-101's code to test, PBI-102 is not self-testable.\n\n**Isolated:** Changes target distinct files/modules. Two concurrent PBIs modifying the same file create merge conflicts and non-deterministic outcomes.\n\n### Dependency Declaration\n\nWhen a PBI requires another to complete first, the dependency is declared explicitly in the PBI structure—not discovered at merge time.\n\n## Relationship to Other Patterns\n\n**[The Spec](/patterns/the-spec)** — The permanent source of truth that PBIs reference. The Spec defines state; the PBI defines delta.\n\n**[PBI Authoring](/practices/pbi-authoring)** — The practice for writing effective PBIs, including templates and lifecycle.\n\n## References\n\n- [Spec-Driven Development](/concepts/spec-driven-development) — The overarching methodology\n- [Context Gates](/concepts/context-gates) — Validation checkpoints for PBI completion",
    "tags": ["Agile", "Product Backlog Item", "Spec-Driven Development", "Bounded Agency"]
  },
  {
    "slug": "the-spec",
    "collection": "patterns",
    "title": "Specs",
    "description": "Living documents that serve as the permanent source of truth for features, solving the context amnesia problem in agentic development.",
    "status": "Live",
    "content": "## Definition\n\nA **Spec** is the permanent source of truth for a feature. It defines *how* the system works (Design) and *how* we know it works (Quality).\n\nUnlike traditional tech specs or PRDs that are \"fire and forget,\" specs are **living documents**. They reside in the repository alongside the code and evolve with every change to the feature.\n\n## The Problem: Context Amnesia\n\nAgents do not have long-term memory. They cannot recall Jira tickets from six months ago or Slack conversations about architectural decisions. When an agent is tasked with modifying a feature, it needs immediate access to:\n\n- The architectural decisions that shaped the feature\n- The constraints that must not be violated\n- The quality criteria that define success\n\nWithout specs, agents reverse-engineer intent from code comments and commit messages—a process prone to hallucination and architectural drift.\n\nTraditional documentation fails because:\n- **Wikis decay** — separate systems fall out of sync with code\n- **Tickets disappear** — issue trackers capture deltas (changes), not state (current rules)\n- **Comments lie** — code comments describe implementation, not architectural intent\n- **Memory fails** — tribal knowledge evaporates when team members leave\n\nSpecs solve this by making documentation a **first-class citizen** in the codebase, subject to the same version control and review processes as the code itself.\n\n## State vs Delta\n\nThis is the core distinction that makes agentic development work at scale.\n\n| Dimension | The Spec | The PBI |\n|-----------|----------|---------|\n| **Purpose** | Define the State (how it works) | Define the Delta (what changes) |\n| **Lifespan** | Permanent (lives with the code) | Transient (closed after merge) |\n| **Scope** | Feature-level rules | Task-level instructions |\n| **Audience** | Architects, Agents (Reference) | Agents, Developers (Execution) |\n\nThe Spec defines the **current state** of the system:\n- \"All notifications must deliver within 100ms\"\n- \"API must handle 1000 req/sec\"\n\nThe PBI defines the **change**:\n- \"Add SMS fallback to notification system\"\n- \"Optimize database query for search endpoint\"\n\nThe PBI *references* the Spec for context and *updates* the Spec when it changes contracts.\n\n### Why Separation Matters\n\n```\nSprint 1: PBI-101 \"Build notification system\"\n  → Creates /plans/notifications/spec.md\n  → Spec defines: \"Deliver within 100ms via WebSocket\"\n\nSprint 3: PBI-203 \"Add SMS fallback\"\n  → Updates spec.md with new transport rules\n  → PBI-203 is closed, but the spec persists\n\nSprint 8: PBI-420 \"Refactor notification queue\"\n  → Agent reads spec.md, sees all rules still apply\n  → Refactoring preserves all documented contracts\n```\n\nWithout this separation, the agent in Sprint 8 has no visibility into decisions made in Sprint 1.\n\n## The Assembly Model\n\nSpecs serve as the context source for Feature Assembly. Multiple PBIs reference the same spec, and the spec's contracts are verified at quality gates.\n\n```mermaid\nflowchart LR\n  A[/spec.md/]\n\n  B[\\pbi-101.md\\]\n  C[\\pbi-203.md\\]\n  D[\\pbi-420.md\\]\n\n  B1[[FEATURE ASSEMBLY]]\n  C1[[FEATURE ASSEMBLY]]\n  D1[[FEATURE ASSEMBLY]]\n\n  E{GATE}\n\n  F[[MIGRATION]]\n\n  A --> B\n  A --> C\n  A --> D\n\n  B --> B1\n  C --> C1\n  D --> D1\n\n  B1 --> E\n  C1 --> E\n  D1 --> E\n\n  A --> |Context|E\n\n  E --> F\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/the-spec-fig-1.svg\" alt=\"Mermaid Diagram\" />\n  \n</figure>\n\n## Anatomy\n\nEvery spec consists of two parts:\n\n### Blueprint (Design)\nDefines **implementation constraints** that prevent agents from hallucinating invalid architectures.\n\n- **Context** — Why does this feature exist?\n- **Architecture** — API contracts, schemas, dependency directions\n- **Anti-Patterns** — What agents must NOT do\n\n### Contract (Quality)\nDefines **verification rules** that exist independently of any specific task.\n\n- **Definition of Done** — Observable success criteria\n- **Regression Guardrails** — Invariants that must never break\n- **Scenarios** — Gherkin-style journeys for E2E tests\n\nFor detailed structure, examples, and templates, see the [Living Specs Practice Guide](/practices/living-specs).\n\n## Relationship to Other Patterns\n\n**[The PBI](/patterns/the-pbi)** — PBIs are the transient execution units (Delta) that reference specs for context. When a PBI changes contracts, it updates the spec in the same commit.\n\n**[Feature Assembly](/practices/feature-assembly)** — Specs define the acceptance criteria verified during assembly. The diagram above shows this flow.\n\n**[Experience Modeling](/patterns/experience-modeling)** — Experience models capture user journeys; specs capture the technical contracts that implement those journeys.\n\n**[Context Engineering](/concepts/context-engineering)** — Specs are structured context assets optimized for agent consumption, with predictable sections (Blueprint, Contract) for efficient extraction.\n\n## References\n\n- [Living Specs Practice Guide](/practices/living-specs) — Implementation instructions, templates, and best practices\n- [Living Documentation (Martin Fowler)](https://martinfowler.com/bliki/LivingDocumentation.html)",
    "tags": [
      "Documentation",
      "Living Documentation",
      "Spec-Driven Development",
      "Context Engineering"
    ]
  },
  {
    "slug": "agent-personas",
    "collection": "practices",
    "title": "Agent Personas",
    "description": "A guide on how to add multiple personas to an AGENTS.md file, with examples.",
    "status": "Live",
    "content": "## Overview\n\nDefining clear personas for your agents is crucial for ensuring they understand their role, trigger constraints, and goals. This guide demonstrates how to structure multiple personas within your `AGENTS.md` file.\n\nPersonas are a context engineering practice—they scope agent work by defining boundaries and focus, not by role-playing. When combined with [Model Routing](/patterns/model-routing), personas can also specify which computational tool (LLM) to use for each type of work.\n\nFor the full specification of the `AGENTS.md` file, see the [AGENTS.md Specification](./agents-md-spec).\n\n## How to Add Multiple Personas\n\nYou can define multiple personas by specifying triggers, goals, and guidelines for each. This allows different agents (or the same agent in different contexts) to adopt specific behaviors suited for the task at hand.\n\n### Example: Our Internal Personas\n\nBelow are the personas we use, serving as a template for your own `AGENTS.md`.\n\n```markdown\n### 1.1. Lead Developer / Astro Architect (@Lead)\n**Trigger:** When asked about system design, specs, or planning.\n* **Goal**: Specify feature requirements, architecture, and required changes. Analyze the project state and plan next steps.\n* **Guidelines**\n  - **Schema Design:** When creating new content types, immediately define the Zod schema in `src/content/config.ts`.\n  - **Routing:** Use Astro's file-based routing. For dynamic docs, use `[...slug].astro` and `getStaticPaths()`.\n  - **SEO:** Ensure canonical URLs and Open Graph tags are generated for every new page.\n  - **Dev Performace:** Focus on tangible, deliverable outcomes.\n  - **Spec driven development:** Always produce clear, concise specifications before handing off to implementation agents.\n  - **Planned iterations:** Break down large tasks into manageable PBIs with clear acceptance criteria.\n\n### 1.2. Designer / User Experience Lead (@Designer)\n**Trigger:** When asked about Design system UI/UX, design systems, or visual consistency.\n* **Goal**: Ensure the design system can be effectively utilized by agents and humans alike.\n* **Guidelines**\n  - **Design Tokens:** Tokens must be set in `src/styles/tokens.css`. No hardcoded colors or fonts.\n  - **Component Consistency:** All components must adhere to the design system documented in `src/pages/resources/design-system.astro`. \n  - **Accessibility:** Ensure all components meet WCAG 2.1 AA standards.\n  - **Documentation:** Update the Design System page with any new components or styles introduced.\n  - **Experience Modeling Allowed:** Design system components are protected by a commit rule: use \\[EM] tag to override the rule.\n  \n### 1.3. Content Engineer / Technical Writer (@Content)\n**Trigger:** When asked to create or update documentation, articles, or knowledge base entries.\n* **Goal**: Produce high-quality, structured content that adheres to the project's schema and style guidelines.\n* **Guidelines**\n  - **Content Structure:** Follow the established folder structure in `src/content/` for concepts\n  \n### 1.4. Developer / Implementation Agent (@Dev)\n**Trigger:** When assigned implementation tasks or bug fixes.\n* **Goal**: Implement features, fix bugs, and ensure the codebase remains healthy and maintainable.\n* **Guidelines**\n  - **Expect PBIs:** Always work from a defined Product Backlog Item (PBI) with clear acceptance criteria, if available.\n  - **Type Safety:** Use TypeScript strictly. No `any` types allowed.\n  - **Component Imports:** Explicitly import all components used in `.astro` files.\n  - **Testing:** Ensure all changes pass `pnpm check` and `pnpm lint`\n  - **Document progress:** Update the relevant PBI in `docs/backlog/` with status and notes.md after completing tasks.\n```\n\n## Model Routing and Personas\n\nPersonas define **what work to do** and **how to scope it**. [Model Routing](/patterns/model-routing) is a separate practice that defines **which computational tool to use**.\n\n### Current State (December 2025)\n\nAI-assisted IDEs (Cursor, Windsurf, Claude Code) do **not** automatically select models based on persona definitions. Model selection is manual.\n\n### Best Practice: Keep Them Separate\n\n**Don't add model profiles to `AGENTS.md`** - It adds noise to the context window without providing automation value.\n\nInstead:\n1. **Keep personas focused** on triggers, goals, and guidelines\n2. **Use Model Routing separately** - Manually select models based on the task characteristics\n3. **Reference the pattern** when deciding which model to use\n\n### Matching Personas to Model Profiles\n\nWhen you invoke a persona, choose your model based on the work type:\n\n| Persona Type | Typical Work | Recommended Profile |\n|---|---|---|\n| Lead / Architect | System design, specs, architectural decisions | High Reasoning |\n| Developer / Implementation | Code generation, refactoring, tests | High Throughput |\n| Documentation Analyst | Legacy code analysis, comprehensive docs | Massive Context |\n\nThe workflow:\n1. **Identify the persona** needed for your task\n2. **Select the appropriate model** manually in your IDE\n3. **Invoke the persona** with your prompt\n\nThis keeps `AGENTS.md` lean and focused on scoping agent work, while model selection remains a deliberate engineering decision.",
    "tags": ["agents", "personas", "guide"]
  },
  {
    "slug": "agents-md-spec",
    "collection": "practices",
    "title": "AGENTS.md Specification",
    "description": "The definitive guide to the AGENTS.md file, including philosophy, anatomy, and implementation strategy.",
    "status": "Live",
    "content": "## DEFINITION\n\n`AGENTS.md` is an open format for guiding coding agents, acting as a \"README for agents.\" It provides a dedicated, predictable place for context and instructions—such as build steps, tests, and conventions—that help AI coding agents work effectively on a project.\n\nWe align with the [agents.md specification](https://agents.md), treating this file as the authoritative source of truth for agentic behavior within the ASDLC.\n\n## CORE PHILOSOPHY\n\n**1. A README for Agents**\n\nJust as `README.md` is for humans, `AGENTS.md` is for agents. It complements existing documentation by containing the detailed context—build commands, strict style guides, and test instructions—that agents need but might clutter a human-facing README.\n\n**2. Context is Code**\n\nIn the ASDLC, we treat `AGENTS.md` with the same rigor as production software:\n\n- **Version Controlled**: Tracked via git and PRs.\n- **Falsifiable**: Contains clear success criteria for agent actions.\n- **Optimized**: Structured to maximize signal-to-noise ratio for LLM context windows, preventing \"Lost in the Middle\" issues.\n\n## Format Philosophy\n\nThe structures in this specification (YAML maps, XML standards, tiered boundaries) are optimized for large teams and complex codebases. For smaller projects:\n\n- A simple markdown list may suffice\n- Focus on the *concepts* (persona, boundaries, commands) rather than exact syntax\n- Iterate on what produces best adherence from your specific model\n\nThe goal is signal density, not format compliance. Overly rigid specs create adoption friction. Let teams scale complexity to their needs.\n\n## TOOL-SPECIFIC CONSIDERATIONS\n\nDifferent AI coding tools look for different filenames. While `AGENTS.md` is the emerging standard, some tools require specific naming:\n\n| Tool | Expected Filename | Notes |\n| :--- | :--- | :--- |\n| **Cursor** | `.cursorrules` | Also reads `AGENTS.md` |\n| **Windsurf** | `.windsurfrules` | Also reads `AGENTS.md` |\n| **Claude Code** | `CLAUDE.md` | Does not read `AGENTS.md`; case-sensitive |\n| **Codex** | `AGENTS.md` | Native support |\n| **Zed** | `.rules` | Priority-based; reads `AGENTS.md` at lower priority |\n| **VS Code / Copilot** | `AGENTS.md` | Requires `chat.useAgentsMdFile` setting enabled |\n\n### Zed Priority Order\n\nZed uses the first matching file from this list:\n1. `.rules`\n2. `.cursorrules`\n3. `.windsurfrules`\n4. `.clinerules`\n5. `.github/copilot-instructions.md`\n6. `AGENT.md`\n7. `AGENTS.md`\n8. `CLAUDE.md`\n9. `GEMINI.md`\n\n### VS Code Configuration\n\nVS Code requires explicit opt-in for `AGENTS.md` support:\n- Enable `chat.useAgentsMdFile` setting to use `AGENTS.md`\n- Enable `chat.useNestedAgentsMdFiles` for subfolder-specific instructions\n\n### Recommendation\n\nCreate a symlink to support Claude Code without duplicating content:\n\n```bash\nln -s AGENTS.md CLAUDE.md\n```\n\nThis ensures Claude Code users get the same guidance while maintaining a single source of truth. Note that Claude Code also supports `CLAUDE.local.md` for personal preferences that shouldn't be version-controlled.\n\n## ASDLC IMPLEMENTATION STRATEGY\n\nWhile the [agents.md](https://agents.md) standard provides the format, the ASDLC recommends a structured implementation to ensure reliability. We present our `AGENTS.md` format not just as a list of tips, but as a segmented database of rules. This is *one* valid implementation strategy, particularly suited for rigorous engineering environments.\n\n### 1. Identity Anchoring (The Persona)\n\nEstablishes the specific expertise required to prune the model's search space. Without this, the model reverts to the \"average\" developer found in its training data. For detailed examples of defining multiple personas, see [Agent Personas](./agent-personas).\n\nBad: \"You are a coding assistant.\"\n\nGood: \"You are a Principal Systems Engineer specializing in Go 1.22, gRPC, and high-throughput concurrency patterns. You favor composition over inheritance.\"\n\n### 2. Contextual Alignment (The Mission)\n\nA concise, high-level summary of the project’s purpose and business domain. This is often formatted as a blockquote at the top of the file to \"set the stage\" for the agent's session.\n\n- **Why:** LLMs are stateless. A 50-token description differentiates a \"User\" in a banking app (high security/compliance) from a \"User\" in a casual game (low friction), reducing the need for corrective follow-up prompts.\n    \n- **Format:** Focus on the \"What\" and \"Why,\" not the narrative history.\n    \n\n**Example:**\n\n> **Project:** \"ZenTask\" - A minimalist productivity app. **Core Philosophy:** Local-first data architecture; offline support is mandatory.\n\n### 3. Operational Grounding (The Tech Stack)\n\nExplicitly defines the software environment to prevent \"Library Hallucination.\" This section must be exhaustive regarding key dependencies and restrictive regarding alternatives.\n\n- Directive: \"Runtime: Node.js v20 (LTS) exclusively.\"\n- Directive: \"Styling: Tailwind CSS only. Do not use CSS Modules or Emotion.\"\n- Directive: \"State: Zustand only. Do not use Redux.\"\n\n### 4. Behavioral Boundaries (Context Gates)\n\nReplaces vague \"Guardrails\" with a \"Three-Tiered Boundary\" system, or _constitution_. As the models are probabilistic, absolute prohibitions are unrealistic. Instead, this system categorizes rules by severity and required action. These rules are aimed to reducing the likelihood of critical errors. Note that you should always complement\nthe _constitution_ with explicit and deterministic _quality gates_ enforced by tests, linters, and CI/CD pipelines.\n\n**Tier 1 (Constitutive - ALWAYS): Non-negotiable standards.**\n\nExample: \"Always add JSDoc to exported functions.\"\n\n**Tier 2 (Procedural - ASK): High-risk operations requiring Human-in-the-Loop.**\n\nExample: \"Ask before running database migrations or deleting files.\"\n\n**Tier 3 (Hard Constraints - NEVER): Safety limits.**\n\nExample: \"Never commit secrets, API keys, or .env files.\"\n\n### 5. Semantic Directory Mapping\n\nWhen documenting the codebase structure in AGENTS.md, prefer Annotated YAML over ASCII trees.\n\n- **Use Valid Syntax:** Ensure the block allows an LLM to parse the structure as a dictionary.\n- **Annotate Key Files:** do not just list files; map them to a brief string describing their responsibility. This acts as a 'map legend' for the Agent, allowing it to route coding tasks to the correct file without needing to scan the file content first.\n- **Omit Noise:** Only include directories and files relevant to the Agent's operation or the architectural scope.\n\n**Example:**\n\n```yaml\ndirectory_map:\n  src:\n    # Core Application Logic\n    main.py: \"Application entry point; initializes the Agent Orchestrator\"\n    \n    agents:\n      # Individual Agent definitions\n      base_agent.py: \"Abstract base class defining the 'step()' and 'memory' interfaces\"\n    \n    utils:\n      # Shared libraries\n      llm_client.py: \"Wrapper for OpenAI/Anthropic APIs with retry logic\"\n```\n\n### 6. The Command Registry\n\nA lookup table mapping intent to execution. Agents often default to standard commands (npm test) which may fail in custom environments (make test-unit). The Registry forces specific tool usage.\n\n| Intent | Command | Notes |\n| :--- | :--- | :--- |\n| **Build** | pnpm build | Outputs to `dist/` |\n| **Test** | pnpm test:unit | Flags: --watch=false |\n| **Lint** | pnpm lint --fix | Self-correction enabled |\n\n\n### 7. Implementation notes\n\nXML-Tagging for Semantic Parsing\n\nTo maximize adherence, use pseudo-XML tags to encapsulate rules. This creates a \"schema\" that the model can parse more strictly than bullet points.\n\n```xml\n<coding_standard name=\"React Hooks\">\n  <instruction>\n    Use functional components and Hooks. Avoid Class components.\n    Ensure extensive use of custom hooks for logic reuse.\n  </instruction>\n  <anti_pattern>\n    class MyComponent extends React.Component {... }\n  </anti_pattern>\n  <preferred_pattern>\n    const MyComponent = () => {... }\n  </preferred_pattern>\n</coding_standard>\n```\n\n## REFERENCE TEMPLATE\n\n`Filename: AGENTS.md`\n\n```md\n# AGENTS.md - Context & Rules for AI Agents\n\n> **Project Mission:** High-throughput gRPC service for processing real-time financial transactions.\n> **Core Constraints:** Zero-trust security model, ACID compliance required for all writes.\n\n## 1. Identity & Persona\n- **Role:** Senior Systems Engineer\n- **Specialization:** High-throughput distributed systems in Go.\n- **Objective:** Write performant, thread-safe, and maintainable code.\n\n## 2. Tech Stack (Ground Truth)\n- **Language:** Go 1.22 (Use `iter` package for loops)\n- **Transport:** gRPC (Protobuf v3)\n- **Database:** PostgreSQL 15 with `pgx` driver (No ORM allowed)\n- **Infra:** Kubernetes, Helm, Docker\n\n## 3. Operational Boundaries (CRITICAL)\n- **NEVER** commit secrets, tokens, or `.env` files.\n- **NEVER** modify `api/proto` without running `buf generate`.\n- **ALWAYS** handle errors; never use `_` to ignore errors.\n- **ASK** before adding external dependencies.\n\n## 4. Command Registry\n| Action | Command | Note |\n| :--- | :--- | :--- |\n| **Build** | `make build` | Outputs to `./bin` |\n| **Test** | `make test` | Runs with `-race` detector |\n| **Lint** | `golangci-lint run` | Must pass before commit |\n| **Gen** | `make proto` | Regenerates gRPC stubs |\n\n## 5. Development Map\n```yaml\ndirectory_map:\n  api:\n    proto: \"Protocol Buffers definitions (Source of Truth)\"\n  cmd:\n    server: \"Main entry point, dependency injection wire-up\"\n  internal:\n    biz: \"Business logic and domain entities (Pure Go)\"\n    data: \"Data access layer (Postgres + pgx)\"\n```\n\n## 6. Coding Standards\n```xml\n<rule_set name=\"Concurrency\">\n  <instruction>Use `errgroup` for managing goroutines. Avoid bare `go` routines.</instruction>\n  <example>\n    <bad>go func() {... }()</bad>\n    <good>g.Go(func() error {... })</good>\n  </example>\n</rule_set>\n```\n\n## 7. Context References\n- **Database Schema:** Read `@database/schema.sql`\n- **API Contracts:** Read `@api/v1/service.proto`\n```",
    "tags": ["governance", "agents", "specification"]
  },
  {
    "slug": "living-specs",
    "collection": "practices",
    "title": "Living Specs",
    "description": "Practical guide to creating and maintaining specs that evolve alongside your codebase.",
    "status": "Experimental",
    "content": "## Overview\n\nThis guide provides practical instructions for implementing the [Specs](/patterns/the-spec) pattern. While the pattern describes *what* specs are and *why* they matter, this guide focuses on *how* to create and maintain them.\n\n## When to Create a Spec\n\nCreate a spec when starting work that involves:\n\n**Feature Domains** — New functionality that introduces architectural patterns, API contracts, or data models that other parts of the system depend on.\n\n**User-Facing Workflows** — Features with defined user journeys and acceptance criteria that need preservation for future reference.\n\n**Cross-Team Dependencies** — Any feature that other teams will integrate with, requiring clear contract definitions.\n\n**Don't create specs for:** Simple bug fixes, trivial UI changes, configuration updates, or dependency bumps.\n\n## Spec granularity\n\nA spec should be detailed enough to serve as a contract for the feature, but not so detailed that it becomes a maintenance burden.\n\nSome spec features, like gherkin scenarios, are not always necessary if the feature is simple or well-understood. \n\n## When to Update a Spec\n\nUpdate an existing spec when:\n\n- API contracts change (new endpoints, modified payloads, deprecated routes)\n- Data schemas evolve (migrations, new fields, constraint changes)\n- Quality targets shift (performance, security, accessibility requirements)\n- Anti-patterns are discovered (during review or post-mortems)\n- Architecture decisions are made (any ADR should update relevant specs)\n\n**Golden Rule:** If code behavior changes, the spec MUST be updated in the same commit.\n\n## File Structure\n\nOrganize specs by **feature domain**, not by sprint or ticket number.\n\n```\n/project-root\n├── ARCHITECTURE.md           # Global system rules\n├── plans/                    # Feature-level specs\n│   ├── user-authentication/\n│   │   └── spec.md\n│   ├── payment-processing/\n│   │   └── spec.md\n│   └── notifications/\n│       └── spec.md\n└── src/                      # Implementation code\n```\n\n**Conventions:**\n- Directory name: kebab-case, matches the feature's conceptual name\n- File name: always `spec.md`\n- Location: `/plans/{feature-domain}/spec.md`\n- Scope: one spec per independently evolvable feature\n\n## Maintenance Protocol\n\n### Same-Commit Rule\n\nIf code changes behavior, update the spec in the same commit. Add \"Spec updated\" to your PR checklist.\n\n```\ngit commit -m \"feat(notifications): add SMS fallback\n\n- Implements SMS delivery when WebSocket fails\n- Updates /plans/notifications/spec.md with new transport layer\"\n```\n\n### Deprecation Over Deletion\n\nMark outdated sections as deprecated rather than removing them. This preserves historical context.\n\n```markdown\n### Architecture\n\n**[DEPRECATED 2024-12-01]**\n~~WebSocket transport via Socket.io library~~\nReplaced by native WebSocket API to reduce bundle size.\n\n**Current:**\nNative WebSocket connection via `/api/ws/notifications`\n```\n\n### Bidirectional Linking\n\nLink code to specs and specs to code:\n\n```typescript\n// Notification delivery must meet 100ms latency requirement\n// See: /plans/notifications/spec.md#contract\n```\n\n```markdown\n### Data Schema\nImplemented in `src/types/Notification.ts` using Zod validation.\n```\n\n## Template\n\n```markdown\n# Feature: [Feature Name]\n\n## Blueprint\n\n### Context\n[Why does this feature exist? What business problem does it solve?]\n\n### Architecture\n- **API Contracts:** `[METHOD] /api/v1/[endpoint]` - [Description]\n- **Data Models:** See `[file path]`\n- **Dependencies:** [What this depends on / what depends on this]\n\n### Anti-Patterns\n- [What agents must avoid, with rationale]\n\n## Contract\n\n### Definition of Done\n- [ ] [Observable success criterion]\n\n### Regression Guardrails\n- [Critical invariant that must never break]\n\n### Scenarios\n**Scenario: [Name]**\n- Given: [Precondition]\n- When: [Action]\n- Then: [Expected outcome]\n```\n\n## Anti-Patterns\n\n### The Stale Spec\n**Problem:** Spec created during planning, never updated as the feature evolves.\n\n**Solution:** Make spec updates mandatory in Definition of Done. Add PR checklist item.\n\n### The Spec in Slack\n**Problem:** Design decisions discussed in chat but never committed to the repo.\n\n**Solution:** After consensus, immediately update `spec.md` with a commit linking to the discussion.\n\n### The Monolithic Spec\n**Problem:** A single 5000-line spec tries to document the entire application.\n\n**Solution:** Split into feature-domain specs. Use `ARCHITECTURE.md` only for global cross-cutting concerns.\n\n### The Spec-as-Tutorial\n**Problem:** Spec reads like a beginner's guide, full of basic programming concepts.\n\n**Solution:** Assume engineering competence. Document constraints and decisions, not general knowledge.\n\n### The Copy-Paste Code\n**Problem:** Spec duplicates large chunks of implementation code.\n\n**Solution:** Reference canonical sources with file paths. Only include minimal examples to illustrate patterns.\n\n## References\n\n- [Specs Pattern](/patterns/the-spec) — Conceptual foundation\n- [The PBI](/patterns/the-pbi) — Execution units that reference specs\n- [Living Documentation (Martin Fowler)](https://martinfowler.com/bliki/LivingDocumentation.html)",
    "tags": ["Documentation", "Spec-Driven Development", "Living Documentation"]
  },
  {
    "slug": "micro-commits",
    "collection": "practices",
    "title": "Micro-Commits",
    "description": "Ultra-granular commit practice for agentic workflows, treating version control as reversible save points.",
    "status": "Live",
    "content": "## Definition\n\n**Micro-Commits** is the practice of committing code changes at much higher frequency than traditional development workflows. Each discrete task—often a single function, test, or file—receives its own commit.\n\nWhen working with LLM-generated code, commits become \"save points in a game\": checkpoints that enable instant rollback when probabilistic outputs introduce bugs or architectural drift.\n\n## The Problem: Coarse-Grained Commits in Agentic Workflows\n\nTraditional commit practices optimize for human readability and PR review: \"logical units of work\" that span multiple files and implement complete features.\n\nThis fails in agentic workflows because:\n\n**LLM outputs are probabilistic** — A model might generate correct code for 3 files and introduce subtle bugs in the 4th. Bundling all 4 files into one commit makes rollback destructive.\n\n**Regression to mediocrity** — Without checkpoints, it's difficult to identify where LLM output drifted from the [Spec](/patterns/the-spec) contracts.\n\n**Context loss** — Large commits obscure the sequence of decisions. When debugging, you need to know \"what changed, when, and why.\"\n\n**No emergency exit** — If an LLM generates a tangled mess across 10 files, your only option is manual surgery or discarding hours of work.\n\n## The Solution: Commit After Every Task\n\nMake a commit immediately after:\n- Completing a [PBI](/patterns/the-pbi) subtask\n- Generating a single function or module\n- Making a file pass linting/compilation\n- Adding one test\n- Any LLM-assisted edit that produces working code\n\nThis creates a breadcrumb trail of working states.\n\n## The Practice\n\n### 4.1. Atomic Tasks → Atomic Commits\n\nBreak work into small, testable chunks. Each chunk maps to one commit.\n\n**Example PBI:** \"Add OAuth login flow\"\n\n**Commit sequence:**\n```\n1. feat: add OAuth config schema\n2. feat: implement token exchange endpoint\n3. feat: add session storage for OAuth tokens\n4. test: add OAuth flow integration test\n5. refactor: extract OAuth error handling\n```\n\nThis aligns with atomic [PBIs](/patterns/the-pbi): small, bounded execution units.\n\n### 4.2. Commit Messages as Execution Log\n\nCommit messages document the sequence of LLM-assisted changes. They serve as:\n- **Context for debugging** — \"The bug appeared after commit 7.\"\n- **Briefing material for AI** — Feed recent commits to an LLM to explain current state.\n- **Audit trail** — Track architectural decisions embedded in code changes.\n\n**Format:**\n```\ntype(scope): brief description\n\n- Detail 1\n- Detail 2\n```\n\n**Example:**\n```\nfeat(auth): implement OAuth token validation\n\n- Add JWT verification middleware\n- Extract claims from token payload\n- Return 401 on expired tokens\n```\n\n### 4.3. Branches and Worktrees for Isolation\n\nUse branches or git worktrees to isolate LLM experiments:\n\n**Branches** — Separate experimental work from stable code. Merge only after validation.\n\n**Worktrees** — Run parallel LLM sessions on the same repository without context conflicts. Each worktree is an independent working directory.\n\n**Example workflow:**\n```bash\n# Create worktree for LLM experiment\ngit worktree add ../project-experiment experiment-oauth\n\n# Work in worktree, commit frequently\ncd ../project-experiment\n# ... LLM generates code ...\ngit commit -m \"feat: add OAuth callback handler\"\n\n# If successful, merge into main\ngit checkout main\ngit merge experiment-oauth\n\n# If failed, discard worktree\ngit worktree remove ../project-experiment\n```\n\nThis prevents contaminating the main branch with failed LLM output.\n\n### 4.4. Rollback as First-Class Operation\n\nWhen LLM output introduces bugs:\n\n**Identify the bad commit** — Review recent history to find where issues appeared.\n\n**Rollback to last known good state:**\n```bash\n# Soft reset (keeps changes as uncommitted)\ngit reset --soft HEAD~1\n\n# Hard reset (discards changes entirely)\ngit reset --hard HEAD~1\n```\n\n**Selective revert:**\n```bash\n# Revert specific commit without losing subsequent work\ngit revert <commit-hash>\n```\n\nThis is only safe because micro-commits isolate changes.\n\n### 5. Tidy History for Comprehension\n\nGranular commits create noisy history. Before merging to main, optionally squash related commits into logical units:\n\n```bash\n# Interactive rebase to squash last 5 commits\ngit rebase -i HEAD~5\n```\n\nThis preserves detailed history during development while creating clean history for long-term maintenance.\n\n**Trade-off:** Squashing removes granular rollback points. Only squash after validation passes [Quality Gates](/concepts/context-gates).\n\n## Relationship to The PBI\n\n[PBIs](/patterns/the-pbi) define **what to build**. Micro-Commits define **how to track progress**.\n\n**Atomic PBIs** (small, bounded tasks) naturally produce micro-commits. Each PBI generates 1-5 commits depending on complexity.\n\n**Example mapping:**\n- **PBI:** \"Implement retry logic with exponential backoff\"\n- **Commits:**\n  1. `feat: add retry wrapper function`\n  2. `feat: implement exponential backoff calculation`\n  3. `test: add retry logic unit tests`\n  4. `docs: update retry behavior in spec`\n\nThis makes PBI progress traceable and reversible.\n\n## References\n\n### Framework Patterns\n- [The PBI](/patterns/the-pbi) — Atomic execution units that map to commit sequences\n- [Context Gates](/concepts/context-gates) — Validation checkpoints that rely on granular commits\n- [Agentic SDLC](/concepts/agentic-sdlc) — The cybernetic loop where micro-commits enable rapid iteration\n\n### External Resources\n- [My LLM Coding Workflow Going into 2026](https://addyo.substack.com/p/my-llm-coding-workflow-going-into) — Addy Osmani's practical guide emphasizing commits as \"save points in a game\"",
    "tags": ["Version Control", "Git", "Safety", "Rollback"]
  },
  {
    "slug": "pbi-authoring",
    "collection": "practices",
    "title": "PBI Authoring",
    "description": "How to write Product Backlog Items that agents can read, execute, and verify—with templates and lifecycle guidance.",
    "status": "Live",
    "content": "## Definition\n\n**PBI Authoring** is the practice of writing Product Backlog Items optimized for agent execution. This includes structuring the four-part anatomy, ensuring machine accessibility, and managing the PBI lifecycle from planning through closure.\n\nFollowing this practice produces PBIs that agents can programmatically access, unambiguously interpret, and verifiably complete.\n\n## When to Use\n\n**Use this practice when:**\n- Creating work items for agent execution\n- Planning a sprint with AI-assisted development\n- Converting legacy user stories to agent-ready format\n- Setting up a new project's backlog structure\n\n**Skip this practice when:**\n- Work is purely exploratory with no defined outcome\n- The task is a one-off command (use direct prompting instead)\n- Human-only execution with no agent involvement\n\n## Process\n\n### Step 1: Ensure Accessibility\n\n**Invisibility is a bug.** If an agent cannot read the PBI, the workflow is broken.\n\nA PBI locked inside a web UI without API or MCP integration is useless to an AI developer. The agent must programmatically access the work item without requiring human copy-paste.\n\n**Valid access methods:**\n\n| Method | Description |\n|--------|-------------|\n| **MCP Integration** | Agent connected to Issue Tracker (Linear, Jira, GitHub) via MCP |\n| **Repo-Based** | PBI exists as a markdown file (e.g., `tasks/PBI-123.md`) |\n| **API Access** | Tracker exposes REST/GraphQL API the agent can query |\n\n**If your tracker has no API access:** Mirror PBIs as markdown files during sprint planning, or implement MCP integration.\n\n### Step 2: Write the Directive\n\nState what to do with explicit scope boundaries. Be imperative, not conversational.\n\n**Good:**\n```\nImplement the API Layer for user notification preferences.\nScope: Only touch the `src/api/notifications` folder.\n```\n\n**Bad:**\n```\nAs a user, I want to manage my notification preferences so that I can control what emails I receive.\n```\n\nThe second example requires interpretation. The first is executable.\n\n### Step 3: Add Context Pointers\n\nReference the permanent spec—don't copy design decisions into the PBI.\n\n```\nReference: `plans/notifications/spec.md` Part A for the schema.\nSee the \"Architecture\" section for endpoint contracts.\n```\n\n**Why pointers, not copies:** Specs evolve. A copied schema in a PBI becomes stale the moment the spec updates. Pointers ensure the agent always reads the authoritative source.\n\n### Step 4: Define Verification Criteria\n\nLink to success criteria in the spec, or define inline checkboxes.\n\n```\nMust pass \"Scenario 3: Preference Update\" defined in \n`plans/notifications/spec.md` Part B (Contract).\n```\n\nOr inline:\n```\n- [ ] POST /preferences returns 201 on valid input\n- [ ] Invalid payload returns 400 with error schema\n- [ ] Unit test coverage > 80%\n```\n\n### Step 5: Declare Dependencies\n\nExplicitly state what blocks this PBI and what it blocks.\n\n```\n## Dependencies\n- Blocked by: PBI-101 (creates the base schema)\n- Must merge before: PBI-103 (extends this endpoint)\n```\n\n**Anti-Pattern:** Implicit dependencies discovered at merge time. Identify during planning; either sequence the work or refactor into independent units.\n\n### Step 6: Set the Refinement Rule\n\nDefine what happens when reality diverges from the spec.\n\n```\nIf implementation requires changing the Architecture, you MUST \nupdate `spec.md` in the same PR with a changelog entry.\n```\n\nOptions to specify:\n- **Update spec in same PR** — Agent has authority to evolve the design\n- **Flag for human review** — Agent stops and requests guidance\n- **Proceed with deviation log** — Agent continues but documents the gap\n\n## Template\n\n```markdown\n# PBI-XXX: [Brief Imperative Title]\n\n## Directive\n[What to build/change in 1-2 sentences]\n\n**Scope:**\n- [Explicit file/folder boundaries]\n- [What NOT to touch]\n\n## Dependencies\n- Blocked by: [PBI-YYY if any, or \"None\"]\n- Must merge before: [PBI-ZZZ if any, or \"None\"]\n\n## Context\nRead: `[path/to/spec.md]`\n- [Specific section to reference]\n\n## Verification\n- [ ] [Criterion 1: Functional requirement]\n- [ ] [Criterion 2: Performance/quality requirement]\n- [ ] [Criterion 3: Test coverage requirement]\n\n## Refinement Protocol\n[What to do if the spec needs updating during implementation]\n```\n\n## PBI Lifecycle\n\n| Phase | Actor | Action |\n|-------|-------|--------|\n| **Planning** | Human | Creates PBI with 4-part structure |\n| **Assignment** | Human/System | PBI assigned to Agent or Developer |\n| **Execution** | Agent | Reads Spec, implements Delta |\n| **Review** | Human | Verifies against Spec's Contract section |\n| **Merge** | Human/System | Code merged, Spec updated if needed |\n| **Closure** | System | PBI archived, linked to commit/PR |\n\n## Common Mistakes\n\n### The User Story Hangover\n\n**Problem:** PBI written as \"As a user, I want...\" with no explicit scope or verification.\n\n**Solution:** Rewrite in imperative form with scope boundaries and checkable criteria.\n\n### The Spec Copy\n\n**Problem:** PBI contains copied design decisions that drift from the spec.\n\n**Solution:** Use pointers to spec sections, never copy content that lives elsewhere.\n\n### The Hidden Dependency\n\n**Problem:** Two PBIs touch the same files; discovered at merge time.\n\n**Solution:** During planning, map file ownership. If overlap exists, sequence the PBIs or refactor scope.\n\n### The Untestable Increment\n\n**Problem:** PBI verification requires another PBI to complete first.\n\n**Solution:** Ensure each PBI is self-testable. If not possible, merge into a single PBI or create test fixtures.\n\n## Related Patterns\n\nThis practice implements:\n\n- **[The PBI](/patterns/the-pbi)** — The structural pattern this practice executes\n\nSee also:\n\n- **[The Spec](/patterns/the-spec)** — The permanent context PBIs reference\n- **[Living Specs](/practices/living-specs)** — How to maintain the specs PBIs point to",
    "tags": ["Agile", "Product Backlog Item", "Workflow", "Agent Execution"]
  }
]
