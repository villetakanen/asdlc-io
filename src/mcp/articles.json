[
  {
    "slug": "4d-framework",
    "collection": "concepts",
    "title": "The 4D Framework (Anthropic)",
    "description": "A cognitive model codifying four essential competencies—Delegation, Description, Discernment, and Diligence—for effective generative AI use.",
    "status": "Live",
    "content": "## Definition\n\nThe **4D Framework** is a cognitive model for human-AI collaboration developed by [Anthropic](https://www.anthropic.com/research) in partnership with Dr. Joseph Feller and Rick Dakan as part of the *AI Fluency* curriculum.\n\nThe framework codifies four essential competencies for leveraging generative AI effectively and responsibly:\n\n1. **Delegation** — The Strategy\n2. **Description** — The Prompt\n3. **Discernment** — The Review\n4. **Diligence** — The Liability\n\nUnlike process models (e.g., Agile or Double Diamond) that dictate workflow timing, the 4D Framework specifies *how* to interact with AI systems. It positions the human not merely as a \"prompter,\" but as an **Editor-in-Chief**, accountable for strategic direction and risk management.\n\n## The Four Dimensions\n\n### Delegation (The Strategy)\n\nBefore engaging with the tool, the human operator must determine what, if anything, should be assigned to the AI. This is a strategic decision between **Automation** (offloading repetitive tasks) and **Augmentation** (leveraging AI as a thought partner).\n\n**Core Question:** \"Is this task 'boilerplate' with well-defined rules (High Delegation), or does it demand nuanced judgment, deep context, or ethical considerations (Low Delegation)?\"\n\n### Description (The Prompt)\n\nAI output quality is directly proportional to input quality. \"Description\" transcends prompt engineering hacks by emphasizing **Context Transfer**—delivering explicit goals, constraints, and data structures required for the task.\n\n**Core Question:** \"Have I specified the constraints, interface definitions, and success criteria needed for this task?\"\n\n### Discernment (The Review)\n\nThis marks the transition from **Creator** to **Editor**. The human must rigorously assess AI output for accuracy, hallucinations, bias, and overall quality. Failing to apply discernment is a leading cause of \"AI Technical Debt.\"\n\n**Core Question:** \"If I authored this output, would it meet code review standards? Does it introduce fictitious libraries or violate design tokens?\"\n\n### Diligence (The Liability)\n\nThe human user retains full accountability for outcomes. Diligence acknowledges that while AI accelerates execution, it never removes user responsibility for security, copyright, or ethical compliance.\n\n**Core Question:** \"Am I exposing PII in the context window? Am I deploying unvetted code to production?\"\n\n## Key Characteristics\n\n### The Editor-in-Chief Mental Model\n\nThe 4D Framework repositions the human from \"prompt writer\" to \"editorial director.\" Just as a newspaper editor doesn't write every article but maintains accountability for what gets published, the AI-fluent professional maintains responsibility for all AI-generated outputs.\n\n### Continuous Cycle\n\nThese four dimensions are not sequential steps but concurrent concerns. Every AI interaction requires simultaneous attention to all four:\n\n- What should I delegate?\n- How clearly have I described it?\n- How critically am I reviewing the output?\n- What risks am I accepting?\n\n## Anti-Patterns\n\n| Anti-Pattern | Description |\n|--------------|-------------|\n| **Over-Delegation** | Assigning strategic decisions or ethically sensitive tasks to AI |\n| **Vague Description** | Using natural language prompts without context, constraints, or examples |\n| **Blind Acceptance** | Copy-pasting AI output without verification |\n| **Liability Denial** | Assuming AI-generated content is inherently trustworthy or legally defensible |\n\n## ASDLC Usage\n\nApplied in: [AGENTS.md Specification](/practices/agents-md-spec), [Context Engineering](/concepts/context-engineering), [Context Gates](/patterns/context-gates)\n\nThe 4D dimensions map to ASDLC constructs: Delegation → agent autonomy levels, Description → context engineering, Discernment → context gates, Diligence → guardrail protocols.",
    "tags": ["AI Fluency", "Human-AI Collaboration", "Cognitive Model", "Prompt Engineering"],
    "references": [
      {
        "type": "website",
        "title": "Anthropic AI Fluency Course",
        "url": "https://anthropic.skilljar.com/page/ai-fluency",
        "author": "Anthropic",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Original source of the 4D Framework for effective generative AI use, teaching the core competencies needed for human-AI collaboration."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2026-01-09T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Internal study of 132 engineers quantifying the 4D dimensions in practice: delegation patterns, cold start problems, the paradox of supervision, and human-owned decision categories."
      }
    ]
  },
  {
    "slug": "agentic-sdlc",
    "collection": "concepts",
    "title": "Agentic SDLC",
    "description": "Framework for industrializing software development where agents serve as the logistic layer while humans design, govern, and optimize the flow.",
    "status": "Live",
    "content": "## Definition\n\nThe Agentic Software Development Life Cycle (ASDLC) is a framework for industrializing software engineering. It represents the shift from craft-based development (individual artisans, manual tooling, implicit knowledge) to industrial-scale production (standardized processes, agent orchestration, deterministic protocols).\n\n> \"Agentic architecture is the conveyor belt for knowledge work.\" — [Ville Takanen](https://villetakanen.com/blog/scenario-26-industrialization-of-knowledge-work/)\n\nASDLC is not about \"AI coding assistants\" that make developers 10% faster. It's about building the **software factory**—systems where agents serve as the architecture of labor while humans design, govern, and optimize the flow.\n\n## The Industrial Thesis\n\n**Agents do not replace humans; they industrialize execution.**\n\nJust as robotic arms automate welding without replacing manufacturing expertise, agents automate high-friction parts of knowledge work (logistics, syntax, verification) while humans focus on intent, architecture, and governance.\n\nIn this model:\n- **Agents are the logistic layer** — Moving information, verifying specs, executing tests\n- **Context is the supply chain** — Just-in-Time delivery of requirements, schemas, and code\n- **Standardization is mandatory** — Schemas, typed interfaces, deterministic protocols replace \"vibes\"\n\n## The Cybernetic Model\n\nASDLC operates at [L3 Conditional Autonomy](/concepts/levels-of-autonomy)—a \"Fighter Jet\" model where the Agent acts as the Pilot executing maneuvers, and the Human acts as the Instructor-in-the-Cockpit.\n\n**Key Insight:** Compute is cheap, but novelty and correctness are expensive. Agents naturally drift toward the \"average\" solution (Regression to the Mean). The Instructor's role is not to write code, but to define failure boundaries (Determinism) and inject strategic intent (Steering) that guides agents out of mediocrity.\n\n## The Cybernetic Loop\n\nThe lifecycle replaces the linear CI/CD pipeline with a high-frequency feedback loop:\n\n**Mission Definition**: The Instructor defines the \"Objective Packet\" (Intent + Constraints). This is the core of Context Engineering.\n\n**Generation (The Maneuver)**: The Agent autonomously maps context—often using the Model Context Protocol (MCP) to fetch live data—and executes the task.\n\n**Verification (The Sim)**: Automated Gates check for technical correctness (deterministic), while the Agent's Constitution steers semantic intent (probabilistic).\n\n**Course Correction (HITL)**: The Instructor intervenes on technically correct but \"generic\" solutions to enforce architectural novelty.\n\n## Strategic Pillars\n\n### Factory Architecture (Orchestration)\nProjects structured with agents as connective tissue, moving from monolithic context windows to discrete, specialized stations (Planning, Spec-Definition, Implementation, Review).\n\n### Standardized Parts (Determinism)\nSchema-First Development where agents fulfill contracts, not guesses. `AGENTS.md`, `specs/`, and strict linting serve as the \"jigs\" and \"molds\" that constrain agent output.\n\n### Quality Control (Governance)\nAutomated, rigorous inspection through Probabilistic Unit Tests and Human-in-the-Loop (HITL) gates. Trust the _process_, not just the output.\n\n## ASDLC Usage\n\nFull project vision: [/docs/vision.md](../../docs/vision.md)\n\nApplied in: [Specs](/patterns/the-spec), [AGENTS.md Specification](/practices/agents-md-spec), [Context Gates](/patterns/context-gates), [Model Routing](/patterns/model-routing)",
    "tags": ["Core", "SDLC", "Methodology", "Industrialization"],
    "references": []
  },
  {
    "slug": "behavior-driven-development",
    "collection": "concepts",
    "title": "Behavior-Driven Development",
    "description": "A collaborative specification methodology that defines system behavior in natural language scenarios, bridging business intent and machine-verifiable acceptance criteria.",
    "status": "Live",
    "content": "## Definition\n\nBehavior-Driven Development (BDD) is a collaborative specification methodology that defines system behavior in natural language scenarios. It synthesizes Test-Driven Development (TDD) and Acceptance Test-Driven Development (ATDD), emphasizing the \"Five Whys\" principle: every user story should trace to a business outcome.\n\nThe key evolution from testing to BDD is the shift from \"test\" to \"specification.\" Tests verify correctness; specifications define expected behavior. In agentic workflows, this distinction matters because agents need to understand *what* behavior is expected, not just *what code to write*.\n\n## Key Characteristics\n\n### From Tests to Specifications of Behavior\n\n| Aspect | Unit Testing (TDD) | Behavior-Driven Development |\n|--------|-------------------|----------------------------|\n| **Primary Focus** | Correctness of code at unit level | System behavior from user perspective |\n| **Language** | Code-based (Python, Java, etc.) | Natural language ([Gherkin](/concepts/gherkin)) |\n| **Stakeholders** | Developers | Developers, QA, Business Analysts, POs |\n| **Signal** | Pass/Fail on logic | Alignment with business objectives |\n| **Agent Role** | Minimal (code generation) | Central (agent interprets and executes behavior) |\n\n### The Three Roles in BDD\n\nBDD emphasizes collaboration between three perspectives:\n\n1. **Business** — Defines the \"what\" and \"why\" (business value, user outcomes)\n2. **Development** — Defines the \"how\" (implementation approach)\n3. **Quality** — Defines the \"proof\" (verification criteria)\n\nIn agentic development, the AI agent often handles Development while Business and Quality remain human-defined. BDD provides the structured handoff format.\n\n### BDD in the Probabilistic Era\n\nTraditional BDD was designed for deterministic systems: given specific inputs, expect specific outputs. Agentic systems are probabilistic—LLM outputs vary based on context, temperature, and emergent behavior.\n\nBDD adapts to this by:\n- Defining **behavioral contracts** rather than implementation details\n- Allowing agents to determine *how* to achieve specified behavior\n- Providing **semantic anchors** that constrain the reasoning space without over-specifying\n\n## BDD and the Spec Pattern\n\nIn ASDLC, BDD principles are implemented through the [Spec](/patterns/the-spec) pattern:\n\n| BDD Component | Spec Implementation |\n|---------------|---------------------|\n| Feature description | Spec Context section |\n| Business rules | Blueprint constraints |\n| Acceptance scenarios | Contract section (Gherkin scenarios) |\n| Step definitions | Agent tool calls and verification functions |\n\nThe Spec's Contract section explicitly uses \"Gherkin-style journeys for E2E tests\"—this is BDD applied to agentic context.\n\n## Evolution: Evaluation-Driven Development\n\nAs AI systems require multi-dimensional evaluation beyond binary pass/fail, BDD is evolving toward Evaluation-Driven Development (EDD):\n\n| Dimension | Metric | Purpose |\n|-----------|--------|---------|\n| **Grounding** | LLM-as-a-Judge score | Is response based on source data? |\n| **Alignment** | Prompt-following accuracy | Did agent follow specific instructions? |\n| **Safety** | Toxicity/bias detection | Is output harmful or discriminatory? |\n| **Factuality** | Hallucination detection | Is information accurate? |\n| **Performance** | Latency, token efficiency | Is system performant and cost-effective? |\n\nThis aligns with ASDLC's [Learning Loop](/concepts/learning-loop)—evaluation results crystallize into updated specifications through the same Explore→Learn→Crystallize cycle.\n\n## ASDLC Usage\n\nBDD's value in agentic development is **semantic anchoring**. When an agent is given a Gherkin scenario, it receives a \"specification of behavior\" that:\n\n- Partitions the reasoning space into manageable segments (Given/When/Then)\n- Defines success criteria without over-specifying implementation\n- Aligns technical execution with business intent\n\nThis is why BDD scenarios belong in Specs, not just test suites. They're not just verification artifacts—they're **functional blueprints** that guide agent reasoning.\n\nApplied in:\n- [The Spec](/patterns/the-spec) — Implements BDD through Blueprint (constraints) and Contract (scenarios)\n- [Context Gates](/patterns/context-gates) — BDD scenarios define verification criteria at gates\n- [Living Specs](/practices/living-specs) — BDD scenarios are maintained as living documentation\n\n## Anti-Patterns\n\n| Anti-Pattern | Description | Failure Mode |\n|--------------|-------------|--------------|\n| **Implementation Leakage** | Scenarios that specify *how* instead of *what* | Over-constrained agents; brittle specifications |\n| **UI-Coupled Scenarios** | \"Click button X, enter text Y\" instead of behavioral intent | Breaks when UI changes; not agent-friendly |\n| **Scenario Explosion** | Exhaustive scenarios for every edge case | Unmaintainable; agents can't prioritize |\n| **Tech Jargon** | Scenarios written in developer language | Business stakeholders can't validate |\n| **Fire-and-Forget** | Writing scenarios once, never updating | Context amnesia; specs drift from reality |",
    "tags": ["Testing", "Specification", "Agile", "Requirements"],
    "references": [
      {
        "type": "website",
        "title": "Introducing BDD",
        "url": "https://dannorth.net/introducing-bdd/",
        "author": "Dan North",
        "published": "2006-03-01T00:00:00.000Z",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Original formulation of Behavior-Driven Development by its creator."
      },
      {
        "type": "website",
        "title": "Behavior Driven Development",
        "url": "https://www.agilealliance.org/glossary/bdd/",
        "author": "Agile Alliance",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Industry-standard glossary definition from the Agile Alliance."
      }
    ]
  },
  {
    "slug": "context-engineering",
    "collection": "concepts",
    "title": "Context Engineering",
    "description": "Context Engineering is the practice of structuring information to optimize LLM comprehension and output quality.",
    "status": "Live",
    "content": "## Definition\n\nContext Engineering is the systematic approach to designing and structuring the input context provided to Large Language Models (LLMs) to maximize their effectiveness, accuracy, and reliability in generating outputs.\n\nThe practice emerged from the recognition that LLMs operate on explicit information only—they cannot intuit missing business logic or infer unstated constraints. Context Engineering addresses this by making tacit knowledge explicit, machine-readable, and version-controlled.\n\nWhile ASDLC focuses on software development, Context Engineering is domain-agnostic:\n* **In Design:** Design system tokens and Figma layer naming conventions fed to UI agents\n* **In Law:** Briefs restricting paralegal agents to specific case law precedents  \n* **In SDLC:** The `AGENTS.md` file steering agents toward implementation patterns\n\nAnywhere agents operate, context is the constraint that turns raw intelligence into specific value.\n\nMartin Fowler observes: \"As I listen to people who are serious with AI-assisted programming, the crucial thing I hear is managing context.\"\n\nAnthropic's research confirms this. Engineers cite the **cold start problem** as the biggest blocker:\n\n> \"There is a lot of intrinsic information that I just have about how my team's code base works that Claude will not have by default… I could spend time trying to iterate on the perfect prompt [but] I'm just going to go and do it myself.\"\n\nContext Engineering solves cold start by making tacit knowledge explicit, machine-readable, and version-controlled so agents can act on it without prompt iteration.\n\n## Key Characteristics\n\n**The Requirements Gap**\n\n\"Prompt Engineering\" is often a misnomer. It is simply **Requirements Engineering** applied to a non-human entity that cannot intuit missing business logic. Human developers ask clarifying questions when requirements are vague (\"What happens if the payment fails?\"). AI models build something based on probability. Errors generally surface only when the system breaks in production.\n\n**Core Attributes**\n\n1.  **Version Controlled:** Context exists as a software asset that lives in the repo, is diffed in PRs, and is subject to peer review.\n2.  **Standardized:** Formatted to be readable by any agent (Cursor, Windsurf, Devin, GitHub Copilot).\n3.  **Iterative:** Continuously refined based on agent failure modes and tacit information discovered by Human-in-the-loop (HITL) workflows.\n4.  **Schema-First:** Data structures defined before requesting content generation to ensure type safety and validation.\n5.  **Hierarchical:** Information organized by importance—critical instructions first, references second, examples last.\n\n## ASDLC Usage\n\nIn ASDLC, context is treated as version-controlled code, not ephemeral prompts.\n\n**Context vs Guardrails:**\n\nA distinction exists between `Guardrails` (Safety) and `Context` (Utility). Currently, many `AGENTS.md` files contain defensive instructions like \"Do not delete files outside this directory\" or \"Do not output raw secrets.\" This is likely a transitional state. OpenAI, Anthropic, Google, and platform wrappers are racing to bake these safety constraints directly into the inference layer. Soon, telling an agent \"Don't leak API keys\" will be as redundant as telling a compiler \"Optimize for speed.\"\n\nApplied in:\n- [AGENTS.md Specification](/practices/agents-md-spec) — The practical application of context engineering in repositories.\n- [Context Gates](/concepts/context-gates) — Checkpoints where context is validated or injected.\n- [Model Context Protocol](/concepts/model-context-protocol) — The standard for serving context to agents.\n\n## Applications\n\n- **Code Generation**: Provide file structure, type definitions, and examples\n- **Content Creation**: Supply style guides, templates, and domain knowledge\n- **Data Analysis**: Include schema definitions, sample data, and output formats\n\n## Best Practices\n\n1. **Front-load critical information**: LLMs prioritize the beginning and end of the context window.\n2. **Use consistent formatting**: Delimiters (XML tags, markdown sections) help models separate distinct information types.\n3. **Provide clear success criteria**: Define what \"good\" looks like to reduce probabilistic guessing.\n4. **Include negative examples**: Explicitly stating what *not* to do is often more effective than generic constraints.\n5. **Test with minimal viable context**: Start small and add context only when the agent fails, to avoid \"distractor\" noise.\n\n## Bounded Context Reconstruction\n\nRecent research formalizes a key principle in file-centric agent design: context windows are not memory. The InfiAgent framework (Yu et al., 2026) demonstrates that agent performance degrades severely when relying on compressed long-context prompts, even with strong models.\n\nTheir formulation:\n\n```\nc_bounded_t = g(Fₜ, a_{t-k:t-1})\n```\n\nWhere:\n- `Fₜ` = persistent state (files in the workspace)\n- `a_{t-k:t-1}` = fixed window of recent actions (typically k=10)\n- `g(·)` = deterministic context-construction function\n\n**Key finding**: Replacing file-centric state with compressed context dropped task completion from 80/80 to 27.7/80 average, even with Claude 4.5 Sonnet. Long context is not a substitute for persistent state.\n\nThis validates a core ASDLC claim: treat context as a reconstructed view of authoritative file state, not as accumulating conversation history.",
    "tags": ["AI", "LLM", "Prompt Engineering", "Context Engineering"],
    "references": [
      {
        "type": "website",
        "title": "OpenAI Best Practices for Prompt Engineering",
        "url": "https://platform.openai.com/docs/guides/prompt-engineering",
        "author": "OpenAI",
        "published": "2024-01-15T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Foundational guidance on structuring prompts and context for optimal LLM performance."
      },
      {
        "type": "website",
        "title": "Constitutional AI Documentation",
        "url": "https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback",
        "author": "Anthropic",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Documentation on Anthropic's approach to AI alignment and context-based safety constraints."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Research identifying the cold start problem as the primary blocker in AI-assisted development."
      },
      {
        "type": "paper",
        "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
        "url": "https://arxiv.org/abs/2601.03204",
        "author": "Chenglin Yu, Yuchen Wang, Songmiao Wang, Hongxia Yang, Ming Li",
        "published": "2026-01-06T00:00:00.000Z",
        "accessed": "2026-01-10T00:00:00.000Z",
        "annotation": "Empirical validation of file-centric state management for long-horizon agent stability."
      }
    ]
  },
  {
    "slug": "event-modeling",
    "collection": "concepts",
    "title": "Event Modeling",
    "description": "A system blueprinting method that centers on events as the primary source of truth, serving as a rigorous bridge between visual design and technical implementation.",
    "status": "Experimental",
    "content": "## Definition\n\n**Event Modeling** is a method for designing information systems by mapping what happens over time. It creates a linear blueprint that serves as the single source of truth for Product, Design, and Engineering.\n\nUnlike static diagrams (like ERDs or UML) that focus on structure, Event Modeling focuses on the **narrative of the system**. It visualizes the system as a film strip, showing exactly how a user’s action impacts the system state and what information is displayed back to them.\n\n### Core Components\nAn Event Model is composed of four distinct elements:\n\n* **Commands (Blue)**: The intent or action initiated by the user (e.g., \"Submit Order\").\n* **Events (Orange)**: A fact recorded by the system that cannot be changed (e.g., \"OrderPlaced\"). This is the single source of truth.\n* **Views (Green)**: Information displayed to the user, derived from previous events (e.g., \"Order Confirmation Screen\").\n* **Processes**: The logic or automation that reacts to events to trigger other commands or update views.\n\n## Why It Matters for AI\n\nIn modern software development, ambiguity is the enemy. While human engineers can infer intent from a loose visual mockup, AI models require explicit instructions.\n\nEvent Modeling forces implicit business rules to become explicit. By defining the exact data payload of every *Command* and the resulting state change of every *Event*, we provide AI agents with a deterministic roadmap. This ensures the generated code handles edge cases and data consistency correctly, rather than just \"looking right\" on the frontend.\n\n## Relationship to Requirements\n\nEvent Modeling acts as a bridge between **Visual Design** (what it looks like) and **Technical Architecture** (how it works).\n\nIt does not replace functional requirements; rather, it validates them. A feature is only considered \"defined\" when there is a complete path mapped from the user's view, through the command, to the stored event, and back to the view. This \"closed loop\" guarantees that every pixel on the screen is backed by real data.",
    "tags": ["Architecture", "Requirements", "Standards"],
    "references": [
      {
        "type": "website",
        "title": "EventModeling.org",
        "url": "https://eventmodeling.org/",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "The official home of the Event Modeling methodology, defining the complete framework for event-centric system design."
      }
    ]
  },
  {
    "slug": "gherkin",
    "collection": "concepts",
    "title": "Gherkin",
    "description": "A structured, domain-specific language using Given-When-Then syntax to define behavioral specifications that are both human-readable and machine-actionable.",
    "status": "Live",
    "content": "## Definition\n\nGherkin is a structured, domain-specific language using Given-When-Then syntax to define behavioral specifications in plain text. While [Behavior-Driven Development](/concepts/behavior-driven-development) provides the methodology, Gherkin provides the concrete syntax.\n\nGherkin's effectiveness for LLM agents stems from its properties: human-readable without technical jargon, machine-parseable with predictable structure, and aligned between technical and non-technical stakeholders. Each keyword defines a phase of reasoning that prevents agents from conflating setup, action, and verification into an undifferentiated blob.\n\n## The Given-When-Then Structure\n\nGherkin scenarios follow a consistent three-part structure:\n\n```gherkin\nFeature: User Authentication\n  As a registered user\n  I want to log into the system\n  So that I can access my personalized dashboard\n\n  Scenario: Successful login with valid credentials\n    Given a registered user with email \"user@example.com\"\n    And the user has password \"SecurePass123\"\n    When the user submits login credentials\n    Then the user should be redirected to the dashboard\n    And a session token should be created\n```\n\n## Keyword Semantics\n\n| Keyword | Traditional BDD | Agentic Translation |\n|---------|-----------------|---------------------|\n| **Given** | Preconditions or initial state | Context setting, memory retrieval, environment setup |\n| **When** | The trigger event or user action | Task execution, tool invocation, decision step |\n| **Then** | The observable outcome | Verification criteria, alignment check, evidence-of-done |\n| **And/But** | Additional conditions within a step | Logical constraints, secondary validation parameters |\n| **Feature** | High-level description of functionality | Functional blueprint, overall agentic goal |\n| **Background** | Steps common to all scenarios | Pre-test fixtures, global environment variables |\n\n## Why Gherkin Works for Agents\n\n### Reasoning Space Partitioning\n\nEach keyword defines a phase of reasoning:\n- **Given** constrains what the agent should assume\n- **When** defines the specific action to take\n- **Then** defines what success looks like\n\nThis maps directly to the [OODA Loop](/concepts/ooda-loop): Given=Observe context, When=Act, Then=Observe outcome.\n\n### What vs How Separation\n\nGherkin specifies *what* behavior is expected without dictating *how* to implement it. This allows agents to interpret intent dynamically, select appropriate tools, and adapt implementation to context.\n\n### The Right Level of Abstraction\n\nGherkin steps should be specific enough to verify but abstract enough to allow agent interpretation:\n\n| Too Specific (Brittle) | Too Vague (Unverifiable) | Right Level |\n|------------------------|--------------------------|-------------|\n| \"Click the blue Submit button at coordinates (450, 320)\" | \"Do the thing\" | \"Submit the registration form\" |\n| \"POST to /api/v2/users with JSON body {...}\" | \"Create user somehow\" | \"Create a new user account\" |\n| \"Wait 3000ms then check element #success\" | \"Verify it worked\" | \"The operation should complete successfully\" |\n\n## Gherkin in ASDLC Specs\n\nThe [Spec](/patterns/the-spec) pattern uses Gherkin in the Contract section:\n\n```markdown\n## Contract\n\n### Scenarios\n\n#### Happy Path\nGiven a valid API key\nWhen the user requests /api/notifications\nThen the response returns within 100ms\nAnd the payload contains the user's notifications\n\n#### Rate Limiting\nGiven a user who has exceeded rate limits\nWhen the user requests /api/notifications\nThen the response returns 429 Too Many Requests\nAnd the Retry-After header indicates wait time\n```\n\nThis structure enables:\n- **Agents** to understand acceptance criteria before implementation\n- **Context Gates** to verify scenarios pass before promotion\n- **Adversarial Review** to validate against documented behavior\n\n## Best Practices for Agentic Gherkin\n\n### Declarative Over Imperative\n\n```gherkin\n# Bad (imperative)\nWhen I click the Login button\nAnd I wait for the page to load\nAnd I check the URL contains \"/dashboard\"\n\n# Good (declarative)\nWhen the user logs in\nThen the user should see the dashboard\n```\n\n### Business Language Over Technical Language\n\n```gherkin\n# Bad (technical)\nGiven a user record exists in the PostgreSQL users table\n\n# Good (business)\nGiven a registered user\n```\n\n### Scenario Outlines for Data Variations\n\n```gherkin\nScenario Outline: Rate limiting by tier\n  Given a user on the <tier> plan\n  When the user makes <requests> requests per minute\n  Then the response should be <status>\n\n  Examples:\n    | tier       | requests | status |\n    | free       | 10       | 200    |\n    | free       | 11       | 429    |\n    | premium    | 100      | 200    |\n    | premium    | 101      | 429    |\n```\n\n## ASDLC Usage\n\nGherkin isn't just a testing syntax—it's a **semantic constraint language** for agent behavior.\n\nWhen an agent reads a Gherkin scenario:\n- **Given** tells it what to assume (context setup)\n- **When** tells it what action to take (execution scope)\n- **Then** tells it what success looks like (verification criteria)\n\nThis partitioning prevents \"context bleed\" where agents conflate setup, action, and verification.\n\nApplied in:\n- [The Spec](/patterns/the-spec) — Contract section uses Gherkin scenarios\n- [Context Gates](/patterns/context-gates) — Gherkin scenarios define gate verification criteria\n- [Living Specs](/practices/living-specs) — Gherkin scenarios evolve with the feature\n\n## Anti-Patterns\n\n| Anti-Pattern | Description | Failure Mode |\n|--------------|-------------|--------------|\n| **Imperative Steps** | \"Click X, type Y, wait Z\" instead of declarative behavior | Brittle; breaks on UI changes; not agent-friendly |\n| **Implementation Details** | \"Call the createUser() method with params\" | Couples scenarios to implementation |\n| **Scenario Novels** | 50-step scenarios covering everything | Unreadable; unclear what's being verified |\n| **Copy-Paste Scenarios** | Duplicating steps instead of using Background | Maintenance nightmare |\n| **Testing the UI** | Scenarios that verify UI state, not business outcomes | Miss the point; break frequently |",
    "tags": ["Testing", "Specification", "BDD", "Syntax"],
    "references": [
      {
        "type": "website",
        "title": "Gherkin Reference",
        "url": "https://cucumber.io/docs/gherkin/reference/",
        "author": "Cucumber",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Canonical documentation for Gherkin syntax and semantics."
      },
      {
        "type": "website",
        "title": "Behavior Driven Development",
        "url": "https://www.agilealliance.org/glossary/bdd/",
        "author": "Agile Alliance",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Industry-standard glossary covering BDD and Gherkin terminology."
      }
    ]
  },
  {
    "slug": "learning-loop",
    "collection": "concepts",
    "title": "The Learning Loop",
    "description": "The iterative cycle between exploratory implementation and spec refinement, balancing vibe coding velocity with captured learnings.",
    "status": "Live",
    "content": "## Definition\n\nThe Learning Loop is the iterative cycle between exploratory implementation and constraint crystallization. It acknowledges that understanding emerges through building, while ensuring that understanding is captured for future agent sessions.\n\nKent Beck critiques spec-driven approaches that assume \"you aren't going to learn anything during implementation.\" He's right—discovery is essential. But pure vibe coding loses those discoveries. The next agent session starts from zero, re-discovering (or missing) the same constraints.\n\nThe Learning Loop preserves discoveries as machine-readable context, enabling compounding understanding across sessions.\n\n## The Cycle\n\n1. **Explore** — Vibe code to discover edge cases, performance characteristics, or API behaviors\n2. **Learn** — Identify constraints that weren't obvious from requirements\n3. **Crystallize** — Update the Spec with discovered constraints\n4. **Verify** — Gate future implementations against the updated Spec\n5. **Repeat**\n\nEach iteration builds on the last. The spec grows smarter, and agents inherit the learnings of every previous session.\n\n## OODA Foundation\n\nThe Learning Loop is an application of the [OODA Loop](/concepts/ooda-loop) to software development:\n\n| Learning Loop Phase | OODA Equivalent |\n|---------------------|------------------|\n| **Explore** | Observe + Act (gather information through building) |\n| **Learn** | Orient (interpret what was discovered) |\n| **Crystallize** | Decide (commit learnings to persistent format) |\n| **Verify** | Observe (confirm crystallized constraints via gates) |\n\nThe key insight: in software development, **Orient and Observe are interleaved**. You often can't observe relevant constraints until you've built something that reveals them. The Learning Loop makes this explicit by treating Explore as a legitimate phase rather than a deviation from the plan.\n\n## Key Characteristics\n\n### Not Waterfall\n\nThe Learning Loop explicitly rejects the waterfall assumption that all constraints can be known upfront. Specs are scaffolding that evolve, not stone tablets.\n\n### Not Pure Vibe Coding\n\nThe Learning Loop also rejects the vibe coding assumption that documentation is optional. Undocumented learnings are lost learnings—the next agent (or human) will repeat the same mistakes.\n\n### Machine-Readable Capture\n\nLearnings must be captured in formats agents can consume: schemas, constraints in YAML, acceptance criteria in markdown. Natural language is acceptable but structured data is preferred.\n\n> \"The real capability—our ability to respond to change—comes not from how fast we can produce code, but from how deeply we understand the system we are shaping.\"\n> — Unmesh Joshi\n\n## Automation: The Ralph Loop\n\nThe Learning Loop describes an iterative cycle that typically involves human judgment at each phase. The [Ralph Loop](/patterns/ralph-loop) automates this cycle for tasks with machine-verifiable completion criteria:\n\n| Learning Loop Phase | Ralph Loop Implementation |\n|---------------------|---------------------------|\n| **Explore** | Agent implements based on PBI/Spec |\n| **Learn** | Agent reads error logs, test failures, build output |\n| **Crystallize** | Agent updates progress.txt; commits to Git |\n| **Verify** | External tools (Jest, tsc, Docker) confirm success |\n\nWhen verification fails, Ralph automatically re-enters Explore with the learned context. The loop continues until external verification passes or iteration limit is reached.\n\n**Key difference:** The Learning Loop expects human judgment in the Learn and Crystallize phases. The Ralph Loop requires that \"learning\" be expressible as observable state (error logs, test results) and \"crystallization\" be automatic (Git commits, progress files).\n\nRalph Loops work best when success criteria are machine-verifiable (tests pass, builds complete). For tasks requiring human judgment—ambiguous requirements, architectural decisions, product direction—the Learning Loop remains the appropriate model.\n\n## ASDLC Usage\n\nIn ASDLC, the Learning Loop connects several core concepts:\n\n- **OODA Loop** — The foundational cognitive model the Learning Loop implements\n- **Vibe Coding** is the Explore phase (valid for prototyping and discovery)\n- **Living Specs** capture the Crystallize phase\n- **Context Gates** enforce the Verify phase\n- **Ralph Loop** — Automated implementation for machine-verifiable tasks\n- **PBIs** trigger iteration through the loop\n\nApplied in:\n- [OODA Loop](/concepts/ooda-loop) — The cognitive model foundation\n- [Spec-Driven Development](/concepts/spec-driven-development) — Iterative refinement of specs\n- [Living Specs](/practices/living-specs) — Maintenance of captured learnings\n- [Context Gates](/patterns/context-gates) — Verification checkpoints\n- [Ralph Loop](/patterns/ralph-loop) — Automated terminal implementation\n\n## Anti-Patterns\n\n| Anti-Pattern | Description |\n|--------------|-------------|\n| **Waterfall Specs** | Writing exhaustive specs before any implementation, assuming no learning will occur |\n| **Ephemeral Vibe Coding** | Generating code without ever crystallizing learnings into specs |\n| **Spec-as-Paperwork** | Updating specs for compliance rather than genuine constraint capture |\n| **Post-Hoc Documentation** | Writing specs after implementation is complete, losing the iterative benefit |",
    "tags": ["Core", "Methodology", "Learning"],
    "references": [
      {
        "type": "website",
        "title": "Martin Fowler Fragment: January 8, 2026",
        "url": "https://martinfowler.com/fragments/2026-01-08.html",
        "author": "Martin Fowler",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Commentary on the tension between specification and learning during implementation."
      },
      {
        "type": "website",
        "title": "Kent Beck on Spec-Driven Development",
        "url": "https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/",
        "author": "Kent Beck",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Critique emphasizing that specifications must accommodate learning during implementation."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Internal study showing how engineers balance AI velocity with understanding."
      }
    ]
  },
  {
    "slug": "levels-of-autonomy",
    "collection": "concepts",
    "title": "Levels of Autonomy",
    "description": "SAE-inspired taxonomy for AI agent autonomy in software development, from L1 (assistive) to L5 (full), standardized at L3 conditional autonomy.",
    "status": "Live",
    "content": "## Definition\n\nThe **Levels of Autonomy** scale categorizes AI systems based on their operational independence in software development contexts. Inspired by the SAE J3016 automotive standard, it provides a shared vocabulary for discussing human oversight requirements.\n\nThe scale identifies where the **Context Gate** (the boundary of human oversight) must be placed for each level. Under this taxonomy, autonomy is not a measure of intelligence—it is a measure of operational risk and required human involvement.\n\n## The Scale\n\n| Level | Designation | Description | Human Role | Failure Mode |\n| :--- | :--- | :--- | :--- | :--- |\n| **L1** | Assistive | Autocomplete, Chatbots. Zero state retention. | Driver. Hands on wheel 100% of time. | Distraction / Minor Syntax Errors |\n| **L2** | Task-Based | \"Fix this function.\" Single-file context. | Reviewer. Checks output before commit. | Logic bugs within a single file. |\n| **L3** | Conditional | \"Implement this feature.\" Multi-file orchestration. | Instructor. Defines constraints & intervenes on \"drift.\" | Regression to the Mean (Mediocrity). |\n| **L4** | High | \"Manage this backlog.\" Self-directed planning. | Auditor. Post-hoc analysis. | Silent Failure. Strategic drift over time. |\n| **L5** | Full | \"Run this company.\" | Consumer. Passive beneficiary. | Existential alignment drift. |\n\n## Analogy: The Self-Driving Standard (SAE)\n\nThe software autonomy scale maps directly to SAE J3016, the automotive standard for autonomous vehicles. This clarifies \"Human-in-the-Loop\" requirements using familiar terminology.\n\n| ASDLC Level | SAE Equivalent | The \"Steering Wheel\" Metaphor |\n| :--- | :--- | :--- |\n| **L1** | L1 (Driver Assist) | **Hands On, Feet On.** AI nudges the wheel (Lane Keep) or gas (Cruise), but Human drives. |\n| **L2** | L2 (Partial) | **Hands On (mostly).** AI handles steering and speed in bursts, but Human monitors constantly. |\n| **L3** | L3 (Conditional) | **Hands Off, Eyes On.** AI executes the maneuver (The Drive). Human is the Instructor ready to grab the wheel immediately. |\n| **L4** | L4 (High) | **Mind Off.** Sleeping in the back seat within a geo-fenced area. Dangerous if the \"fence\" (Context) breaks. |\n| **L5** | L5 (Full) | **No Steering Wheel.** The vehicle has no manual controls. |\n\n## ASDLC Usage\n\nASDLC standardizes practices for **Level 3 (Conditional Autonomy)** in software engineering. While the industry frequently promotes L5 as the ultimate goal, this perspective is often counterproductive given current tooling maturity. L3 is established as the sensible default.\n\n> [!WARNING]\n> **Level 4 Autonomy Risks**\n> \n> At L4, agents operate for days without human intervention but lack the strategic foresight needed to maintain system integrity. This results in **Silent Drift**—the codebase continues to function technically but gradually deteriorates into an unmanageable state.\n> \n> Mitigation strategies exist (Advanced Context Gates, architectural health monitoring), but these solutions require further validation.\n\n> [!NOTE]\n> **Empirical Support for L3**\n> \n> Anthropic's 2025 internal study of 132 engineers validates L3 as the practical ceiling:\n> - Engineers fully delegate only **0-20%** of work\n> - Average **4.1 human turns** per Claude Code session\n> - High-level design and \"taste\" decisions remain **exclusively human-owned**\n> - The \"paradox of supervision\"—effective oversight requires skills that AI use may atrophy\n\nApplied in:\n- [Context Gates](/patterns/context-gates) — The mechanism enabling safe L3 operation\n- [Guardrails](/concepts/guardrails) — Safety constraints for agent behavior\n- [Agentic SDLC](/concepts/agentic-sdlc) — The broader methodology context",
    "tags": ["Taxonomy", "Standards"],
    "references": [
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Research showing 0-20% full delegation, 4.1 human turns per session, and exclusively human-owned high-level design decisions."
      }
    ]
  },
  {
    "slug": "mermaid",
    "collection": "concepts",
    "title": "Mermaid",
    "description": "A text-based diagramming language that renders flowcharts, sequences, and architectures from markdown, enabling version-controlled visual specifications.",
    "status": "Live",
    "content": "## Definition\n\nMermaid is a text-based diagramming language that renders flowcharts, sequence diagrams, and architecture visualizations from markdown-style code blocks. In agentic development, Mermaid serves as the specification language for processes, workflows, and system relationships.\n\nWhere [Gherkin](/concepts/gherkin) specifies *behavior* and [YAML](/concepts/yaml) specifies *structure*, Mermaid specifies *process*—how components interact, how data flows, and how state transitions occur.\n\n## Key Characteristics\n\n### Text-Based Diagrams\n\nMermaid diagrams are defined in plain text, making them:\n- **Version-controllable** — Diagram changes appear in diffs\n- **Reviewable** — Same PR process as code\n- **Agent-parseable** — LLMs can read and modify diagrams\n\n```mermaid\nflowchart LR\n    A[Input] --> B[Process]\n    B --> C[Output]\n```\n\n### Diagram Types\n\n| Type | Use Case | ASDLC Application |\n|------|----------|-------------------|\n| **Flowchart** | Process flows, decision trees | Feature Assembly, Context Gates |\n| **Sequence** | API interactions, message flows | Service contracts, Integration specs |\n| **State** | State machines, lifecycle | Component state, Workflow phases |\n| **Class** | Object relationships | Domain models, Architecture |\n| **ER** | Entity relationships | Data models, Schema design |\n| **Gantt** | Timeline, scheduling | Roadmaps, Sprint planning |\n\n### Subgraphs for Grouping\n\nSubgraphs partition complex diagrams into logical regions:\n\n```mermaid\nflowchart LR\n    subgraph Input\n        A[Source]\n    end\n    \n    subgraph Processing\n        B[Transform]\n        C[Validate]\n        B --> C\n    end\n    \n    A --> B\n    C --> D[Output]\n```\n\n## Mermaid in ASDLC\n\n### Spec Diagrams\n\nThe [Spec](/patterns/the-spec) pattern uses Mermaid to visualize feature architecture and assembly flows. Diagrams are embedded in markdown and rendered to SVG via `pnpm diagrams`.\n\n### Design System Integration\n\nASDLC diagrams use the Avionics design system palette:\n\n| Element | Color | Token |\n|---------|-------|-------|\n| Nodes | `#f04e30` | `--c-brand` |\n| Subgraphs | `#ebebe6` | `--c-bg-surface` |\n| Borders | `#d1d1c7` | `--c-border` |\n| Text | `#111111` | `--c-text-primary` |\n\nConfiguration lives in `mermaid.json` at the project root.\n\n### Generation Workflow\n\n1. Write ```` ```mermaid ```` block in markdown\n2. Run `pnpm diagrams` to generate SVG\n3. Script outputs to `public/mermaid/{slug}-fig-{n}.svg`\n4. Script adds `<figure>` reference after the mermaid block\n\nThis produces static SVGs that render consistently across browsers and export cleanly to PDF.\n\n## ASDLC Usage\n\nMermaid serves as the **process specification language** in ASDLC, completing the specification triad:\n\n| Language | Specifies | Example |\n|----------|-----------|---------|\n| **[Gherkin](/concepts/gherkin)** | Behavior | Given/When/Then scenarios |\n| **[YAML](/concepts/yaml)** | Structure | Schemas, configuration |\n| **Mermaid** | Process | Flowcharts, sequences |\n\nApplied in:\n- [The Spec](/patterns/the-spec) — Assembly flow diagrams\n- [Context Engineering](/concepts/context-engineering) — Context flow visualization\n- Design System — Component documentation\n\n## Anti-Patterns\n\n| Anti-Pattern | Description | Failure Mode |\n|--------------|-------------|--------------|\n| **Box Soup** | Too many nodes without grouping | Unreadable; no hierarchy |\n| **Arrow Spaghetti** | Excessive cross-connections | Confusing; hard to follow |\n| **No Labels** | Edges without descriptive text | Ambiguous relationships |\n| **Static Screenshots** | Images instead of text diagrams | Not version-controllable; decay |\n| **Over-Detailed** | Including implementation details | Noise; maintenance burden |\n\n## Best Practices\n\n1. **Group with Subgraphs** — Partition logical regions for clarity\n2. **Label Edges** — Describe what flows between nodes\n3. **Left-to-Right Flow** — Use `flowchart LR` for process flows\n4. **Top-to-Bottom Hierarchy** — Use `flowchart TB` for hierarchies\n5. **Limit Complexity** — If a diagram needs >15 nodes, split it\n6. **Use Design Tokens** — Configure colors via `mermaid.json`, not inline",
    "tags": ["Diagrams", "Visualization", "Specification", "Documentation"],
    "references": [
      {
        "type": "website",
        "title": "Mermaid Documentation",
        "url": "https://mermaid.js.org/intro/",
        "author": "Mermaid",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Official documentation for Mermaid diagramming syntax and features."
      }
    ]
  },
  {
    "slug": "ooda-loop",
    "collection": "concepts",
    "title": "OODA Loop",
    "description": "The Observe-Orient-Decide-Act decision cycle—a strategic model from military combat adapted for autonomous agent behavior in software development.",
    "status": "Live",
    "content": "## Definition\n\nThe OODA Loop—Observe, Orient, Decide, Act—is a strategic decision-making cycle originally developed by U.S. Air Force Colonel John Boyd for aerial combat. Boyd's insight: the combatant who cycles through these phases faster than their opponent gains decisive advantage. The key isn't raw speed—it's **tempo relative to environmental change**.\n\nBoyd's less-quoted but crucial insight: **Orient is everything**. The Orient phase is where mental models, context, and prior experience shape how observations become decisions. A faster but poorly-oriented loop loses to a slower but well-oriented one.\n\nIn agentic software development, OODA provides the cognitive model for how autonomous agents should behave: continuously cycling through observation, interpretation, planning, and execution.\n\n## The Four Phases\n\n1. **Observe** — Gather information about the current state of the environment\n2. **Orient** — Interpret observations through mental models, context, and constraints\n3. **Decide** — Formulate a specific plan for action based on orientation\n4. **Act** — Execute the plan, producing changes that feed new observations\n\nThe loop is continuous. Each Act produces new state, triggering new Observe, and the cycle repeats.\n\n## Key Characteristics\n\n### Tempo, Not Raw Speed\n\nThe strategic value of OODA isn't speed—it's cycling faster than the environment changes. In software development, the \"environment\" is the codebase, requirements, and constraints. An agent that can cycle through OODA before context rot sets in converges on correct solutions.\n\n### Orient as the Critical Phase\n\nFor AI agents, Orient is the **context window**. The quality of orientation depends on:\n\n- **Spec Clarity** — Garbage spec → garbage orientation\n- **Constitution Directives** — Values that shape interpretation\n- **Context Gates** — Filtering noise so orientation isn't polluted\n- **Prior State** — Git history, progress files, previous learnings\n\nThis is why [Context Engineering](/concepts/context-engineering) isn't optional overhead. It's engineering the Orient phase, which determines whether fast cycling produces progress or noise.\n\n### OODA vs. Single-Shot Interactions\n\nStandard LLM interactions are **Observe-Act**: user provides input, model produces output. No explicit Orient or Decide phase. The model's \"orientation\" is implicit in training and whatever context happens to be present.\n\nAgentic workflows make OODA explicit:\n\n| Phase | Single-Shot LLM | Agentic Workflow |\n|-------|-----------------|------------------|\n| **Observe** | User prompt | Instrumented: read files, run tests, check logs |\n| **Orient** | Implicit (training + context) | Engineered: Specs, Constitution, Context Gates |\n| **Decide** | Implicit | Explicit: agent states plan before acting |\n| **Act** | Generate response | Verified: external tools confirm success/failure |\n\nThis explicit structure enables debugging. When an agent fails, you can diagnose *which phase* broke down:\n\n- **Bad Observe?** Agent missed relevant information\n- **Bad Orient?** Context was polluted or incomplete\n- **Bad Decide?** Plan was incoherent given good orientation\n- **Bad Act?** Execution failed despite good plan\n\n## ASDLC Usage\n\nIn ASDLC, OODA explains why cyclic workflows outperform linear pipelines:\n\n| OODA Phase | Agent Behavior | ASDLC Component |\n|------------|----------------|-----------------|\n| **Observe** | Read codebase state, error logs, test results | File state, test output |\n| **Orient** | Interpret against context and constraints | [Context Gates](/patterns/context-gates), [AGENTS.md](/practices/agents-md-spec) |\n| **Decide** | Formulate implementation plan | PBI decomposition |\n| **Act** | Write code, run tests, commit | Micro-commits |\n\nThe [Learning Loop](/concepts/learning-loop) is OODA with an explicit \"Crystallize\" step that improves future Orient phases. Where OODA cycles continuously, Learning Loop captures discoveries into machine-readable context for subsequent agent sessions.\n\nApplied in:\n- [Context Engineering](/concepts/context-engineering) — The discipline of engineering the Orient phase\n- [Context Gates](/patterns/context-gates) — Checkpoints between OODA phases\n- [Levels of Autonomy](/concepts/levels-of-autonomy) — Higher autonomy requires more sophisticated Orient capabilities\n\n## Anti-Patterns\n\n| Anti-Pattern | Description | Failure Mode |\n|--------------|-------------|--------------|\n| **Observe-Act** | Skipping Orient/Decide. Classic vibe coding. | Works for simple tasks; fails at scale; no learning |\n| **Orient Paralysis** | Over-engineering context, never acting | Analysis paralysis; no forward progress |\n| **Stale Orient** | Not updating mental model when observations change | Context rot; agent operates on outdated assumptions |\n| **Observe Blindness** | Not instrumenting observation of relevant state | Agent misses critical information (failed tests, error logs) |\n| **Act Without Verify** | Not confirming action results before next cycle | Cascading errors; false confidence |",
    "tags": ["AI", "Agent Architecture", "Decision Making", "Military Strategy"],
    "references": [
      {
        "type": "book",
        "title": "Certain to Win: The Strategy of John Boyd, Applied to Business",
        "author": "Chet Richards",
        "isbn": "978-1413453775",
        "published": "2004-04-01T00:00:00.000Z",
        "annotation": "Accessible introduction to Boyd's OODA loop concepts applied beyond military contexts."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Research on tempo in human-AI collaboration; engineers who cycle faster with quality context converge on solutions."
      }
    ]
  },
  {
    "slug": "product-requirement-prompt",
    "collection": "concepts",
    "title": "Product Requirement Prompt (PRP)",
    "description": "A structured methodology combining PRD, codebase context, and agent runbook—the minimum spec for production-ready AI code.",
    "status": "Experimental",
    "content": "## Definition\n\nA **Product Requirement Prompt (PRP)** is a structured methodology that answers the question: *\"What's the minimum viable specification an AI coding agent needs to plausibly ship production-ready code in one pass?\"*\n\nAs creator Rasmus Widing defines it: **\"A PRP is PRD + curated codebase intelligence + agent runbook.\"**\n\nUnlike traditional PRDs (which exclude implementation details) or simple prompts (which lack structure), PRPs occupy the middle ground—a complete context packet that gives an agent everything it needs to execute autonomously within bounded scope.\n\nThe methodology emerged from practical engineering work in 2024 and has since become the foundation for agentic engineering training.\n\n## Key Characteristics\n\nPRPs are built on three core principles:\n\n1. **Plan before you prompt** — Structure thinking before invoking AI\n2. **Context is everything** — Comprehensive documentation enables quality output\n3. **Scope to what the model can reliably do in one pass** — Bounded execution units\n\nA complete PRP includes six components:\n\n| Component | Purpose |\n|-----------|---------|\n| **Goal** | What needs building |\n| **Why** | Business value and impact justification |\n| **Success Criteria** | Measurable checkpoints |\n| **All Needed Context** | Documentation references, file paths, code snippets |\n| **Implementation Blueprint** | Task breakdown and pseudocode |\n| **Validation Loop** | Multi-level testing (syntax, unit, integration) |\n\n### Key Differentiators from Traditional PRDs\n\n- **Precise context:** Specific file paths, library versions, code examples\n- **Documentation integration:** Links to relevant library docs and architectural patterns\n- **Known gotchas:** Critical warnings about potential pitfalls\n- **Validation frameworks:** Executable tests the AI can run and fix iteratively\n\n## ASDLC Usage\n\nPRP components map directly to ASDLC concepts—a case of convergent evolution in agentic development practices.\n\n| PRP Component | ASDLC Equivalent |\n|---------------|------------------|\n| Goal | [The Spec](/patterns/the-spec) — Blueprint |\n| Why | [Product Thinking](/concepts/product-thinking) |\n| Success Criteria | [Context Gates](/patterns/context-gates) |\n| All Needed Context | [Context Engineering](/concepts/context-engineering) |\n| Implementation Blueprint | [The PBI](/patterns/the-pbi) |\n| Validation Loop | [Context Gates](/patterns/context-gates) — Quality Gates |\n\nIn ASDLC terms, a PRP is equivalent to **The Spec + The PBI + curated Context Engineering**—bundled into a single artifact optimized for agent consumption.\n\nASDLC separates these concerns for reuse: multiple PBIs reference the same Spec, and context is curated per-task rather than duplicated. For simpler projects or rapid prototyping, the PRP's unified format may be more practical. The methodologies are complementary—PRPs can be thought of as \"collapsed ASDLC artifacts\" for single-pass execution.\n\nApplied in:\n- [Spec-Driven Development](/concepts/spec-driven-development) — The philosophy PRPs implement\n- [The Spec](/patterns/the-spec) — ASDLC's permanent specification pattern\n- [The PBI](/patterns/the-pbi) — ASDLC's transient execution unit\n\nSee also:\n- [Industry Alignment](/resources/industry-alignment) — Convergent frameworks in agentic development\n- [Spec-Driven Development](/concepts/spec-driven-development) — ASDLC's foundational methodology\n- [The Spec](/patterns/the-spec) — ASDLC's specification pattern\n- [Vibe Coding](/concepts/vibe-coding) — The anti-pattern both PRP and SDD address",
    "tags": ["Industry Term", "Spec-Driven Development", "Context Engineering"],
    "references": [
      {
        "type": "repository",
        "title": "PRPs: Agentic Engineering",
        "url": "https://github.com/Wirasm/PRPs-agentic-eng",
        "author": "Rasmus Widing",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Original methodology and templates for Product Requirement Prompts, defining the framework for AI-ready specifications."
      },
      {
        "type": "website",
        "title": "Rasmus Widing - LinkedIn Profile",
        "url": "https://www.linkedin.com/in/rasmuswiding/",
        "author": "Rasmus Widing",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Creator of PRP methodology, providing context on the convergent evolution with ASDLC principles."
      }
    ]
  },
  {
    "slug": "product-thinking",
    "collection": "concepts",
    "title": "Product Thinking",
    "description": "The practice of engineers thinking about user outcomes, business context, and the 'why' before the 'how'—the core human skill in the AI era.",
    "status": "Experimental",
    "content": "## Definition\n\nProduct Thinking is the practice of engineers understanding and prioritizing user outcomes, business context, and the reasoning behind technical work (\"why\") before focusing on implementation details (\"how\"). \n\nRather than waiting for fully-specified requirements and executing tasks mechanically, product-thinking engineers actively engage with the problem space. They ask:\n- What user problem does this solve?\n- Which tradeoffs are acceptable for this context?\n- How will this decision impact long-term maintainability?\n- Is this the right problem to solve at all?\n\nThis mindset originated in product management but has become essential for modern engineering teams, especially as AI increasingly handles implementation while humans must provide strategic judgment.\n\n## Key Characteristics\n\n**Outcome Orientation**\nProduct-thinking engineers measure success by user and business outcomes, not just task completion. They question whether closing a ticket actually moved the product forward.\n\n**Context Awareness**\nThey understand the broader system: user workflows, business constraints, competitive landscape, and technical debt landscape. Code decisions are made with this context, not in isolation.\n\n**Tradeoff Evaluation**\nEvery technical decision involves tradeoffs (speed vs maintainability, generality vs simplicity, build vs buy). Product-thinking engineers explicitly identify and evaluate these tradeoffs rather than defaulting to \"best practice.\"\n\n**Ownership Mindset**\nThey take responsibility for outcomes, not just implementations. If a feature ships but users don't adopt it, a product-thinking engineer investigates why, even if the code \"worked as specified.\"\n\n**Risk Recognition**\nThey can look at technically correct code and identify product risks: \"This will confuse users,\" \"This locks us into a vendor,\" \"This creates a support burden.\" These risks are invisible to AI.\n\n## The AI Era Shift\n\nMatt Watson (5x Founder/CTO, author of *Product Driven*) argues that **vibe coders outperform average engineers not because of superior coding skill, but because they think about the product**:\n\n> \"A lot of engineers? They're just waiting for requirements. That's usually a leadership problem. For years, we rewarded engineers for staying in their lane, closing tickets, and not rocking the boat. Then we act surprised when they don't think like owners.\"\n\n**The traditional model:**\n1. Product Manager writes requirements\n2. Engineer implements requirements\n3. Success = code matches spec\n\n**Why this fails in the AI era:**\n- AI can already handle \"just build this\" work faster than humans\n- The bottleneck shifts from implementation to **deciding what to build**\n- Engineers who only execute become redundant; those who evaluate and steer remain essential\n\n**The new competitive advantage:**\n- AI writes code; humans decide what matters\n- AI generates implementations; humans evaluate which tradeoffs are dangerous\n- AI follows instructions; humans recognize when \"the clean implementation is still the wrong product\"\n\nWatson's conclusion: **\"Product thinking isn't a bonus skill anymore. In an AI world, it's the job.\"**\n\n## The Leadership Problem\n\nProduct thinking doesn't emerge by accident. Watson identifies the structural cause:\n\n**Anti-patterns that kill product thinking:**\n- Engineers rewarded for \"staying in their lane\" instead of challenging requirements\n- Context withheld (\"you don't need to know the business reason, just build it\")\n- Decisions flowing top-down through a single bottleneck (PM or architect)\n- Success measured by velocity (story points closed) rather than outcomes (user problems solved)\n\n**What builds product thinking:**\n- Clearly explain **what** needs to be done and **why**\n- Give context instead of just tasks\n- Trust engineers to figure out the **how**\n- Train them to own outcomes, not just implementations\n\nIf every technical decision must flow through a product manager or architect, the organization has created a dependency on human bottlenecks that AI cannot solve.\n\n## Applications\n\n**Pre-AI Era:**\nProduct thinking was a differentiator for senior engineers and those in \"full-stack\" or startup environments. Most engineers could succeed by executing well-defined requirements.\n\n**AI Era:**\nProduct thinking becomes the baseline. As AI handles implementation, the human contribution shifts entirely to:\n1. Defining the problem worth solving\n2. Evaluating whether AI-generated solutions actually solve it\n3. Recognizing risks and tradeoffs the model cannot see\n\n**Where product thinking is essential:**\n- **Greenfield products:** No established patterns; every decision sets precedent\n- **Strategic refactoring:** Deciding which technical debt to address and why\n- **API design:** Tradeoffs between developer experience, performance, and flexibility\n- **Early-stage startups:** Speed-to-market vs maintainability requires constant judgment calls\n- **AI-assisted development:** Evaluating whether vibe-coded solutions are \"good enough\" or hiding risks\n\n## ASDLC Usage\n\nIn ASDLC, product thinking is **why Specs exist**. The Spec is not bureaucratic overhead—it's the forcing function that makes product thinking explicit and sharable.\n\n**The connection:**\n- **Product Thinking** = The human capability (understanding \"why\")\n- **The Spec** = The artifact that captures product thinking (machine-readable \"why\")\n- **Spec-Driven Development** = The workflow that ensures product thinking happens before code generation\n\nWhen an engineer writes a Spec, they're forced to answer:\n- What user problem does this solve?\n- What are the acceptance criteria?\n- Which edge cases matter and which don't?\n- What are the non-functional requirements (performance, security, observability)?\n\nIf they can't answer these questions, they don't understand the product problem yet. Vibe coding without this foundation produces code that works but solves the wrong problem.\n\n**The ASDLC position:**\n- AI agents execute maneuvers (implementation)\n- Human engineers provide strategic judgment (product thinking)\n- Specs encode that judgment in machine-readable form\n- Context Gates enforce that specs were actually written\n\nThis is the \"Instructor-in-the-Cockpit\" model: the pilot (AI) flies the plane, but the instructor (human) decides where to fly and evaluates whether the flight is safe.\n\nApplied in:\n- [Spec-Driven Development](/concepts/spec-driven-development) — Product thinking as prerequisite to code generation\n- [The Spec](/patterns/the-spec) — The artifact that captures product thinking\n- [Vibe Coding](/concepts/vibe-coding) — The failure mode when product thinking is skipped\n\n## Best Practices\n\n**For Individual Engineers:**\n1. Before writing code, write the \"why\" in plain English\n2. Question requirements that don't explain user impact\n3. Propose alternatives when you see tradeoff mismatches\n4. Treat AI-generated code skeptically: Does it solve the right problem?\n\n**For Engineering Leaders:**\n1. Share business context, even when it feels like \"too much detail\"\n2. Reward engineers who challenge bad requirements, not just those who ship fast\n3. Make \"why\" documentation non-optional (use Specs or equivalent)\n4. Measure outcomes (user adoption, retention, error rates) not just velocity (story points)\n\n**For Organizations:**\n1. Flatten decision-making: trust engineers to own tradeoffs in their domain\n2. Train product thinking explicitly (it's not intuitive for engineers trained to \"just code\")\n3. Create feedback loops: engineers see how their code impacts users\n4. Recognize that AI scales implementation, not judgment—invest in the latter\n\n## Anti-Patterns\n\n**\"Just Build It\" Culture:**\nEngineers discouraged from asking \"why\" or proposing alternatives. Leads to technically correct code that solves the wrong problem.\n\n**Context Hoarding:**\nProduct managers or architects hold all context and dole out tasks. Creates dependency bottleneck and prevents engineers from exercising judgment.\n\n**Velocity Worship:**\nSuccess measured by tickets closed, not problems solved. Optimizes for speed of wrong solutions.\n\n**\"Stay In Your Lane\" Enforcement:**\nEngineers punished for thinking beyond their assigned component. Prevents system-level thinking required for good product decisions.\n\nSee also:\n- [Industry Alignment](/resources/industry-alignment) — External voices on the product thinking shift\n- [Spec-Driven Development](/concepts/spec-driven-development) — How ASDLC encodes product thinking\n- [Vibe Coding](/concepts/vibe-coding) — What happens when product thinking is absent",
    "tags": ["Product Management", "Engineering Culture", "AI Era"],
    "references": [
      {
        "type": "book",
        "title": "Product Driven: Creating Products Customers Love Through Product-Led Growth",
        "author": "Matt Watson",
        "published": "2023-01-01T00:00:00.000Z",
        "annotation": "Comprehensive framework for product-thinking engineers, defining how to build products customers actually want."
      },
      {
        "type": "website",
        "title": "Vibe coders perform better than the average software engineer",
        "url": "https://www.linkedin.com/posts/mattwatson_vibe-coders-perform-better-than-the-average-activity-7286106547847921664-8Xvr",
        "author": "Matt Watson",
        "published": "2026-01-15T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "LinkedIn post arguing that vibe coders succeed because traditional software training missed product thinking fundamentals."
      },
      {
        "type": "website",
        "title": "Product Thinking Frameworks",
        "url": "https://www.linkedin.com/in/shreyasdoshi/",
        "author": "Shreyas Doshi",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Mental models and frameworks for evaluating product decisions from a product management perspective."
      },
      {
        "type": "book",
        "title": "Empowered: Ordinary People, Extraordinary Products",
        "author": "Marty Cagan",
        "published": "2020-12-03T00:00:00.000Z",
        "annotation": "Defining the difference between product teams (empowered to solve problems) and feature teams (told what to build)."
      }
    ]
  },
  {
    "slug": "spec-driven-development",
    "collection": "concepts",
    "title": "Spec-Driven Development",
    "description": "Industry term for methodologies that define specifications before code—implemented in ASDLC through the Specs pattern.",
    "status": "Live",
    "content": "## Definition\n\n**Spec-Driven Development (SDD)** is an umbrella term for methodologies that define specifications before implementation. The core inversion: instead of code serving as the source of documentation, the spec becomes the authority that code must fulfill.\n\n> **Contrast:** For the anti-pattern SDD addresses, see [Vibe Coding](/concepts/vibe-coding).\n\n## Industry Context\n\nSDD emerged as a response to documentation decay in software projects. Traditional approaches treated specs as planning artifacts that diverged from reality post-implementation. Modern SDD treats specs as **living documents** co-located with code.\n\nKent Beck critiques SDD implementations that assume \"you aren't going to learn anything during implementation.\" This is a valid concern—specs must evolve during implementation, not block it.\n\n> [!WARNING]\n> **The Figma Trap**\n> A beautiful mockup is not a specification; it is a suggestion. Mockups typically demonstrate the \"happy path\" but hide the edge cases, error states, and data consistency strictures where production bugs live.\n>\n> **Never** treat a visual design as a complete technical requirement.\n\n## ASDLC Implementation\n\nASDLC implements Spec-Driven Development through:\n\n- **[The Specs Pattern](/patterns/the-spec)** — The structural blueprint defining what a spec contains (Blueprint + Contract) and how it relates to PBIs\n- **[Living Specs Practice](/practices/living-specs)** — How to create, maintain, and evolve specs alongside code\n- **[The Learning Loop](/concepts/learning-loop)** — The iterative cycle that addresses Beck's critique\n\nFor step-by-step guidance, see [Living Specs](/practices/living-specs).",
    "tags": ["Disambiguation", "Methodology", "Industry Term"],
    "references": [
      {
        "type": "website",
        "title": "Martin Fowler Fragment: January 8, 2026",
        "url": "https://martinfowler.com/fragments/2026-01-08.html",
        "author": "Martin Fowler",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Commentary on Anthropic research and Kent Beck's critique of spec-driven approaches."
      },
      {
        "type": "website",
        "title": "Kent Beck on Spec-Driven Development",
        "url": "https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/",
        "author": "Kent Beck",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Critique emphasizing that specifications must accommodate learning during implementation."
      }
    ]
  },
  {
    "slug": "vibe-coding",
    "collection": "concepts",
    "title": "Vibe Coding",
    "description": "Natural language code generation without formal specs—powerful for prototyping, problematic for production systems.",
    "status": "Experimental",
    "content": "## Definition\n\nVibe Coding is the practice of generating code directly from natural language prompts without formal specifications, schemas, or contracts. Coined by Andrej Karpathy, the term describes an AI-assisted development mode where engineers describe desired functionality conversationally (\"make this faster,\" \"add a login button\"), and the LLM produces implementation code.\n\nThis approach represents a fundamental shift: instead of writing specifications that constrain implementation, developers describe intent and trust the model to infer the details. The result is rapid iteration—code appears almost as fast as you can articulate what you want.\n\nWhile vibe coding accelerates prototyping and exploration, it inverts traditional software engineering rigor: the specification emerges *after* the code, if at all.\n\n## The Seduction of Speed\n\nThe productivity gains from vibe coding are undeniable:\n\n* **At Anthropic:** 80-90% of Claude Code's codebase is now written by Claude Code itself, with a 70% productivity increase per engineer since adoption.\n* **At Google:** Approximately 30% of code committed in 2024 was AI-generated.\n* **Industry-wide:** Engineers report 2-10x faster feature delivery for greenfield projects and prototypes.\n\nThis velocity is seductive. When a feature that previously took three days can be scaffolded in thirty minutes, the economic pressure to adopt vibe coding becomes overwhelming.\n\nThe feedback loop is immediate: describe the behavior, see the code, run it, iterate. For throwaway scripts, MVPs, and rapid exploration, this workflow is transformative.\n\n## The Failure Modes\n\nThe velocity advantage of vibe coding collapses when code must be maintained, extended, or integrated into production systems:\n\n### Technical Debt Accumulation\n\n**Forrester Research predicts that by 2026, 75% of technology leaders will face moderate-to-severe technical debt** directly attributable to AI-generated code. The mechanism is straightforward: code generated from vague prompts encodes vague assumptions.\n\nWhen specifications exist only in the prompt history (or the engineer's head), future maintainers inherit code without contracts. They must reverse-engineer intent from implementation—the exact problem formal specifications solve.\n\n### Copy-Paste Culture\n\n2024 marked the first year in industry history where **copy-pasted code exceeded refactored code**. This is a direct symptom of vibe coding: when generating fresh code is faster than understanding existing code, engineers default to regeneration over refactoring.\n\nThe result is systemic duplication. The same logic appears in fifteen places with fifteen slightly different implementations, none validated against a shared contract.\n\n### Silent Drift\n\nLLMs are probabilistic. When generating code from vibes, they make assumptions:\n- Error handling strategies (fail silently? throw? log?)\n- Data validation rules (what's a valid email?)\n- Concurrency models (locks? optimistic? eventual consistency?)\n\nThese assumptions are *never documented*. The code passes tests (if tests exist), but violates implicit architectural contracts. Over time, the system drifts toward inconsistency—different modules make different assumptions about the same concepts.\n\nBoris Cherny (Principal Engineer, Anthropic; creator of Claude Code) warns: **\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes.\"**\n\n> **\"Speed is seductive. Maintainability is survival.\"**  \n> — Boris Cherny, *The Peterman Podcast* (December 2025)\n\n### Vibe Coded Into a Corner\n\nAnthropic's internal research found that engineers who spend *more* time on Claude-assisted tasks often do so because they \"vibe code themselves into a corner\"—generating code without specs until debugging and cleanup overhead exceeds the initial velocity gains.\n\n> \"When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.\"\n> — Anthropic engineer\n\nThis creates a debt spiral: vibe coding is fast until it isn't, and by then the context needed to fix issues was never documented.\n\n### Regression to the Mean\n\nWithout deterministic constraints, LLMs trend toward generic solutions. Vibe coding produces code that works but lacks the specific optimizations, domain constraints, and architectural decisions that distinguish production systems from prototypes.\n\nThe model doesn't know that \"user IDs must never be logged\" or \"this cache must invalidate within 100ms.\" These constraints exist in specifications, not prompts.\n\n## Applications\n\nVibe coding is particularly effective in specific contexts:\n\n**Rapid Prototyping:** When validating product hypotheses, speed of iteration outweighs code quality. Vibe coding enables designers and product managers to generate functional prototypes without deep programming knowledge.\n\n**Throwaway Scripts:** One-off data migrations, analysis scripts, and temporary tooling benefit from vibe coding's velocity. Since the code has no maintenance burden, formal specifications are unnecessary overhead.\n\n**Learning and Exploration:** When experimenting with new APIs, frameworks, or architectural patterns, vibe coding provides immediate feedback. The goal is understanding, not production-ready code.\n\n**Greenfield MVPs:** Early-stage startups building minimum viable products often prioritize speed-to-market over maintainability. Vibe coding accelerates this phase, though technical debt must be managed during the transition to production.\n\n## ASDLC Usage\n\nIn ASDLC, vibe coding is recognized as a legitimate operational mode for bounded contexts (exploration, prototyping, throwaway code). However, for production systems, ASDLC mandates a transition to deterministic development.\n\n**The ASDLC position:**\n- Vibe coding is **steering** (probabilistic guidance via prompts)\n- Production requires **determinism** (schemas, tests, typed interfaces)\n- Both are necessary: prompts steer the agent; schemas enforce correctness\n\nApplied in:\n- [Spec-Driven Development](/concepts/spec-driven-development) — The production-grade alternative to vibe coding\n- [Context Gates](/patterns/context-gates) — Deterministic enforcement layer\n- [Levels of Autonomy](/concepts/levels-of-autonomy) — Human oversight model (L3: \"Hands Off, Eyes On\")\n\nSee also:\n- [Industry Alignment](/resources/industry-alignment) — External voices converging on ASDLC principles\n- [Spec-Driven Development](/concepts/spec-driven-development) — ASDLC's production-grade methodology\n- [Context Gates](/patterns/context-gates) — Deterministic enforcement layer",
    "tags": ["Disambiguation", "AI", "Code Quality", "Anti-Pattern"],
    "references": [
      {
        "type": "podcast",
        "title": "Claude Code and the Future of AI-Assisted Development",
        "url": "https://peterman.fm/boris-cherny",
        "author": "Boris Cherny",
        "publisher": "The Peterman Podcast",
        "published": "2025-12-01T00:00:00.000Z",
        "annotation": "Claude Code creator's framework for disciplined AI-assisted development, discussing the balance between automation and rigor."
      },
      {
        "type": "website",
        "title": "Forrester Research on AI-Generated Code Technical Debt",
        "author": "Forrester Research",
        "published": "2024-01-01T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Technical debt predictions and analysis for AI-generated code in production systems."
      },
      {
        "type": "website",
        "title": "Google's AI-Generated Code Adoption Metrics",
        "author": "Google",
        "published": "2024-01-01T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Industry data on Google's 30% AI-generated code adoption rate and analysis of copy-paste versus refactor patterns."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Research showing engineers vibe code themselves into corners, with cleanup overhead exceeding initial velocity."
      }
    ]
  },
  {
    "slug": "yaml",
    "collection": "concepts",
    "title": "YAML",
    "description": "A human-readable data serialization language that serves as the structured specification format for configuration, schemas, and file structures in agentic workflows.",
    "status": "Live",
    "content": "## Definition\n\nYAML (YAML Ain't Markup Language) is a human-readable data serialization language designed for configuration files, data exchange, and structured documentation. In agentic development, YAML serves as the specification language for data structures, schemas, and file organization.\n\nWhere [Gherkin](/concepts/gherkin) specifies *behavior* (Given-When-Then), YAML specifies *structure* (keys, values, hierarchies). Both are human-readable formats that bridge the gap between human intent and machine execution.\n\n## Key Characteristics\n\n### Human-Readable Structure\n\nYAML's indentation-based syntax mirrors how humans naturally organize hierarchical information:\n\n```yaml\nnotification:\n  channels:\n    - websocket\n    - email\n    - sms\n  constraints:\n    latency_ms: 100\n    retry_count: 3\n  fallback:\n    enabled: true\n    order: [websocket, email, sms]\n```\n\n### Schema-First Design\n\nYAML enables schema-first development where data structures are defined before implementation:\n\n```yaml\n# Schema definition in spec\nuser:\n  id: string (UUID)\n  email: string (email format)\n  roles: array of enum [admin, user, guest]\n  created_at: datetime (ISO 8601)\n```\n\nAgents can validate implementations against these schemas, catching type mismatches and missing fields before runtime.\n\n### Configuration as Code\n\nYAML configurations live in version control alongside code, enabling:\n- **Diff visibility** — Configuration changes appear in PRs\n- **Review process** — Same rigor as code changes\n- **History tracking** — Git blame shows who changed what and when\n\n## YAML in ASDLC\n\n### Spec Frontmatter\n\nAll ASDLC articles use YAML frontmatter for structured metadata:\n\n```yaml\n---\ntitle: \"Feature Name\"\nstatus: \"Live\"\nlastUpdated: 2026-01-12\nrelatedIds:\n  - patterns/the-spec\n  - concepts/context-engineering\n---\n```\n\n### AGENTS.md Configuration\n\nThe [AGENTS.md Specification](/practices/agents-md-spec) uses YAML for structured directives:\n\n```yaml\n# Project context\nproject:\n  name: \"asdlc-io\"\n  type: \"astro-site\"\n  \n# Agent constraints\nconstraints:\n  - Never delete files without explicit instruction\n  - Prefer composition over inheritance\n```\n\n### File Structure Specifications\n\nYAML defines expected directory layouts and file patterns:\n\n```yaml\nsrc/\n  content/\n    concepts/: \"*.md\"\n    patterns/: \"*.md\"\n    practices/: \"*.md\"\n  components/: \"*.astro\"\n  styles/ds/: \"*.css\"\n```\n\n## ASDLC Usage\n\nYAML serves as the **data structure specification language** in ASDLC, complementing:\n\n- **[Gherkin](/concepts/gherkin)** — Specifies behavior (what happens)\n- **YAML** — Specifies structure (what exists)\n- **Mermaid** — Specifies process (how it flows)\n\nApplied in:\n- [The Spec](/patterns/the-spec) — Frontmatter and schema definitions\n- [AGENTS.md Specification](/practices/agents-md-spec) — Agent configuration\n- [Context Engineering](/concepts/context-engineering) — Structured context formats\n\n## Anti-Patterns\n\n| Anti-Pattern | Description | Failure Mode |\n|--------------|-------------|--------------|\n| **Indentation Errors** | Mixing tabs and spaces | Parse failures; silent data corruption |\n| **Stringly Typed** | Using strings where structured types exist | No validation; runtime errors |\n| **Implicit Types** | Relying on YAML's type inference | \"no\" becomes boolean false; unexpected behavior |\n| **Deep Nesting** | Excessive hierarchy depth | Unreadable; difficult to navigate |\n| **No Schema** | YAML without validation schema | Agents hallucinate invalid structures |\n\n## Best Practices\n\n1. **Explicit Types** — Use quotes for strings that look like other types: `version: \"1.0\"`\n2. **Flat When Possible** — Prefer flat structures over deep nesting\n3. **Comments** — Document non-obvious fields inline\n4. **Schema Validation** — Pair YAML with JSON Schema or Zod for type safety\n5. **Consistent Formatting** — Use tools like Prettier for consistent style",
    "tags": ["Data", "Configuration", "Specification", "Syntax"],
    "references": [
      {
        "type": "website",
        "title": "YAML Specification",
        "url": "https://yaml.org/spec/1.2.2/",
        "author": "YAML Language Development Team",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Official YAML 1.2.2 specification defining syntax and semantics."
      }
    ]
  },
  {
    "slug": "adversarial-code-review",
    "collection": "patterns",
    "title": "Adversarial Code Review",
    "description": "Consensus verification pattern using a secondary Critic Agent to review Builder Agent output against the Spec.",
    "status": "Experimental",
    "content": "## Definition\n\n**Adversarial Code Review** is a verification pattern where a distinct AI session—the **Critic Agent**—reviews code produced by the **Builder Agent** against the [Spec](/patterns/the-spec) before human review.\n\nThis extends the [Critic (Hostile Agent)](/patterns/agentic-double-diamond) pattern from the design phase into the implementation phase, creating a verification checkpoint that breaks the \"echo chamber\" where a model validates its own output.\n\nThe Builder Agent (optimized for speed and syntax) generates code. The Critic Agent (optimized for reasoning and logic) attempts to reject it based on spec violations.\n\n## The Problem: Self-Validation Ineffectiveness\n\nLLMs are probabilistic text generators trained to be helpful. When asked \"Check your work,\" a model that just generated code will often:\n\n**Hallucinate correctness** — Confidently affirm that buggy logic is correct because it matches the plausible pattern in training data.\n\n**Double down on errors** — Explain why the bug is actually a feature, reinforcing the original mistake.\n\n**Share context blindness** — Miss gaps because it operates within the same context window and reasoning path that produced the original output.\n\nIf the same computational session writes and reviews code, the \"review\" provides minimal independent validation.\n\n## The Solution: Separated Roles\n\nTo create effective verification, separate the generation and critique roles:\n\n**The Builder** — Optimizes for implementation throughput (e.g., Gemini 3 Flash, Claude Haiku 4.5). Generates code from the PBI and Spec.\n\n**The Critic** — Optimizes for logical consistency and constraint satisfaction (e.g., Gemini 3 Deep Think, DeepSeek V3.2). Validates code against Spec contracts without rewriting.\n\nThe Critic does not generate alternative implementations. It acts as a gatekeeper, producing either **PASS** or a list of **spec violations** that must be addressed.\n\n## The Workflow\n\n### 1. Build Phase\n\nThe Builder Agent implements the PBI according to the Spec.\n\n**Output:** Code changes, implementation notes.\n\n**Example:** \"Updated `auth.ts` to support OAuth login flow.\"\n\n### 2. Context Swap (Fresh Eyes)\n\n**Critical:** Start a new AI session or chat thread for critique. This clears conversation drift and forces the Critic to evaluate only the artifacts (Spec + Diff), not the Builder's reasoning process.\n\nIf using the same model, close the current chat and open a fresh session. If using [Model Routing](/patterns/model-routing), switch to a High Reasoning model.\n\n### 3. Critique Phase\n\nFeed the Spec and the code diff to the Critic Agent with adversarial framing:\n\n**System Prompt:**\n```\nYou are a rigorous Code Reviewer validating implementation against contracts.\n\nInput:\n- Spec: specs/auth-system.md\n- Code Changes: src/auth.ts (diff)\n\nTask:\nCompare the code strictly against the Spec's Blueprint (constraints) and Contract (quality criteria).\n\nIdentify:\n1. Spec violations (missing requirements, violated constraints)\n2. Security issues (injection vulnerabilities, auth bypasses)\n3. Edge cases not handled (error paths, race conditions)\n4. Anti-patterns explicitly forbidden in the Spec\n\nOutput Format:\n- PASS (if no violations)\n- For each violation, provide:\n  1. Violation Description (what contract was broken)\n  2. Impact Analysis (why this matters: performance, security, maintainability)\n  3. Remediation Path (ordered list of fixes, prefer standard patterns, escalate if needed)\n  4. Test Requirements (what tests would prevent regression)\n\nThis transforms critique from \"reject\" to \"here's how to fix it.\"\n```\n\n### 4. Verdict\n\n**If PASS:** Code moves to human Acceptance Gate (L3 review for strategic fit).\n\n**If FAIL:** Violations are fed back to Builder as a new task: \"Address these spec violations before proceeding.\"\n\nThis creates a [Context Gate](/patterns/context-gates) between code generation and human review.\n\n## Relationship to Context Gates\n\nAdversarial Code Review implements a **Review Gate** as defined in [Context Gates](/patterns/context-gates):\n\n**Quality Gates** (deterministic) — Verify syntax, compilation, linting, test passage.\n\n**Review Gates** (probabilistic, adversarial) — Verify semantic correctness, spec compliance, architectural consistency. **This is where Adversarial Code Review operates.**\n\n**Acceptance Gates** (subjective, HITL) — Verify strategic fit and product vision alignment.\n\nThe Critic sits between automated tooling and human review, catching issues that compilers miss but that don't require human strategic judgment.\n\n## Integration with Model Routing\n\nUse [Model Routing](/patterns/model-routing) to assign models by capability profile:\n\n| Role | Model Profile | Rationale |\n|------|---------------|-----------|\n| Builder | High Throughput | Fast code generation with strong syntax knowledge |\n| Critic | High Reasoning | Deep logic evaluation, constraint satisfaction, edge case discovery |\n\nThis leverages the strengths of each model class: speed for generation, reasoning depth for validation.\n\n## Strategic Value\n\n**Reduces L3 Cognitive Load** — Human reviewers focus on \"Is this the right product?\" rather than catching spec deviations or missing error handling.\n\n**Catches Regression to Mediocrity** — Coding models gravitate toward average solutions. The Critic enforces novelty and architectural intent from the Spec.\n\n**Enforces Spec Quality** — If the Critic can't determine whether code is correct, the Spec is ambiguous. This surfaces specification gaps.\n\n**Prevents Silent Failures** — The Critic catches implementation shortcuts (skipped validation, missing edge cases) that pass tests but violate contracts.\n\n## Validated in Practice\n\n**Case Study: Claudio Lassala (January 2026)**\n\nA production implementation validated this pattern's effectiveness:\n\n**Context:** A user story required filtering audit logs by date range. The Builder Agent implemented the requirement, tests passed, and the code compiled without errors.\n\n**Issue Detected:** The Critic Agent identified a silent performance violation:\n\n```csharp\n// Implementation passed all Quality Gates but violated architectural constraint\nvar logs = await repository.LoadAll(); // Loads entire table into memory\nreturn logs.Where(log => log.Date > startDate); // Filters in-memory\n```\n\n**Critic Output:**\n```\nVIOLATION: Performance - Data Access Pattern\n\nSpec requires database-level filtering for datasets exceeding 1k records.\nImplementation loads full table then filters in-memory.\n\nImpact: Works with small data; fails at scale (10k+ records cause memory issues)\n\nRemediation Path:\n1. Push filtering to database: repository.FindWhere(x => x.Date > startDate)\n2. If ORM doesn't support this filter, use raw SQL\n3. Add performance test with 10k+ mock records to prevent regression\n4. Document the constraint in the repository interface\n```\n\n**Key Learnings:**\n\n1. **Silent Performance Risks** — Code that passes all tests can still violate architectural constraints. The Critic caught the `LoadAll().Filter()` anti-pattern before production.\n\n2. **Iterative Refinement** — The Critic initially flagged \"missing E2E tests,\" which were actually present but structured differently. The team updated the Critic's instructions to recognize the project's test architecture, demonstrating the pattern's adaptability.\n\n3. **Tone Calibration** — Using \"Approve with suggestions\" framing prevented blocking valid code while surfacing genuine risks. The Critic didn't reject the PR—it flagged optimization opportunities with clear remediation paths.\n\nThis validates the pattern's core thesis: adversarial review catches architectural violations that pass deterministic checks but violate semantic contracts.\n\n## Example: The Silent Performance Bug\n\n**Spec Contract:** \"All database retries must use exponential backoff to prevent thundering herd during outages.\"\n\n**Builder Output:** Clean code with a simple retry loop using fixed 1-second delays. Tests pass.\n\n```typescript\n// src/db.ts\nasync function queryWithRetry(sql: string) {\n  for (let i = 0; i < 5; i++) {\n    try {\n      return await db.query(sql);\n    } catch (err) {\n      await sleep(1000); // Fixed delay\n    }\n  }\n}\n```\n\n**Critic Response:**\n```\nVIOLATION: src/db.ts Line 45\n\nSpec requires exponential backoff. Implementation uses constant sleep(1000).\n\nImpact: During database outages, this will cause thundering herd problems\nas all clients retry simultaneously.\n\nRequired: Implement delay = baseDelay * (2 ** attemptNumber)\n```\n\nWithout the Critic, a human skimming the PR might miss the constant delay. The automated tests wouldn't catch it (the code works). The Critic, reading against the contract, identifies the violation.\n\n## Implementation Constraints\n\n**Not Automated (Yet)** — As of December 2025, this requires manual orchestration. Engineers must manually switch sessions/models and feed context to the Critic.\n\n**Context Window Limits** — Large diffs may exceed even Massive Context models. Use [Context Gates](/patterns/context-gates) filtering to provide only changed files + relevant Spec sections.\n\n**Critic Needs Clear Contracts** — The Critic can only enforce what's documented in the Spec. Vague specs produce vague critiques.\n\n**Model Capability Variance** — Not all \"reasoning\" models perform equally at code review. Validate your model's performance on representative examples.\n\n## Relationship to Agent Constitution\n\nThe [Agent Constitution](/patterns/agent-constitution) defines behavioral directives for agents. For Adversarial Code Review:\n\n**Builder Constitution:** \"Implement the Spec's contracts. Prioritize clarity and correctness over cleverness.\"\n\n**Critic Constitution:** \"You are skeptical. Your job is to reject code that violates the Spec, even if it 'works.' Favor false positives over false negatives.\"\n\nThis frames the Critic's role as adversarial by design—it's explicitly told to be rigorous and skeptical, counterbalancing the Builder's helpfulness bias.\n\n## Future Automation Potential\n\nThis pattern is currently manual but has clear automation paths:\n\n**CI/CD Integration** — Run Critic automatically on PR creation, posting violations as review comments.\n\n**IDE Integration** — Real-time critique as code is written, similar to linting but spec-aware.\n\n**Multi-Agent Orchestration** — Automated handoff between Builder and Critic until PASS is achieved.\n\nAs agent orchestration tooling matures, this pattern may move from Experimental to Standard.\n\nSee also:\n- [Context Gates](/patterns/context-gates) — The architectural checkpoint pattern this implements\n- [The Spec](/patterns/the-spec) — The source of truth the Critic validates against\n- [Model Routing](/patterns/model-routing) — How to assign different models to Builder and Critic roles\n- [Agentic Double Diamond](/patterns/agentic-double-diamond) — The design-phase Critic pattern this extends\n- [Agent Constitution](/patterns/agent-constitution) — How to frame Critic behavior as adversarial\n\n### Related Concepts\n- [Agentic SDLC](/concepts/agentic-sdlc) — The Verification phase where this pattern operates\n- [Levels of Autonomy](/concepts/levels-of-autonomy) — L3 autonomy requires verification before human review",
    "tags": ["Code Review", "Quality Gates", "Multi-Agent", "Verification", "Context Engineering"],
    "references": [
      {
        "type": "website",
        "title": "A Method for AI-Assisted Pull Request Reviews: Aligning Code with Business Value",
        "url": "https://lassala.net/2026/01/05/a-method-for-ai-assisted-pull-request-reviews-aligning-code-with-business-value/",
        "author": "Claudio Lassala",
        "published": "2026-01-05T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Production implementation validating the pattern's effectiveness in catching silent performance bugs and architectural violations."
      }
    ]
  },
  {
    "slug": "agent-constitution",
    "collection": "patterns",
    "title": "Agent Constitution",
    "description": "Prime directives that align agent behavior with system goals before action, acting as driver training vs. context gates as brakes.",
    "status": "Experimental",
    "content": "## The Prime Directives\n\nAn Agent Constitution is a set of \"Prime Directives\" injected into an agent's context window to align its intent and behavior with high-level principles. Unlike specific task instructions, the constitution provides the ethical and operational boundaries within which the agent must operate. It serves as the foundational \"superego\" for the agent, ensuring that even in ambiguous situations, its actions remain consistent with the system's overarching goals and safety guidelines.\n\n## Driver Training vs. Brakes\n\nTo understand the role of an Agent Constitution, consider the analogy of a car. Context Gates and other restrictive mechanisms act as the \"brakes\"—they stop the agent from doing something wrong after it has already attempted or formulated an action. The Agent Constitution, however, is the \"driver training.\" It shapes the agent's decision-making process *before* it even considers an action. By internalizing these rules, the agent is less likely to need the \"brakes\" because it is steering itself correctly from the start.\n\n## Implementation\n\nFor the standard implementation of this pattern, see the [AGENTS.md Specification](/practices/agents-md-spec). The specification details how to formally document and inject these directives into your agentic workflows.",
    "tags": [],
    "references": []
  },
  {
    "slug": "constitutional-review",
    "collection": "patterns",
    "title": "Constitutional Review",
    "description": "Verification pattern that validates implementation against both functional requirements (Spec) and architectural values (Constitution).",
    "status": "Experimental",
    "content": "## Definition\n\n**Constitutional Review** is a verification pattern that validates code against two distinct contracts:\n\n1. **The Spec** (functional requirements) — Does it do what was asked?\n2. **The Constitution** (architectural values) — Does it do it *the right way*?\n\nThis pattern extends [Adversarial Code Review](/patterns/adversarial-code-review) by adding a second validation layer. Code can pass all tests and satisfy the Spec's functional requirements while still violating the project's architectural principles documented in the [Agent Constitution](/patterns/agent-constitution).\n\n## The Problem: Technically Correct But Architecturally Wrong\n\nStandard verification catches functional bugs:\n- **Tests**: Does the code produce expected outputs?\n- **Spec Compliance**: Does it implement all requirements?\n- **Type Safety**: Does it compile without errors?\n\nBut code can pass all these checks and still violate architectural constraints:\n\n**Example: The Performance Violation**\n\n```typescript\n// Spec requirement: \"Filter audit logs by date range\"\nasync function getAuditLogs(startDate: Date) {\n  const logs = await db.auditLogs.findAll(); // ❌ Loads entire table\n  return logs.filter(log => log.date > startDate); // ❌ Filters in memory\n}\n```\n\n**Quality Gates**: ✅ Tests pass (small dataset)  \n**Spec Compliance**: ✅ Returns filtered logs  \n**Constitutional Review**: ❌ Violates \"push filtering to database layer\"\n\nThe code is **functionally correct** but **architecturally unsound**. It works fine with 100 records but fails catastrophically at 10,000+.\n\n## The Solution: Dual-Contract Validation\n\nConstitutional Review solves this by validating against **two sources of truth**:\n\n### Traditional Review (Functional)\n- **Input**: Spec + Code Diff\n- **Question**: \"Does the code implement the requirements?\"\n- **Validates**: Functional correctness\n\n### Constitutional Review (Architectural)\n- **Input**: Constitution + Spec + Code Diff\n- **Question**: \"Does the code exhibit our architectural values?\"\n- **Validates**: Architectural consistency\n\nThe Critic Agent validates against BOTH contracts:\n1. **Functional correctness** (from the Spec)\n2. **Architectural consistency** (from the Constitution)\n\n## Anatomy\n\nConstitutional Review consists of three key components:\n\n### The Dual-Contract Input\n\n**Spec Contract** — Defines functional requirements, API contracts, and data schemas. Answers \"what should it do?\"\n\n**Constitution Contract** — Defines architectural patterns, performance constraints, and security rules. Answers \"how should it work?\"\n\nBoth contracts are fed to the Critic Agent for validation.\n\n### The Critic Agent\n\nA secondary AI session (ideally using a reasoning-optimized model) that:\n\n- Reads both the Spec and the Constitution\n- Compares implementation against both contracts\n- Identifies where code satisfies functional requirements but violates architectural principles\n- Provides structured violation reports with remediation paths\n\nThis extends the [Adversarial Code Review](/patterns/adversarial-code-review) Critic with constitutional awareness.\n\n### The Violation Report\n\nWhen constitutional violations are detected, the Critic produces:\n\n1. **Violation Description** — What constitutional principle was violated\n2. **Impact Analysis** — Why this matters at scale (performance, security, maintainability)\n3. **Remediation Path** — Ordered steps to fix (prefer standard patterns, escalate if needed)\n4. **Test Requirements** — What tests would prevent regression\n\nThis transforms review from rejection to guidance.\n\n## Relationship to Other Patterns\n\n**[Adversarial Code Review](/patterns/adversarial-code-review)** — The base pattern that Constitutional Review extends. Adds the Constitution as a second validation contract.\n\n**[Agent Constitution](/patterns/agent-constitution)** — The source of architectural truth. Defines the \"driver training\" that shapes initial behavior; Constitutional Review verifies the training was followed.\n\n**[The Spec](/patterns/the-spec)** — The source of functional truth. Constitutional Review validates against both Spec and Constitution.\n\n**[Context Gates](/patterns/context-gates)** — Constitutional Review implements a specialized Review Gate that validates architectural consistency.\n\n**Feedback Loop**: Constitution shapes behavior → Constitutional Review catches violations → Violations inform Constitution updates (if principles aren't clear enough).\n\n## Integration with Context Gates\n\nConstitutional Review implements a specialized [Review Gate](/patterns/context-gates) that sits between Quality Gates and Acceptance Gates:\n\n| Gate Type | Question | Validated By |\n|-----------|----------|--------------|\n| Quality Gates | Does it compile and pass tests? | Toolchain (deterministic) |\n| Spec Review Gate | Does it implement requirements? | Critic Agent (probabilistic) |\n| **Constitutional Review Gate** | **Does it follow principles?** | **Critic Agent (probabilistic)** |\n| Acceptance Gate | Is it the right solution? | Human (subjective) |\n\nThe Constitutional Review Gate catches architectural violations that pass functional verification.\n\n## Strategic Value\n\n**Catches \"Regression to Mediocrity\"** — LLMs are trained on average code from the internet. Without constitutional constraints, they gravitate toward common but suboptimal patterns.\n\n**Enforces Institutional Knowledge** — Architectural decisions (performance patterns, security rules, error handling strategies) are documented once in the Constitution and verified on every implementation.\n\n**Surfaces Specification Gaps** — If the Critic can't determine whether code violates constitutional principles, the Constitution needs clarification. This improves the entire system.\n\n**Reduces L3 Review Burden** — Human reviewers focus on strategic fit (\"Is this the right feature?\") rather than catching architectural violations (\"Why are you loading the entire table?\").\n\n**Prevents Silent Failures** — Code that \"works\" but violates architectural principles (like the LoadAll().Filter() anti-pattern) is caught before production.\n\n## Validated in Practice\n\n**Case Study: Claudio Lassala (January 2026)**\n\nA production implementation caught a constitutional violation that passed all other gates:\n\n**Context**: User story required filtering audit logs by date range. Builder Agent implemented the requirement, tests passed, code compiled without errors.\n\n**Code Behavior**:\n- Loaded entire audit log table into memory\n- Filtered in-memory using LINQ/collection methods\n\n**Gate Results**:\n- **Quality Gates**: ✅ Passed (compiled, tests passed with small dataset)\n- **Spec Compliance**: ✅ Passed (functional requirement met: returns filtered logs)\n- **Constitutional Review**: ❌ **FAILED** (violated \"push filtering to database layer\")\n\n**Critic Output**: Provided specific remediation path:\n1. Push filter to database query layer\n2. If ORM doesn't support pattern, use raw SQL\n3. Add performance test with 10k+ records\n4. Document constraint in repository interface\n\n**Impact**: Silent performance bug caught before production. The code worked perfectly in development (small dataset) but would have failed catastrophically at scale.\n\nSee full case study in [Adversarial Code Review](/patterns/adversarial-code-review).\n\n## Implementing Practice\n\nFor step-by-step implementation guidance, see:\n\n- [Constitutional Review Implementation](/practices/constitutional-review-implementation) — How to configure Critic Agent prompts, document architectural constraints, and integrate with your workflow\n\nSee also:\n- [Adversarial Code Review](/patterns/adversarial-code-review) — The base pattern this extends\n- [Agent Constitution](/patterns/agent-constitution) — The source of architectural truth\n- [The Spec](/patterns/the-spec) — The source of functional truth\n- [Context Gates](/patterns/context-gates) — The architectural checkpoint system\n- [Agentic SDLC](/concepts/agentic-sdlc) — The verification phase where this operates\n- [Context Engineering](/concepts/context-engineering) — How to structure constitutional constraints for LLMs",
    "tags": ["Code Review", "Architecture", "Agent Constitution", "Quality Gates", "Verification"],
    "references": [
      {
        "type": "website",
        "title": "A Method for AI-Assisted Pull Request Reviews: Aligning Code with Business Value",
        "url": "https://lassala.net/2026/01/05/a-method-for-ai-assisted-pull-request-reviews-aligning-code-with-business-value/",
        "author": "Claudio Lassala",
        "published": "2026-01-05T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Production validation showing constitutional violations caught after passing quality gates, demonstrating real-world effectiveness of this pattern."
      }
    ]
  },
  {
    "slug": "context-gates",
    "collection": "patterns",
    "title": "Context Gates",
    "description": "Architectural checkpoints that filter input context and validate output artifacts between phases of work to prevent cognitive overload and ensure system integrity.",
    "status": "Experimental",
    "content": "## Definition\n\n**Context Gates** are architectural checkpoints that sit between phases of agentic work. They serve a dual mandate: filtering the **input** context to prevent cognitive overload, and validating the **output** artifacts to ensure system integrity.\n\nUnlike \"Guardrails,\" which conflate prompt engineering with hard constraints, Context Gates are distinct, structural barriers that enforce contracts between agent sessions and phases.\n\n## The Problem: Context Pollution and Unvalidated Outputs\n\nWithout architectural checkpoints, agentic systems suffer from two critical failures:\n\n**Context Pollution** — Agents accumulate massive conversation histories (observations, tool outputs, internal monologues, errors). When transitioning between sessions or tasks, feeding the entire context creates cognitive overload. Signal-to-noise ratio drops, and agents lose focus on the current objective.\n\n**Unvalidated Outputs** — Code that passes automated tests can still violate semantic contracts (spec requirements, architectural constraints, security policies). Without probabilistic validation layers, implementation shortcuts and silent failures slip through to production.\n\n**Why Existing Approaches Fail:**\n- **Single-pass validation** (tests only) misses semantic violations\n- **No context compression** between sessions creates confusion\n- **Flat quality gates** don't distinguish deterministic checks from probabilistic review\n\n## The Solution: Dual-Mandate Checkpoint Architecture\n\nContext Gates solve this by creating **two distinct checkpoint types**:\n\n**Input Gates** — Filter and compress context *entering* an agent session, ensuring only relevant information is presented. This prevents cognitive overload and maintains task focus.\n\n**Output Gates** — Validate artifacts *leaving* an agent session through three tiers of verification: deterministic checks, probabilistic review, and human acceptance.\n\nThe key insight: **Context must be controlled at the boundaries**, not throughout execution. Agents work freely within their session, but transitions enforce strict contracts.\n\n## Anatomy\n\nContext Gates consist of two primary structures, each with distinct sub-components:\n\n### Input Gates\n\nInput Gates control what context enters an agent session.\n\n#### Summary Gates (Cross-Session Transfer)\nWhen transitioning work between agent sessions, Summary Gates compress conversation history into essential state.\n\n- **Type:** LLM-Assisted Summarization\n- **Nature:** Compression / Filtering\n- **Function:** Extract key decisions, discard intermediate reasoning\n- **Outcome:** Clean handoff without context overflow\n\n**Examples:**\n- Design session → Implementation: Extract design decisions, discard exploration paths\n- Bug investigation → Fix: Compress to \"root cause + attempted fixes\"\n- Code review → Revision: Distill to actionable feedback list\n\n#### Context Filtering (Within-Session)\nDuring multi-step tasks within a single session, Context Filtering determines what historical information is relevant to the current sub-task.\n\n- **Type:** Semantic Search / Lightweight Agent\n- **Nature:** Relevance Filtering\n- **Function:** High signal-to-noise ratio for current decision\n- **Outcome:** Precision and low latency\n\n### Output Gates\n\nOutput Gates validate artifacts before they progress to the next phase. Three tiers enforce different types of correctness:\n\n#### Quality Gates (Deterministic)\nBinary, automated checks enforced by the toolchain.\n\n- **Type:** Machine / Toolchain\n- **Nature:** Deterministic (Pass/Fail)\n- **Question:** \"Does it compile and pass tests?\"\n- **Enforcement:** Instant rejection if failed; often triggers self-correction\n\n**Examples:**\n- Syntax & type safety (TypeScript compilation, Zod validation)\n- Linting rules (ESLint, accessibility checks)\n- Unit/E2E test passage\n- Build artifact generation\n\n#### Review Gates (Probabilistic, Adversarial)\nLLM-assisted validation of semantic correctness and contract compliance.\n\n- **Type:** Secondary AI Session (Critic Agent)\n- **Nature:** Probabilistic / Adversarial\n- **Question:** \"Does it satisfy the Spec's contracts?\"\n- **Implementation:** [Adversarial Code Review](/patterns/adversarial-code-review), [Constitutional Review](/patterns/constitutional-review)\n\n**Examples:**\n- Spec compliance (all requirements implemented)\n- Anti-pattern detection (architectural constraint violations)\n- Edge case coverage (error paths, race conditions)\n- Security review (injection vulnerabilities, auth bypasses)\n\n**Output Format:**\nWhen violations are detected, Review Gates provide actionable feedback:\n\n1. **Violation Description** — What contract was broken\n2. **Impact Analysis** — Why this matters (performance, security, maintainability)\n3. **Remediation Path** — Ordered list of fixes (prefer standard patterns, escalate if needed)\n4. **Test Requirements** — What tests would prevent regression\n\nThis transforms Review Gates from \"reject\" mechanisms into \"guide to resolution\" checkpoints.\n\n#### Acceptance Gates (Human-in-the-Loop)\nSubjective checks requiring human strategic judgment.\n\n- **Type:** Human Review (HITL)\n- **Nature:** Subjective / Strategic\n- **Question:** \"Is it the right thing?\"\n- **Purpose:** Ensure solution solves actual user problem and aligns with product vision\n\n**Examples:**\n- Brand tone check (does copy sound like us?)\n- UX review (does interaction feel smooth?)\n- Visual QA (are spacings and layout visually balanced?)\n- Strategic fit (does this feature solve the user's problem?)\n\n## Gate Taxonomy\n\n| Feature | Summary Gates (Input) | Context Filtering (Input) | Quality Gates (Output) | Review Gates (Output) | Acceptance Gates (Output) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Function** | Session handoff | Within-session filtering | Code validity | Spec compliance | Strategic fit |\n| **Goal** | Clean session transfer | Maintain focus | Prevent broken code | Enforce contracts | Prevent bad product |\n| **Mechanism** | LLM Summarization | Semantic Search | Compilers / Tests | LLM Critique | Human Review |\n| **Nature** | Compression | Filtering | Deterministic | Probabilistic | Subjective |\n| **Outcome** | Condensed context | Clean context window | Valid compilation | Spec compliance | Approved release |\n\n## Relationship to Other Patterns\n\n**[Adversarial Code Review](/patterns/adversarial-code-review)** — Implements the Review Gate tier of Output Gates. Uses a Critic Agent to validate code against the Spec's contracts.\n\n**[Constitutional Review](/patterns/constitutional-review)** — Extends Review Gates by validating against both the Spec (functional) and the Agent Constitution (architectural values).\n\n**[Model Routing](/patterns/model-routing)** — Works with Context Gates to assign appropriate model capabilities to different gate types (throughput models for generation, reasoning models for Review Gates).\n\n**[The Spec](/patterns/the-spec)** — Provides the contract that Review Gates validate against.\n\n**[Agent Constitution](/patterns/agent-constitution)** — Provides architectural constraints that Constitutional Review validates against.\n\n## Implementing Practices\n\nThis pattern is implemented by:\n\n- **[Feature Assembly](/practices/feature-assembly)** — Uses all three Output Gates (Quality, Review, Acceptance) in the verification pipeline\n- **[Adversarial Code Review](/patterns/adversarial-code-review)** — Implements Review Gates using Critic Agents\n- **Context Handoff Practice** (TBD) — Would implement Summary Gates for session transitions\n\n## Strategic Value\n\n**Prevents Context Overload** — Agents receive only relevant information, maintaining task focus and reducing token usage.\n\n**Catches Semantic Violations** — Review Gates detect contract violations that pass deterministic checks (performance anti-patterns, security gaps, missing edge cases).\n\n**Reduces Human Review Burden** — Quality and Review Gates filter out obvious errors, letting humans focus on strategic fit rather than technical correctness.\n\n**Enforces Architectural Consistency** — Constitutional Review (via Review Gates) ensures code follows project principles, not just internet-average patterns.\n\n**Creates Clear Contracts** — Each gate type has explicit pass/fail criteria, making verification deterministic where possible and explicit where probabilistic.\n\nSee also:\n- [Adversarial Code Review](/patterns/adversarial-code-review) — Review Gate implementation\n- [Constitutional Review](/patterns/constitutional-review) — Dual-contract Review Gate\n- [The Spec](/patterns/the-spec) — Contract source for Review Gates\n- [Agent Constitution](/patterns/agent-constitution) — Architectural constraint source\n- [Model Routing](/patterns/model-routing) — Model selection for different gate types\n\n### Related Concepts\n- [Agentic SDLC](/concepts/agentic-sdlc) — The lifecycle where gates create phase boundaries\n- [Context Engineering](/concepts/context-engineering) — The practice of structuring context\n- [Guardrails](/concepts/guardrails) — Disambiguated term this pattern replaces\n\n### External Validation\n- [A Method for AI-Assisted Pull Request Reviews](https://lassala.net/2026/01/05/a-method-for-ai-assisted-pull-request-reviews-aligning-code-with-business-value/) (Carlos Lassala, January 2026) — Production implementation validating Review Gates' effectiveness in catching architectural violations through adversarial review",
    "tags": ["Architecture", "Quality Gates", "Context Engineering", "Validation"],
    "references": []
  },
  {
    "slug": "model-routing",
    "collection": "patterns",
    "title": "Model Routing",
    "description": "Strategic assignment of LLM models to SDLC phases based on reasoning capability versus execution speed.",
    "status": "Live",
    "content": "## Definition\n\n**Model Routing** is the strategic assignment of different Large Language Models (LLMs) to different phases of the software development lifecycle based on their capability profile.\n\nDifferent computational tasks have different performance characteristics. Model Routing matches model capabilities to task requirements: **reasoning depth** during design phases and **speed with large context windows** during implementation phases.\n\nThis is a tool selection strategy, not a delegation strategy. Engineers remain accountable for output quality while selecting the appropriate computational tool for each phase.\n\n## The Problem: Single-Model Inefficiency\n\nUsing one model for all phases creates a mismatch between computational capability and task requirements.\n\nHigh-speed models struggle with architectural decisions requiring deep constraint satisfaction. Reasoning models are too slow for high-volume implementation tasks. Models with massive context windows are expensive when you only need to process small, focused changes.\n\nEach model class optimizes for different performance characteristics. Using the wrong one wastes either quality (insufficient reasoning) or resources (excessive capability for simple tasks).\n\n## The Solution: Capability-Based Assignment\n\nWe categorize models into three capability profiles aligned with [Agentic SDLC](/concepts/agentic-sdlc) phases:\n\n| Capability Profile | Optimization | Primary Use Cases | Model Examples |\n|---|---|---|---|\n| **High Reasoning** | Deep logic, high latency, \"System 2\" thinking | Writing [Specs](/patterns/the-spec), architectural decisions, logic debugging, security analysis | Gemini 3 Deep Think, DeepSeek V3.2, OpenAI o3-pro |\n| **High Throughput** | Speed, low latency, real-time execution | Code generation, refactoring, unit tests, UI implementation | Gemini 3 Flash, Llama 4 Scout, Claude Haiku 4.5 |\n| **Massive Context** | Repository-scale context (500k-5M tokens) | Documentation analysis, codebase navigation, legacy system understanding | Gemini 3 Pro (5M tokens), Claude 4.5 Sonnet (500k), GPT-5 (RAG-native) |\n\n*Model examples current as of December 27, 2025. The LLM landscape evolves rapidly—validate capabilities and availability before implementation.*\n\n## Relationship to Levels of Autonomy\n\n[Levels of Autonomy](/concepts/levels-of-autonomy) define human oversight requirements. Model Routing complements this by matching computational capability to task characteristics:\n\n- **Complex architectural decisions** (L3 with high uncertainty) → High Reasoning models\n- **Well-specified implementation tasks** (L3 with clear contracts) → High Throughput models\n- **Exploratory analysis** (L2 with discovery focus) → Massive Context models\n\nThis ensures that the computational tool's capability profile matches the task's computational requirements and the degree of human verification needed.\n\nSee also:\n- [Agent Personas](/practices/agent-personas) — Context engineering practice for scoping agent work, extended by model routing\n- [The Spec](/patterns/the-spec) — The artifact produced by High Reasoning models in the planning phase\n- [Context Engineering](/concepts/context-engineering) — The practice of structuring context for optimal LLM performance",
    "tags": ["LLM Selection", "Context Engineering", "ASDLC", "Agent Architecture"],
    "references": [
      {
        "type": "website",
        "title": "My LLM Coding Workflow Going into 2026",
        "url": "https://addyo.substack.com/p/my-llm-coding-workflow-going-into",
        "author": "Addy Osmani",
        "published": "2026-01-01T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Addy Osmani's workflow guide emphasizing pragmatic model selection and mid-task model switching patterns based on reasoning needs."
      }
    ]
  },
  {
    "slug": "ralph-loop",
    "collection": "patterns",
    "title": "Ralph Loop",
    "description": "A persistence pattern that turns AI coding agents into autonomous, self-correcting workers by treating failure as feedback and iterating until external verification passes.",
    "status": "Experimental",
    "content": "## Definition\n\nThe **Ralph Loop**—named by Geoffrey Huntley after the persistently confused but undeterred Simpsons character Ralph Wiggum—is a persistence pattern that turns AI coding agents into autonomous, self-correcting workers.\n\nThe pattern operationalizes the [OODA Loop](/concepts/ooda-loop) for terminal-based agents and automates the [Learning Loop](/concepts/learning-loop) with machine-verifiable completion criteria. It enables sustained [L3-L4 autonomy](/concepts/levels-of-autonomy)—\"AFK coding\" where the developer initiates and returns to find committed changes.\n\n```mermaid\nflowchart LR\n    subgraph Input\n        PBI[\"PBI / Spec\"]\n    end\n    \n    subgraph \"Human-in-the-Loop (L1-L2)\"\n        DEV[\"Dev + Copilot\"]\n        E2E[\"E2E Tests\"]\n        DEV --> E2E\n    end\n    \n    subgraph \"Ralph Loop (L3-L4)\"\n        AGENT[\"Agent Iteration\"]\n        VERIFY[\"External Verification\"]\n        AGENT --> VERIFY\n        VERIFY -->|\"Fail\"| AGENT\n    end\n    \n    subgraph Output\n        REVIEW[\"Adversarial Review\"]\n        MERGE[\"Merge\"]\n        REVIEW --> MERGE\n    end\n    \n    PBI --> DEV\n    PBI --> AGENT\n    E2E --> REVIEW\n    VERIFY -->|\"Pass\"| REVIEW\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/ralph-loop-fig-1.svg\" alt=\"Mermaid Diagram\" />\n  \n</figure>\n\nBoth lanes start from the same well-structured PBI/Spec and converge at Adversarial Review. The Ralph Loop lane operates autonomously, with human oversight at review boundaries rather than every iteration.\n\n## The Problem: Human-in-the-Loop Bottleneck\n\nTraditional AI-assisted development creates a productivity ceiling: the human reviews every output before proceeding. This makes the human the slow component in an otherwise high-speed system.\n\nThe naive solution—trusting the agent's self-assessment—fails because LLMs confidently approve their own broken code. Research demonstrates that self-correction is only reliable with objective external feedback. Without it, the agent becomes a \"mimicry engine\" that hallucinates success.\n\n| Aspect | Traditional AI Interaction | Failure Mode |\n|--------|---------------------------|--------------|\n| **Execution Model** | Single-pass (one-shot) | Limited by human availability |\n| **Failure Response** | Process termination or manual re-prompt | Blocks on human attention |\n| **Verification** | Human review of every output | Human becomes bottleneck |\n\n## The Solution: External Verification Loop\n\nThe Ralph Loop inverts the quality control model: instead of treating LLM failures as terminal states requiring human intervention, it engineers failure as diagnostic data. The agent iterates until external verification (not self-assessment) confirms success.\n\n**Core insight:** Define the \"finish line\" through machine-verifiable tests, then let the agent iterate toward that finish line autonomously. **Iteration beats perfection.**\n\n| Aspect | Traditional AI | Ralph Loop |\n|--------|---------------|------------|\n| **Execution Model** | Single-pass | Continuous multi-cycle |\n| **Failure Response** | Manual re-prompt | Automatic feedback injection |\n| **Persistence Layer** | Context window | File system + Git history |\n| **Verification** | Human review | External tooling (Docker, Jest, tsc) |\n| **Objective** | Immediate correctness | Eventual convergence |\n\n## Anatomy\n\n### 1. Stop Hooks and Exit Interception\n\nThe agent attempts to exit when it believes it's done. A Stop hook intercepts the exit and evaluates current state against success criteria. If the agent hasn't produced a specific \"completion promise\" (e.g., `<promise>DONE</promise>`), the hook blocks exit and re-injects the original prompt.\n\nThis creates a self-referential loop: the agent confronts its previous work, analyzes why the task remains incomplete, and attempts a new approach.\n\n### 2. External Verification (Generator/Judge Separation)\n\nThe agent is not considered finished when it *believes* it's done—only when external verification confirms success:\n\n| Evaluation Type | Agent Logic | External Tooling |\n|-----------------|-------------|------------------|\n| Self-Assessment | \"I believe this is correct\" | None (Subjective) |\n| External Verification | \"I will run docker build\" | Docker Engine (Objective) |\n| Exit Decision | LLM decides to stop | System stops because tests pass |\n\nThis is the architectural enforcement of Generator/Judge separation from [Adversarial Code Review](/patterns/adversarial-code-review), but mechanized.\n\n### 3. Git as Persistent Memory\n\nContext windows rot, but Git history persists. Each iteration commits changes, so subsequent iterations \"see\" modifications from previous attempts. The codebase becomes the source of truth, not the conversation.\n\nGit also enables easy rollback if an iteration degrades quality.\n\n### 4. Context Rotation and Progress Files\n\n**Context rot:** Accumulation of error logs and irrelevant history degrades LLM reasoning.\n\n**Solution:** At 60-80% context capacity, trigger forced rotation to fresh context. Essential state carries over via structured progress files:\n\n- Summary of tasks completed\n- Failed approaches (to avoid repeating)\n- Architectural decisions to maintain\n- Files intentionally modified\n\nThis is the functional equivalent of `free()` for LLM memory—applied [Context Engineering](/concepts/context-engineering).\n\n### 5. Convergence Through Iteration\n\nThe probability of successful completion P(C) is a function of iterations n:\n\n```\nP(C) = 1 - (1 - p_success)^n\n```\n\nAs n increases (often up to 50 iterations), probability of handling complex bugs approaches 1.\n\n## OODA Loop Mapping\n\nThe Ralph Loop is [OODA](/concepts/ooda-loop) mechanized:\n\n| OODA Phase | Ralph Loop Implementation |\n|------------|--------------------------|\n| **Observe** | Read codebase state, error logs, failed builds |\n| **Orient** | Marshal context, interpret errors, read progress file |\n| **Decide** | Formulate specific plan for next iteration |\n| **Act** | Modify files, run tests, commit changes |\n\nThe cycle repeats until external verification passes.\n\n## Relationship to Other Patterns\n\n**[Context Gates](/patterns/context-gates)** — Context rotation + progress files = state filtering between iterations. Ralph Loops are Context Gates applied to the iteration boundary.\n\n**[Adversarial Code Review](/patterns/adversarial-code-review)** — Ralph architecturally enforces Generator/Judge separation. External tooling is the \"Judge\" that prevents self-assessment failure.\n\n**[The Spec](/patterns/the-spec)** — Completion promises require machine-verifiable success criteria. Well-structured Specs with Gherkin scenarios are ideal Ralph inputs.\n\n**Implementation:** Ralph Loops are typically implemented via CLI plugins (e.g., claude-code/ralph-wiggum) or standalone orchestrators. Specific tooling guidance is TBD.\n\n## Anti-Patterns\n\n| Anti-Pattern | Description | Failure Mode |\n|--------------|-------------|--------------|\n| **Vague Prompts** | \"Improve this codebase\" without specific criteria | Divergence; endless superficial changes |\n| **No External Verification** | Relying on agent self-assessment | Self-Assessment Trap; hallucinates success |\n| **No Iteration Caps** | Running without max iterations limit | Infinite loops; runaway API costs |\n| **No Sandbox Isolation** | Agent has access to sensitive host files | Security breach; SSH keys, cookies exposed |\n| **No Context Rotation** | Letting context window fill without rotation | Context rot; degraded reasoning |\n| **No Progress Files** | Fresh iterations re-discover completed work | Wasted tokens; repeated mistakes |\n\n## Guardrails\n\n| Risk | Mitigation |\n|------|------------|\n| Infinite Looping | Hard iteration caps (20-50 iterations) |\n| Context Rot | Periodic rotation at 60-80% capacity |\n| Security Breach | Sandbox isolation (Docker, WSL) |\n| Token Waste | Exact completion promise requirements |\n| Logic Drift | Frequent Git commits each iteration |\n| Cost Overrun | API cost tracking per session |",
    "tags": ["Agent Architecture", "Automation", "Iteration", "Verification"],
    "references": [
      {
        "type": "website",
        "title": "Understanding the Ralph Loop",
        "url": "https://ghuntley.com/ralph/",
        "author": "Geoffrey Huntley",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Original formulation of the Ralph Loop concept and philosophy."
      },
      {
        "type": "paper",
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
        "url": "https://arxiv.org/abs/2310.01798",
        "author": "Jie Huang et al.",
        "published": "2023-10-03T00:00:00.000Z",
        "accessed": "2026-01-12T00:00:00.000Z",
        "annotation": "Research demonstrating LLM self-correction limitations without external feedback."
      }
    ]
  },
  {
    "slug": "the-pbi",
    "collection": "patterns",
    "title": "The PBI",
    "description": "A transient execution unit that defines the delta (change) while pointing to permanent context (The Spec), optimized for agent consumption.",
    "status": "Live",
    "content": "## Definition\n\nThe Product Backlog Item (PBI) is the unit of execution in the ASDLC. While [The Spec](/patterns/the-spec) defines the **State** (how the system works), the PBI defines the **Delta** (the specific change to be made).\n\nIn an AI-native workflow, the PBI transforms from a \"User Story\" (negotiable conversation) into a **Prompt** (strict directive). The AI has flexibility in *how* code is written, but the PBI enforces strict boundaries on *what* is delivered.\n\n## The Problem: Ambiguous Work Items\n\nTraditional user stories (\"As a user, I want...\") are designed for human negotiation. They assume ongoing dialogue, implicit context, and shared understanding built over time.\n\nAgents don't negotiate. They execute. A vague story becomes a hallucinated implementation.\n\n**What fails without structured PBIs:**\n- Agents interpret scope liberally, touching unrelated code\n- No clear pointer to authoritative design decisions\n- Success criteria scattered across conversations\n- Merge conflicts from parallel agents hitting the same files\n\n## The Solution: Pointer, Not Container\n\nThe PBI acts as a **pointer** to permanent context, not a container for the full design. It defines the delta while referencing The Spec for the state.\n\n| Dimension   | The Spec                        | The PBI                          |\n| :---------- | :------------------------------ | :------------------------------- |\n| **Purpose** | Define the State (how it works) | Define the Delta (what changes)  |\n| **Lifespan**| Permanent (lives with the code) | Transient (closed after merge)   |\n| **Scope**   | Feature-level rules             | Task-level instructions          |\n| **Audience**| Architects, Agents (Reference)  | Agents, Developers (Execution)   |\n\n## Anatomy\n\nAn effective PBI consists of four parts:\n\n### 1. The Directive\n\nWhat to do, with explicit scope boundaries. Not a request—a constrained instruction.\n\n### 2. The Context Pointer\n\nReference to the permanent spec. Prevents the PBI from becoming a stale copy of design decisions that live elsewhere.\n\n### 3. The Verification Pointer\n\nLink to success criteria defined in the spec's Contract section. The agent knows exactly what \"done\" looks like.\n\n### 4. The Refinement Rule\n\nProtocol for when reality diverges from the spec. Does the agent stop? Update the spec? Flag for human review?\n\n## Bounded Agency\n\nBecause AI is probabilistic, it requires freedom to explore the \"How\" (implementation details, syntax choices). However, to prevent hallucination, we bound this freedom with non-negotiable constraints.\n\n**Negotiable (The Path):** Code structure, variable naming, internal logic flow, refactoring approaches.\n\n**Non-Negotiable (The Guardrails):** Steps defined in the PBI, outcome metrics in the Spec, documented anti-patterns, architectural boundaries.\n\nThe PBI is not a request for conversation—it's a constrained optimization problem.\n\n## Atomicity & Concurrency\n\nIn swarm execution (multiple agents working in parallel), each PBI must be:\n\n**Atomic:** The PBI delivers a complete, working increment. No partial states. If the agent stops mid-task, either the full change lands or nothing does.\n\n**Self-Testable:** Verification criteria must be executable without other pending PBIs completing first. If PBI-102 requires PBI-101's code to test, PBI-102 is not self-testable.\n\n**Isolated:** Changes target distinct files/modules. Two concurrent PBIs modifying the same file create merge conflicts and non-deterministic outcomes.\n\n### Dependency Declaration\n\nWhen a PBI requires another to complete first, the dependency is declared explicitly in the PBI structure—not discovered at merge time.\n\n## Relationship to Other Patterns\n\n**[The Spec](/patterns/the-spec)** — The permanent source of truth that PBIs reference. The Spec defines state; the PBI defines delta.\n\n**[PBI Authoring](/practices/pbi-authoring)** — The practice for writing effective PBIs, including templates and lifecycle.\n\nSee also:\n- [Spec-Driven Development](/concepts/spec-driven-development) — The overarching methodology\n- [Context Gates](/patterns/context-gates) — Validation checkpoints for PBI completion",
    "tags": ["Agile", "Product Backlog Item", "Spec-Driven Development", "Bounded Agency"],
    "references": []
  },
  {
    "slug": "the-spec",
    "collection": "patterns",
    "title": "Specs",
    "description": "Living documents that serve as the permanent source of truth for features, solving the context amnesia problem in agentic development.",
    "status": "Live",
    "content": "## Definition\n\nA **Spec** is the permanent source of truth for a feature. It defines *how* the system works (Design) and *how* we know it works (Quality).\n\nUnlike traditional tech specs or PRDs that are \"fire and forget,\" specs are **living documents**. They reside in the repository alongside the code and evolve with every change to the feature.\n\n## The Problem: Context Amnesia\n\nAgents do not have long-term memory. They cannot recall Jira tickets from six months ago or Slack conversations about architectural decisions. When an agent is tasked with modifying a feature, it needs immediate access to:\n\n- The architectural decisions that shaped the feature\n- The constraints that must not be violated\n- The quality criteria that define success\n\nWithout specs, agents reverse-engineer intent from code comments and commit messages—a process prone to hallucination and architectural drift.\n\nTraditional documentation fails because:\n- **Wikis decay** — separate systems fall out of sync with code\n- **Tickets disappear** — issue trackers capture deltas (changes), not state (current rules)\n- **Comments lie** — code comments describe implementation, not architectural intent\n- **Memory fails** — tribal knowledge evaporates when team members leave\n\nSpecs solve this by making documentation a **first-class citizen** in the codebase, subject to the same version control and review processes as the code itself.\n\n## State vs Delta\n\nThis is the core distinction that makes agentic development work at scale.\n\n| Dimension | The Spec | The PBI |\n|-----------|----------|---------|\n| **Purpose** | Define the State (how it works) | Define the Delta (what changes) |\n| **Lifespan** | Permanent (lives with the code) | Transient (closed after merge) |\n| **Scope** | Feature-level rules | Task-level instructions |\n| **Audience** | Architects, Agents (Reference) | Agents, Developers (Execution) |\n\nThe Spec defines the **current state** of the system:\n- \"All notifications must deliver within 100ms\"\n- \"API must handle 1000 req/sec\"\n\nThe PBI defines the **change**:\n- \"Add SMS fallback to notification system\"\n- \"Optimize database query for search endpoint\"\n\nThe PBI *references* the Spec for context and *updates* the Spec when it changes contracts.\n\n### Why Separation Matters\n\n```\nSprint 1: PBI-101 \"Build notification system\"\n  → Creates /plans/notifications/spec.md\n  → Spec defines: \"Deliver within 100ms via WebSocket\"\n\nSprint 3: PBI-203 \"Add SMS fallback\"\n  → Updates spec.md with new transport rules\n  → PBI-203 is closed, but the spec persists\n\nSprint 8: PBI-420 \"Refactor notification queue\"\n  → Agent reads spec.md, sees all rules still apply\n  → Refactoring preserves all documented contracts\n```\n\nWithout this separation, the agent in Sprint 8 has no visibility into decisions made in Sprint 1.\n\n## The Assembly Model\n\nSpecs serve as the context source for Feature Assembly. Multiple PBIs reference the same spec, and the spec's contracts are verified at quality gates.\n\n```mermaid\nflowchart LR\n  A[/spec.md/]\n\n  B[\\pbi-101.md\\]\n  C[\\pbi-203.md\\]\n  D[\\pbi-420.md\\]\n\n  B1[[FEATURE ASSEMBLY]]\n  C1[[FEATURE ASSEMBLY]]\n  D1[[FEATURE ASSEMBLY]]\n\n  E{GATE}\n\n  F[[MIGRATION]]\n\n  A --> B\n  A --> C\n  A --> D\n\n  B --> B1\n  C --> C1\n  D --> D1\n\n  B1 --> E\n  C1 --> E\n  D1 --> E\n\n  A --> |Context|E\n\n  E --> F\n```\n\n<figure class=\"mermaid-diagram\">\n  <img src=\"/mermaid/the-spec-fig-1.svg\" alt=\"Mermaid Diagram\" />\n  \n</figure>\n\n## Anatomy\n\nEvery spec consists of two parts:\n\n### Blueprint (Design)\nDefines **implementation constraints** that prevent agents from hallucinating invalid architectures.\n\n- **Context** — Why does this feature exist?\n- **Architecture** — API contracts, schemas, dependency directions\n- **Anti-Patterns** — What agents must NOT do\n\n### Contract (Quality)\nDefines **verification rules** that exist independently of any specific task.\n\n- **Definition of Done** — Observable success criteria\n- **Regression Guardrails** — Invariants that must never break\n- **Scenarios** — [Gherkin](/concepts/gherkin)-style behavioral specifications\n\nThe Contract section implements [Behavior-Driven Development](/concepts/behavior-driven-development) principles: scenarios define *what* behavior is expected without dictating *how* to implement it. This allows agents to interpret intent dynamically while providing clear verification criteria.\n\nFor detailed structure, examples, and templates, see the [Living Specs Practice Guide](/practices/living-specs).\n\n## Relationship to Other Patterns\n\n**[The PBI](/patterns/the-pbi)** — PBIs are the transient execution units (Delta) that reference specs for context. When a PBI changes contracts, it updates the spec in the same commit.\n\n**[Feature Assembly](/practices/feature-assembly)** — Specs define the acceptance criteria verified during assembly. The diagram above shows this flow.\n\n**[Experience Modeling](/patterns/experience-modeling)** — Experience models capture user journeys; specs capture the technical contracts that implement those journeys.\n\n**[Context Engineering](/concepts/context-engineering)** — Specs are structured context assets optimized for agent consumption, with predictable sections (Blueprint, Contract) for efficient extraction.\n\n**[Behavior-Driven Development](/concepts/behavior-driven-development)** — BDD provides the methodology for the Contract section. [Gherkin](/concepts/gherkin) scenarios serve as \"specifications of behavior\" that guide agent reasoning and define acceptance criteria.\n\n## Iterative Spec Refinement\n\nKent Beck critiques spec-driven approaches that assume \"you aren't going to learn anything during implementation.\" This is valid—specs are not waterfall artifacts.\n\n**The refinement cycle:**\n\n1. **Initial Spec** — Capture known constraints (API contracts, quality targets, anti-patterns)\n2. **Implementation Discovery** — Agent or human encounters edge cases, performance issues, or missing requirements\n3. **Spec Update** — New constraints committed alongside the code that revealed them\n4. **Verification** — Gate validates implementation against updated spec\n5. **Repeat**\n\nThis is the [Learning Loop](/concepts/learning-loop) applied to specs: the spec doesn't prevent learning—it captures learnings so agents can act on them in future sessions.\n\n> \"Large Language Models give us great leverage—but they only work if we focus on learning and understanding.\"\n> — Unmesh Joshi, via Martin Fowler\n\n## Industry Validation\n\nThe Spec pattern has emerged independently across the industry under different names. Notably, Rasmus Widing's **Product Requirement Prompt (PRP)** methodology defines the same structure: Goal + Why + Success Criteria + Context + Implementation Blueprint + Validation Loop.\n\nHis core principles—\"Plan before you prompt,\" \"Context is everything,\" \"Scope to what the model can reliably do\"—mirror ASDLC's Spec-Driven Development philosophy.\n\nSee [Product Requirement Prompts](/concepts/product-requirement-prompt) for the full mapping and [Industry Alignment](/resources/industry-alignment) for convergent frameworks.\n\nSee also:\n- [Living Specs Practice Guide](/practices/living-specs) — Implementation instructions, templates, and best practices\n- [Behavior-Driven Development](/concepts/behavior-driven-development) — The methodology behind Contract scenarios\n- [Gherkin](/concepts/gherkin) — Syntax guidance for writing behavioral specifications",
    "tags": [
      "Documentation",
      "Living Documentation",
      "Spec-Driven Development",
      "Context Engineering"
    ],
    "references": [
      {
        "type": "website",
        "title": "Living Documentation",
        "url": "https://martinfowler.com/bliki/LivingDocumentation.html",
        "author": "Martin Fowler",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Martin Fowler's definition of Living Documentation, the foundation for keeping documentation synchronized with code."
      },
      {
        "type": "repository",
        "title": "PRPs: Agentic Engineering",
        "url": "https://github.com/Wirasm/PRPs-agentic-eng",
        "author": "Rasmus Widing",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Practitioner-validated implementation of spec-driven agentic development, demonstrating convergent evolution with ASDLC principles."
      },
      {
        "type": "website",
        "title": "Martin Fowler Fragment: January 8, 2026",
        "url": "https://martinfowler.com/fragments/2026-01-08.html",
        "author": "Martin Fowler",
        "published": "2022-12-22T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Commentary on Anthropic research and Kent Beck's critique of spec-driven approaches."
      },
      {
        "type": "website",
        "title": "Kent Beck on Spec-Driven Development",
        "url": "https://www.linkedin.com/feed/update/urn:li:activity:7413956151144542208/",
        "author": "Kent Beck",
        "published": "2026-01-08T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Critique that specs must accommodate learning during implementation—addressed by iterative refinement."
      },
      {
        "type": "paper",
        "title": "How AI is Transforming Work at Anthropic",
        "url": "https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic",
        "author": "Saffron Huang et al.",
        "published": "2025-12-02T00:00:00.000Z",
        "accessed": "2026-01-09T00:00:00.000Z",
        "annotation": "Research validating the cold start problem and context transfer challenges in AI-assisted development."
      }
    ]
  },
  {
    "slug": "agent-personas",
    "collection": "practices",
    "title": "Agent Personas",
    "description": "A guide on how to add multiple personas to an AGENTS.md file, with examples.",
    "status": "Live",
    "content": "## Overview\n\nDefining clear personas for your agents is crucial for ensuring they understand their role, trigger constraints, and goals. This guide demonstrates how to structure multiple personas within your `AGENTS.md` file.\n\nPersonas are a context engineering practice—they scope agent work by defining boundaries and focus, not by role-playing. When combined with [Model Routing](/patterns/model-routing), personas can also specify which computational tool (LLM) to use for each type of work.\n\nFor the full specification of the `AGENTS.md` file, see the [AGENTS.md Specification](./agents-md-spec).\n\n## How to Add Multiple Personas\n\nYou can define multiple personas by specifying triggers, goals, and guidelines for each. This allows different agents (or the same agent in different contexts) to adopt specific behaviors suited for the task at hand.\n\n### Example: Our Internal Personas\n\nBelow are the personas we use, serving as a template for your own `AGENTS.md`.\n\n```markdown\n### 1.1. Lead Developer / Astro Architect (@Lead)\n**Trigger:** When asked about system design, specs, or planning.\n* **Goal**: Specify feature requirements, architecture, and required changes. Analyze the project state and plan next steps.\n* **Guidelines**\n  - **Schema Design:** When creating new content types, immediately define the Zod schema in `src/content/config.ts`.\n  - **Routing:** Use Astro's file-based routing. For dynamic docs, use `[...slug].astro` and `getStaticPaths()`.\n  - **SEO:** Ensure canonical URLs and Open Graph tags are generated for every new page.\n  - **Dev Performace:** Focus on tangible, deliverable outcomes.\n  - **Spec driven development:** Always produce clear, concise specifications before handing off to implementation agents.\n  - **Planned iterations:** Break down large tasks into manageable PBIs with clear acceptance criteria.\n\n### 1.2. Designer / User Experience Lead (@Designer)\n**Trigger:** When asked about Design system UI/UX, design systems, or visual consistency.\n* **Goal**: Ensure the design system can be effectively utilized by agents and humans alike.\n* **Guidelines**\n  - **Design Tokens:** Tokens must be set in `src/styles/tokens.css`. No hardcoded colors or fonts.\n  - **Component Consistency:** All components must adhere to the design system documented in `src/pages/resources/design-system.astro`. \n  - **Accessibility:** Ensure all components meet WCAG 2.1 AA standards.\n  - **Documentation:** Update the Design System page with any new components or styles introduced.\n  - **Experience Modeling Allowed:** Design system components are protected by a commit rule: use \\[EM] tag to override the rule.\n  \n### 1.3. Content Engineer / Technical Writer (@Content)\n**Trigger:** When asked to create or update documentation, articles, or knowledge base entries.\n* **Goal**: Produce high-quality, structured content that adheres to the project's schema and style guidelines.\n* **Guidelines**\n  - **Content Structure:** Follow the established folder structure in `src/content/` for concepts\n  \n### 1.4. Developer / Implementation Agent (@Dev)\n**Trigger:** When assigned implementation tasks or bug fixes.\n* **Goal**: Implement features, fix bugs, and ensure the codebase remains healthy and maintainable.\n* **Guidelines**\n  - **Expect PBIs:** Always work from a defined Product Backlog Item (PBI) with clear acceptance criteria, if available.\n  - **Type Safety:** Use TypeScript strictly. No `any` types allowed.\n  - **Component Imports:** Explicitly import all components used in `.astro` files.\n  - **Testing:** Ensure all changes pass `pnpm check` and `pnpm lint`\n  - **Document progress:** Update the relevant PBI in `docs/backlog/` with status and notes.md after completing tasks.\n```\n\n## Model Routing and Personas\n\nPersonas define **what work to do** and **how to scope it**. [Model Routing](/patterns/model-routing) is a separate practice that defines **which computational tool to use**.\n\n### Current State (December 2025)\n\nAI-assisted IDEs (Cursor, Windsurf, Claude Code) do **not** automatically select models based on persona definitions. Model selection is manual.\n\n### Best Practice: Keep Them Separate\n\n**Don't add model profiles to `AGENTS.md`** - It adds noise to the context window without providing automation value.\n\nInstead:\n1. **Keep personas focused** on triggers, goals, and guidelines\n2. **Use Model Routing separately** - Manually select models based on the task characteristics\n3. **Reference the pattern** when deciding which model to use\n\n### Matching Personas to Model Profiles\n\nWhen you invoke a persona, choose your model based on the work type:\n\n| Persona Type | Typical Work | Recommended Profile |\n|---|---|---|\n| Lead / Architect | System design, specs, architectural decisions | High Reasoning |\n| Developer / Implementation | Code generation, refactoring, tests | High Throughput |\n| Documentation Analyst | Legacy code analysis, comprehensive docs | Massive Context |\n\nThe workflow:\n1. **Identify the persona** needed for your task\n2. **Select the appropriate model** manually in your IDE\n3. **Invoke the persona** with your prompt\n\nThis keeps `AGENTS.md` lean and focused on scoping agent work, while model selection remains a deliberate engineering decision.",
    "tags": ["agents", "personas", "guide"],
    "references": []
  },
  {
    "slug": "agents-md-spec",
    "collection": "practices",
    "title": "AGENTS.md Specification",
    "description": "The definitive guide to the AGENTS.md file, including philosophy, anatomy, and implementation strategy.",
    "status": "Live",
    "content": "## DEFINITION\n\n`AGENTS.md` is an open format for guiding coding agents, acting as a \"README for agents.\" It provides a dedicated, predictable place for context and instructions—such as build steps, tests, and conventions—that help AI coding agents work effectively on a project.\n\nWe align with the [agents.md specification](https://agents.md), treating this file as the authoritative source of truth for agentic behavior within the ASDLC.\n\n## CORE PHILOSOPHY\n\n**1. A README for Agents**\n\nJust as `README.md` is for humans, `AGENTS.md` is for agents. It complements existing documentation by containing the detailed context—build commands, strict style guides, and test instructions—that agents need but might clutter a human-facing README.\n\n**2. Context is Code**\n\nIn the ASDLC, we treat `AGENTS.md` with the same rigor as production software:\n\n- **Version Controlled**: Tracked via git and PRs.\n- **Falsifiable**: Contains clear success criteria for agent actions.\n- **Optimized**: Structured to maximize signal-to-noise ratio for LLM context windows, preventing \"Lost in the Middle\" issues.\n\n**3. The Context Anchor (Long-Term Memory)**\n\nAGENTS.md solves the \"Context Amnesia\" problem. Agents are stateless—each new session starts with blank context. Without grounding, the agent reverts to generic training weights, forgetting project-specific patterns and lessons learned.\n\nThe `AGENTS.md` file acts as persistent \"standing orders\" for the agent across different sessions. By documenting your research tools, coding styles, architectural decisions, and accumulated lessons here, you prevent session-to-session drift.\n\nThis transforms `AGENTS.md` from a simple configuration file into the project's **institutional memory** for AI collaboration.\n\n## Format Philosophy\n\nThe structures in this specification (YAML maps, XML standards, tiered boundaries) are optimized for large teams and complex codebases. For smaller projects:\n\n- A simple markdown list may suffice\n- Focus on the *concepts* (persona, boundaries, commands) rather than exact syntax\n- Iterate on what produces best adherence from your specific model\n\nThe goal is signal density, not format compliance. Overly rigid specs create adoption friction. Let teams scale complexity to their needs.\n\n## TOOL-SPECIFIC CONSIDERATIONS\n\nDifferent AI coding tools look for different filenames. While `AGENTS.md` is the emerging standard, some tools require specific naming:\n\n| Tool | Expected Filename | Notes |\n| :--- | :--- | :--- |\n| **Cursor** | `.cursorrules` | Also reads `AGENTS.md` |\n| **Windsurf** | `.windsurfrules` | Also reads `AGENTS.md` |\n| **Claude Code** | `CLAUDE.md` | Does not read `AGENTS.md`; case-sensitive |\n| **Codex** | `AGENTS.md` | Native support |\n| **Zed** | `.rules` | Priority-based; reads `AGENTS.md` at lower priority |\n| **VS Code / Copilot** | `AGENTS.md` | Requires `chat.useAgentsMdFile` setting enabled |\n\n### Zed Priority Order\n\nZed uses the first matching file from this list:\n1. `.rules`\n2. `.cursorrules`\n3. `.windsurfrules`\n4. `.clinerules`\n5. `.github/copilot-instructions.md`\n6. `AGENT.md`\n7. `AGENTS.md`\n8. `CLAUDE.md`\n9. `GEMINI.md`\n\n### VS Code Configuration\n\nVS Code requires explicit opt-in for `AGENTS.md` support:\n- Enable `chat.useAgentsMdFile` setting to use `AGENTS.md`\n- Enable `chat.useNestedAgentsMdFiles` for subfolder-specific instructions\n\n### Recommendation\n\nCreate a symlink to support Claude Code without duplicating content:\n\n```bash\nln -s AGENTS.md CLAUDE.md\n```\n\nThis ensures Claude Code users get the same guidance while maintaining a single source of truth. Note that Claude Code also supports `CLAUDE.local.md` for personal preferences that shouldn't be version-controlled.\n\n## ECOSYSTEM TOOLS\n\nAs AGENTS.md adoption grows, tools emerge to bridge compatibility gaps between different coding assistants and enforce standards across heterogeneous environments.\n\n### Ruler\n\n[Ruler](https://github.com/intellectronica/ruler) is a meta-tool that synthesizes agent instructions from multiple sources (AGENTS.md, .cursorrules, project conventions) and injects them into coding assistants that may not natively support the AGENTS.md standard.\n\n**Key capabilities:**\n- **Cross-platform normalization**: Translates AGENTS.md into assistant-specific formats\n- **Multi-source aggregation**: Combines rules from various config files into unified context\n- **Dynamic injection**: Ensures consistent agent behavior across tools like Cursor, Windsurf, and Claude Code\n\n**Use case:** Teams using multiple coding assistants (e.g., some developers on Cursor, others on Claude Code) can maintain a single source of truth in AGENTS.md while Ruler handles distribution to tool-specific formats.\n\nThis demonstrates ecosystem maturity: when third-party tools emerge to solve interoperability problems, the standard has achieved meaningful adoption.\n\n## ASDLC IMPLEMENTATION STRATEGY\n\nWhile the [agents.md](https://agents.md) standard provides the format, the ASDLC recommends a structured implementation to ensure reliability. We present our `AGENTS.md` format not just as a list of tips, but as a segmented database of rules. This is *one* valid implementation strategy, particularly suited for rigorous engineering environments.\n\n### 1. Identity Anchoring (The Persona)\n\nEstablishes the specific expertise required to prune the model's search space. Without this, the model reverts to the \"average\" developer found in its training data. For detailed examples of defining multiple personas, see [Agent Personas](./agent-personas).\n\nBad: \"You are a coding assistant.\"\n\nGood: \"You are a Principal Systems Engineer specializing in Go 1.22, gRPC, and high-throughput concurrency patterns. You favor composition over inheritance.\"\n\n### 2. Contextual Alignment (The Mission)\n\nA concise, high-level summary of the project’s purpose and business domain. This is often formatted as a blockquote at the top of the file to \"set the stage\" for the agent's session.\n\n- **Why:** LLMs are stateless. A 50-token description differentiates a \"User\" in a banking app (high security/compliance) from a \"User\" in a casual game (low friction), reducing the need for corrective follow-up prompts.\n    \n- **Format:** Focus on the \"What\" and \"Why,\" not the narrative history.\n    \n\n**Example:**\n\n> **Project:** \"ZenTask\" - A minimalist productivity app. **Core Philosophy:** Local-first data architecture; offline support is mandatory.\n\n### 3. Operational Grounding (The Tech Stack)\n\nExplicitly defines the software environment to prevent \"Library Hallucination.\" This section must be exhaustive regarding key dependencies and restrictive regarding alternatives.\n\n- Directive: \"Runtime: Node.js v20 (LTS) exclusively.\"\n- Directive: \"Styling: Tailwind CSS only. Do not use CSS Modules or Emotion.\"\n- Directive: \"State: Zustand only. Do not use Redux.\"\n\n### 4. Behavioral Boundaries (Context Gates)\n\nReplaces vague \"Guardrails\" with a \"Three-Tiered Boundary\" system, or _constitution_. As the models are probabilistic, absolute prohibitions are unrealistic. Instead, this system categorizes rules by severity and required action. These rules are aimed to reducing the likelihood of critical errors. Note that you should always complement\nthe _constitution_ with explicit and deterministic _quality gates_ enforced by tests, linters, and CI/CD pipelines.\n\n**Tier 1 (Constitutive - ALWAYS): Non-negotiable standards.**\n\nExample: \"Always add JSDoc to exported functions.\"\n\n**Tier 2 (Procedural - ASK): High-risk operations requiring Human-in-the-Loop.**\n\nExample: \"Ask before running database migrations or deleting files.\"\n\n**Tier 3 (Hard Constraints - NEVER): Safety limits.**\n\nExample: \"Never commit secrets, API keys, or .env files.\"\n\n### 5. Semantic Directory Mapping\n\nWhen documenting the codebase structure in AGENTS.md, prefer Annotated YAML over ASCII trees.\n\n- **Use Valid Syntax:** Ensure the block allows an LLM to parse the structure as a dictionary.\n- **Annotate Key Files:** do not just list files; map them to a brief string describing their responsibility. This acts as a 'map legend' for the Agent, allowing it to route coding tasks to the correct file without needing to scan the file content first.\n- **Omit Noise:** Only include directories and files relevant to the Agent's operation or the architectural scope.\n\n**Example:**\n\n```yaml\ndirectory_map:\n  src:\n    # Core Application Logic\n    main.py: \"Application entry point; initializes the Agent Orchestrator\"\n    \n    agents:\n      # Individual Agent definitions\n      base_agent.py: \"Abstract base class defining the 'step()' and 'memory' interfaces\"\n    \n    utils:\n      # Shared libraries\n      llm_client.py: \"Wrapper for OpenAI/Anthropic APIs with retry logic\"\n```\n\n### 6. The Command Registry\n\nA lookup table mapping intent to execution. Agents often default to standard commands (npm test) which may fail in custom environments (make test-unit). The Registry forces specific tool usage.\n\n| Intent | Command | Notes |\n| :--- | :--- | :--- |\n| **Build** | pnpm build | Outputs to `dist/` |\n| **Test** | pnpm test:unit | Flags: --watch=false |\n| **Lint** | pnpm lint --fix | Self-correction enabled |\n\n\n### 7. Implementation notes\n\nXML-Tagging for Semantic Parsing\n\nTo maximize adherence, use pseudo-XML tags to encapsulate rules. This creates a \"schema\" that the model can parse more strictly than bullet points.\n\n```xml\n<coding_standard name=\"React Hooks\">\n  <instruction>\n    Use functional components and Hooks. Avoid Class components.\n    Ensure extensive use of custom hooks for logic reuse.\n  </instruction>\n  <anti_pattern>\n    class MyComponent extends React.Component {... }\n  </anti_pattern>\n  <preferred_pattern>\n    const MyComponent = () => {... }\n  </preferred_pattern>\n</coding_standard>\n```\n\n## REFERENCE TEMPLATE\n\n`Filename: AGENTS.md`\n\n```md\n# AGENTS.md - Context & Rules for AI Agents\n\n> **Project Mission:** High-throughput gRPC service for processing real-time financial transactions.\n> **Core Constraints:** Zero-trust security model, ACID compliance required for all writes.\n\n## 1. Identity & Persona\n- **Role:** Senior Systems Engineer\n- **Specialization:** High-throughput distributed systems in Go.\n- **Objective:** Write performant, thread-safe, and maintainable code.\n\n## 2. Tech Stack (Ground Truth)\n- **Language:** Go 1.22 (Use `iter` package for loops)\n- **Transport:** gRPC (Protobuf v3)\n- **Database:** PostgreSQL 15 with `pgx` driver (No ORM allowed)\n- **Infra:** Kubernetes, Helm, Docker\n\n## 3. Operational Boundaries (CRITICAL)\n- **NEVER** commit secrets, tokens, or `.env` files.\n- **NEVER** modify `api/proto` without running `buf generate`.\n- **ALWAYS** handle errors; never use `_` to ignore errors.\n- **ASK** before adding external dependencies.\n\n## 4. Command Registry\n| Action | Command | Note |\n| :--- | :--- | :--- |\n| **Build** | `make build` | Outputs to `./bin` |\n| **Test** | `make test` | Runs with `-race` detector |\n| **Lint** | `golangci-lint run` | Must pass before commit |\n| **Gen** | `make proto` | Regenerates gRPC stubs |\n\n## 5. Development Map\n```yaml\ndirectory_map:\n  api:\n    proto: \"Protocol Buffers definitions (Source of Truth)\"\n  cmd:\n    server: \"Main entry point, dependency injection wire-up\"\n  internal:\n    biz: \"Business logic and domain entities (Pure Go)\"\n    data: \"Data access layer (Postgres + pgx)\"\n```\n\n## 6. Coding Standards\n```xml\n<rule_set name=\"Concurrency\">\n  <instruction>Use `errgroup` for managing goroutines. Avoid bare `go` routines.</instruction>\n  <example>\n    <bad>go func() {... }()</bad>\n    <good>g.Go(func() error {... })</good>\n  </example>\n</rule_set>\n```\n\n## 7. Context References\n- **Database Schema:** Read `@database/schema.sql`\n- **API Contracts:** Read `@api/v1/service.proto`\n```",
    "tags": ["governance", "agents", "specification"],
    "references": [
      {
        "type": "video",
        "title": "Beyond Vibe-Coding: Learn Effective AI-Assisted Coding in 4 minutes",
        "url": "https://www.youtube.com/watch?v=HR5f2TDC65E",
        "publisher": "Vanishing Gradients",
        "annotation": "Source material for the Context Anchor concept. Explains how persistent context files ground AI agents across sessions."
      }
    ]
  },
  {
    "slug": "constitutional-review-implementation",
    "collection": "practices",
    "title": "Constitutional Review Implementation",
    "description": "Step-by-step guide for implementing Constitutional Review to validate code against both Spec and Constitution contracts.",
    "status": "Experimental",
    "content": "## Definition\n\n**Constitutional Review Implementation** is the operational practice of configuring and executing [Constitutional Review](/patterns/constitutional-review) to validate code against both functional requirements (the Spec) and architectural values (the Constitution).\n\nThis practice extends [Adversarial Code Review](/patterns/adversarial-code-review) by adding constitutional constraints to the Critic Agent's validation criteria.\n\n## When to Use\n\n**Use this practice when:**\n\n- Your project has documented architectural principles in an [Agent Constitution](/patterns/agent-constitution)\n- Code passes tests but you've experienced architectural violations in production\n- You want to enforce non-functional requirements (performance, security, data access patterns)\n- Your team needs to prevent \"regression to mediocrity\" (LLMs generating internet-average code)\n\n**Skip this practice when:**\n\n- You don't have an Agent Constitution documented (implement [AGENTS.md Specification](/practices/agents-md-spec) first)\n- Your project is a prototype without architectural constraints\n- The overhead of dual-contract validation exceeds the benefit (very small projects)\n\n## Prerequisites\n\nBefore implementing Constitutional Review, ensure you have:\n\n1. **[Agent Constitution](/patterns/agent-constitution)** documented (typically `AGENTS.md`)\n2. **[The Spec](/patterns/the-spec)** for the feature being reviewed\n3. **Critic Agent session** separate from the Builder Agent (fresh context)\n4. **Architectural constraints** clearly defined in the Constitution\n\nIf architectural constraints aren't documented, start with [AGENTS.md Specification](/practices/agents-md-spec).\n\n## Process\n\n### Step 1: Document Architectural Constraints in Constitution\n\nEnsure your Agent Constitution includes **non-functional constraints** that are:\n- **Specific** (not \"be performant\" but \"push filtering to database layer\")\n- **Testable** (can be objectively verified)\n- **Scoped** (applies to specific categories: Data Access, Performance, Security)\n\n**Example Structure**:\n\n```markdown\n## Architectural Constraints\n\n### Data Access\n- All filtering operations MUST be pushed to the database layer\n- Never use `findAll()` or `LoadAll()` followed by in-memory filtering\n- Queries must handle 10k+ records without memory issues\n\n### Performance\n- API responses < 200ms at p99\n- Database queries must use indexes for common filters\n- No N+1 query patterns\n\n### Security\n- User IDs never logged (use hashed identifiers)\n- All inputs validated against Zod schemas before processing\n- Authentication tokens expire within 24 hours\n- No hardcoded secrets (use environment variables)\n\n### Error Handling\n- Never fail silently (all errors logged with context)\n- User-facing errors never expose stack traces\n- Database errors map to generic \"Service unavailable\" messages\n```\n\n### Step 2: Configure Critic Agent Prompt\n\nExtend the standard [Adversarial Code Review](/patterns/adversarial-code-review) prompt to include constitutional validation.\n\n**System Prompt Template**:\n\n```\nYou are a rigorous Code Reviewer validating implementation against TWO sources of truth:\n\n1. The Spec (/plans/{feature-name}/spec.md)\n   - Functional requirements (what should it do?)\n   - API contracts (what are the inputs/outputs?)\n   - Data schemas (what is the structure?)\n\n2. The Constitution (AGENTS.md)\n   - Architectural patterns (e.g., \"push filtering to DB\")\n   - Performance constraints (e.g., \"queries handle 10k+ records\")\n   - Security rules (e.g., \"never log user IDs\")\n   - Error handling policies (e.g., \"never fail silently\")\n\nYOUR JOB:\nIdentify where code satisfies the Spec (functional) but violates the Constitution (architectural).\n\nCOMMON CONSTITUTIONAL VIOLATIONS TO CHECK:\n- LoadAll().Filter() pattern (data access violation)\n- Hardcoded secrets (security violation)\n- Missing error logging (error handling violation)\n- N+1 query patterns (performance violation)\n- User IDs in logs (security violation)\n\nOUTPUT FORMAT:\nFor each violation:\n1. Type: Constitutional Violation - [Category]\n2. Location: File path and line number\n3. Issue: What constitutional principle is violated\n4. Impact: Why this matters at scale (performance, security, maintainability)\n5. Remediation Path: Ordered steps to fix (prefer standard patterns, escalate if needed)\n6. Test Requirements: What tests would prevent regression\n\nIf no violations found, output: PASS - Constitutional Review\n```\n\n### Step 3: Execute Constitutional Review Workflow\n\nFollow this sequence to ensure proper validation:\n\n```\n┌─────────────┐\n│   Builder   │ → Implements Spec\n└──────┬──────┘\n       ↓\n┌─────────────────┐\n│  Quality Gates  │ → Tests, types, linting (deterministic)\n└──────┬──────────┘\n       ↓ (pass)\n┌──────────────────┐\n│ Spec Compliance  │ → Does it meet functional requirements?\n│     Review       │    (Adversarial Code Review)\n└──────┬───────────┘\n       ↓ (pass)\n┌──────────────────┐\n│ Constitutional   │ → Does it follow architectural principles?\n│     Review       │    (This practice)\n└──────┬───────────┘\n       ↓ (pass)\n┌─────────────────┐\n│ Acceptance Gate │ → Human strategic review (is it the right thing?)\n└─────────────────┘\n```\n\n**Execution Steps:**\n\n1. **Builder completes implementation** — Code written, tests pass\n2. **Quality Gates pass** — Compilation, linting, unit tests all green\n3. **Spec Compliance Review** — Critic validates functional requirements met\n4. **⭐ Constitutional Review** — Critic validates architectural principles followed:\n   - Open **new Critic Agent session** (fresh context, no Builder bias)\n   - Provide **Constitution** (AGENTS.md)\n   - Provide **Spec** (feature spec file)\n   - Provide **Code Diff** (changed files only)\n   - Use **Constitutional Review prompt** (from Step 2)\n   - Critic outputs violations or PASS\n5. **If violations found** → Return to Builder with remediation path\n6. **If PASS** → Proceed to Acceptance Gate (human review)\n\n### Step 4: Process Violation Reports\n\nWhen the Critic identifies constitutional violations, the output will follow this format:\n\n```\nVIOLATION: Constitutional - Data Access Pattern\n\nLocation: src/audit/AuditService.cs Line 23\n\nIssue: Loads all records into memory before filtering\nConstitution Violation: \"All filtering operations MUST be pushed to database layer\"\n\nImpact: \n- Works fine with small datasets (< 1k records)\n- Breaks at scale (10k+ records cause memory issues)\n- Creates N+1 query patterns in related queries\n- Violates performance SLA (API responses > 200ms)\n\nRemediation Path:\n1. Push filter to database query:\n   repository.FindWhere(x => x.Date > startDate)\n2. If ORM doesn't support this pattern, use raw SQL:\n   SELECT * FROM audit_logs WHERE date > @startDate\n3. Add performance test with 10k+ mock records to prevent regression\n4. Document the constraint in repository interface comments\n\nTest Requirements:\n- Add test: \"GetLogs with 10k records completes in < 200ms\"\n- Add test: \"GetLogs does not load entire table into memory\"\n  (mock repository, verify FindWhere called, not LoadAll)\n```\n\n**Processing steps:**\n\n1. **Return to Builder Agent** with full violation report\n2. **Builder implements remediation** following the ordered path\n3. **Re-run Constitutional Review** after fixes\n4. **Iterate until PASS** (typically 1-2 cycles)\n\n### Step 5: Update Constitution Based on Violations\n\nIf the Critic struggles to validate or produces unclear violations:\n\n**Indicators Constitution needs update:**\n- Critic says \"unclear whether this violates principles\"\n- False positives (valid code flagged as violation)\n- Violations lack specific remediation paths\n\n**Action**: Refine constitutional constraints:\n\n```markdown\n## Before (vague)\n### Performance\n- Code should be fast\n\n## After (specific)\n### Performance\n- API responses < 200ms at p99\n- Database queries must use indexes for common filters\n- Push filtering to database layer (never LoadAll().Filter())\n```\n\n## Examples\n\n### Example 1: Data Access Violation\n\n**Code Submitted**:\n```typescript\nasync function getActiveUsers() {\n  const users = await db.users.findAll();\n  return users.filter(u => u.status === 'active');\n}\n```\n\n**Constitutional Review Output**:\n```\nVIOLATION: Constitutional - Data Access Pattern\n\nLocation: src/users/service.ts Line 12\n\nIssue: Loads all users then filters in-memory\nConstitution Violation: \"Push filtering to database layer\"\n\nImpact: Works with 100 users, breaks at 10k+\n\nRemediation:\n1. db.users.findWhere({ status: 'active' })\n2. Add test with 10k users to verify performance\n\nTest Requirements:\n- Performance test: 10k users returns in < 200ms\n- Verify db.users.findWhere called (not findAll)\n```\n\n### Example 2: Security Violation\n\n**Code Submitted**:\n```typescript\nlogger.info(`User ${userId} logged in from ${ipAddress}`);\n```\n\n**Constitutional Review Output**:\n```\nVIOLATION: Constitutional - Security\n\nLocation: src/auth/logger.ts Line 45\n\nIssue: Logs user ID directly\nConstitution Violation: \"Never log user IDs (use hashed identifiers)\"\n\nImpact: GDPR compliance risk, audit log exposure\n\nRemediation:\n1. Hash user ID: logger.info(`User ${hashUserId(userId)} logged in...`)\n2. Implement hashUserId utility (SHA-256 with salt)\n3. Update all logging to use hashed IDs\n\nTest Requirements:\n- Verify logs do not contain raw user IDs\n- Verify hashed IDs are consistent (same user = same hash)\n```\n\n## Implementation Constraints\n\n**Requires Clear Constitutional Principles** — Vague constraints produce vague critiques. \"Be performant\" is not actionable. \"API responses < 200ms at p99\" is.\n\n**Not Fully Automated (Yet)** — As of January 2026, requires manual orchestration. You must manually:\n- Start new Critic Agent session\n- Provide Constitution + Spec + Code Diff\n- Interpret violation reports\n\n**Model Capability Variance** — Not all reasoning models perform equally at constitutional review. Recommended:\n- **High Reasoning models** for Critic (DeepSeek R1, Gemini 2.0 Flash Thinking, Claude 3.7 Sonnet)\n- Avoid throughput-optimized models (they skip architectural analysis)\n\n**False Positives Possible** — Architectural rules have exceptions. The Critic may flag valid code that violates general principles for good reasons. Human review in Acceptance Gate remains essential.\n\n**Context Window Limits** — Large diffs may exceed context windows. Solutions:\n- Review changed files only (not entire codebase)\n- Split large PRs into smaller, focused changes\n- Use Summary Gates to compress Spec to relevant sections\n\n## Troubleshooting\n\n### Issue: Critic approves code that violates Constitution\n\n**Cause**: Constitutional constraints not specific enough in AGENTS.md\n\n**Solution**: \n1. Review violation that slipped through\n2. Add specific constraint to Constitution:\n   ```markdown\n   ### Data Access\n   - ❌ Before: \"Queries should be efficient\"\n   - ✅ After: \"Never use LoadAll().Filter() - push filtering to database\"\n   ```\n3. Re-run Constitutional Review with updated Constitution\n\n### Issue: Critic flags valid code as violation\n\n**Cause**: Constitutional rule is too strict or lacks exceptions\n\n**Solution**:\n1. Document exception in Constitution:\n   ```markdown\n   ### Data Access\n   - Push filtering to database layer\n   - Exception: In-memory filtering allowed for cached reference data (< 100 records)\n   ```\n2. Update Critic prompt to recognize exceptions\n3. Proceed to Acceptance Gate (human validates exception is legitimate)\n\n### Issue: Constitutional Review takes too long\n\n**Cause**: Large code diffs or complex Constitution\n\n**Solution**:\n1. **Break up PRs** — Smaller, focused changes\n2. **Parallelize reviews** — Review multiple files concurrently\n3. **Use Summary Gates** — Compress Spec to relevant sections only\n4. **Cache Constitution** — Reuse constitutional context across reviews\n\n## Future Automation Potential\n\nThis practice is currently manual but has clear automation paths:\n\n**CI/CD Integration** — Automated constitutional review on PR creation:\n```yaml\n# .github/workflows/constitutional-review.yml\non: pull_request\njobs:\n  constitutional-review:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run Constitutional Review\n        run: |\n          constitutional-review-agent \\\n            --constitution AGENTS.md \\\n            --spec plans/${FEATURE}/spec.md \\\n            --diff ${{ github.event.pull_request.diff_url }}\n```\n\n**IDE Integration** — Real-time constitutional feedback:\n- Inline warnings when typing code that violates Constitution\n- Suggestions appear as you code (like linting)\n\n**Living Constitution** — Automatic updates:\n- Track approved exceptions to constitutional rules\n- Suggest Constitution updates when patterns emerge\n\n**Violation Analytics** — Dashboard tracking:\n- Which constitutional principles violated most often\n- Identify gaps in agent training\n- Measure constitutional compliance over time\n\nSee also:\n- [Constitutional Review](/patterns/constitutional-review) — The pattern this practice implements\n- [Adversarial Code Review](/patterns/adversarial-code-review) — The base review pattern\n- [Agent Constitution](/patterns/agent-constitution) — Source of architectural truth\n- [The Spec](/patterns/the-spec) — Source of functional truth\n- [AGENTS.md Specification](/practices/agents-md-spec) — How to document the Constitution\n- [Feature Assembly](/practices/feature-assembly) — The full workflow where this practice fits\n\n### External Validation\n- [A Method for AI-Assisted Pull Request Reviews](https://lassala.net/2026/01/05/a-method-for-ai-assisted-pull-request-reviews-aligning-code-with-business-value/) (Carlos Lassala, January 2026) — Production implementation showing this practice in action",
    "tags": ["Code Review", "Implementation", "Critic Agent", "Workflow", "Quality Gates"],
    "references": []
  },
  {
    "slug": "living-specs",
    "collection": "practices",
    "title": "Living Specs",
    "description": "Practical guide to creating and maintaining specs that evolve alongside your codebase.",
    "status": "Experimental",
    "content": "## Overview\n\nThis guide provides practical instructions for implementing the [Specs](/patterns/the-spec) pattern. While the pattern describes *what* specs are and *why* they matter, this guide focuses on *how* to create and maintain them.\n\n## When to Create a Spec\n\nCreate a spec when starting work that involves:\n\n**Feature Domains** — New functionality that introduces architectural patterns, API contracts, or data models that other parts of the system depend on.\n\n**User-Facing Workflows** — Features with defined user journeys and acceptance criteria that need preservation for future reference.\n\n**Cross-Team Dependencies** — Any feature that other teams will integrate with, requiring clear contract definitions.\n\n**Don't create specs for:** Simple bug fixes, trivial UI changes, configuration updates, or dependency bumps.\n\n## Spec granularity\n\nA spec should be detailed enough to serve as a contract for the feature, but not so detailed that it becomes a maintenance burden.\n\nSome spec features, like gherkin scenarios, are not always necessary if the feature is simple or well-understood. \n\n## When to Update a Spec\n\nUpdate an existing spec when:\n\n- API contracts change (new endpoints, modified payloads, deprecated routes)\n- Data schemas evolve (migrations, new fields, constraint changes)\n- Quality targets shift (performance, security, accessibility requirements)\n- Anti-patterns are discovered (during review or post-mortems)\n- Architecture decisions are made (any ADR should update relevant specs)\n\n**Golden Rule:** If code behavior changes, the spec MUST be updated in the same commit.\n\n## File Structure\n\nOrganize specs by **feature domain**, not by sprint or ticket number.\n\n```\n/project-root\n├── ARCHITECTURE.md           # Global system rules\n├── plans/                    # Feature-level specs\n│   ├── user-authentication/\n│   │   └── spec.md\n│   ├── payment-processing/\n│   │   └── spec.md\n│   └── notifications/\n│       └── spec.md\n└── src/                      # Implementation code\n```\n\n**Conventions:**\n- Directory name: kebab-case, matches the feature's conceptual name\n- File name: always `spec.md`\n- Location: `/plans/{feature-domain}/spec.md`\n- Scope: one spec per independently evolvable feature\n\n## Maintenance Protocol\n\n### Same-Commit Rule\n\nIf code changes behavior, update the spec in the same commit. Add \"Spec updated\" to your PR checklist.\n\n```\ngit commit -m \"feat(notifications): add SMS fallback\n\n- Implements SMS delivery when WebSocket fails\n- Updates /plans/notifications/spec.md with new transport layer\"\n```\n\n### Deprecation Over Deletion\n\nMark outdated sections as deprecated rather than removing them. This preserves historical context.\n\n```markdown\n### Architecture\n\n**[DEPRECATED 2024-12-01]**\n~~WebSocket transport via Socket.io library~~\nReplaced by native WebSocket API to reduce bundle size.\n\n**Current:**\nNative WebSocket connection via `/api/ws/notifications`\n```\n\n### Bidirectional Linking\n\nLink code to specs and specs to code:\n\n```typescript\n// Notification delivery must meet 100ms latency requirement\n// See: /plans/notifications/spec.md#contract\n```\n\n```markdown\n### Data Schema\nImplemented in `src/types/Notification.ts` using Zod validation.\n```\n\n## Template\n\n```markdown\n# Feature: [Feature Name]\n\n## Blueprint\n\n### Context\n[Why does this feature exist? What business problem does it solve?]\n\n### Architecture\n- **API Contracts:** `[METHOD] /api/v1/[endpoint]` - [Description]\n- **Data Models:** See `[file path]`\n- **Dependencies:** [What this depends on / what depends on this]\n\n### Anti-Patterns\n- [What agents must avoid, with rationale]\n\n## Contract\n\n### Definition of Done\n- [ ] [Observable success criterion]\n\n### Regression Guardrails\n- [Critical invariant that must never break]\n\n### Scenarios\n**Scenario: [Name]**\n- Given: [Precondition]\n- When: [Action]\n- Then: [Expected outcome]\n```\n\n## Anti-Patterns\n\n### The Stale Spec\n**Problem:** Spec created during planning, never updated as the feature evolves.\n\n**Solution:** Make spec updates mandatory in Definition of Done. Add PR checklist item.\n\n### The Spec in Slack\n**Problem:** Design decisions discussed in chat but never committed to the repo.\n\n**Solution:** After consensus, immediately update `spec.md` with a commit linking to the discussion.\n\n### The Monolithic Spec\n**Problem:** A single 5000-line spec tries to document the entire application.\n\n**Solution:** Split into feature-domain specs. Use `ARCHITECTURE.md` only for global cross-cutting concerns.\n\n### The Spec-as-Tutorial\n**Problem:** Spec reads like a beginner's guide, full of basic programming concepts.\n\n**Solution:** Assume engineering competence. Document constraints and decisions, not general knowledge.\n\n### The Copy-Paste Code\n**Problem:** Spec duplicates large chunks of implementation code.\n\n**Solution:** Reference canonical sources with file paths. Only include minimal examples to illustrate patterns.\n\nSee also:\n- [Specs Pattern](/patterns/the-spec) — Conceptual foundation\n- [The PBI](/patterns/the-pbi) — Execution units that reference specs",
    "tags": ["Documentation", "Spec-Driven Development", "Living Documentation"],
    "references": [
      {
        "type": "website",
        "title": "Living Documentation",
        "url": "https://martinfowler.com/bliki/LivingDocumentation.html",
        "author": "Martin Fowler",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Martin Fowler's definition of Living Documentation, the foundation for keeping documentation synchronized with code."
      }
    ]
  },
  {
    "slug": "micro-commits",
    "collection": "practices",
    "title": "Micro-Commits",
    "description": "Ultra-granular commit practice for agentic workflows, treating version control as reversible save points.",
    "status": "Live",
    "content": "## Definition\n\n**Micro-Commits** is the practice of committing code changes at much higher frequency than traditional development workflows. Each discrete task—often a single function, test, or file—receives its own commit.\n\nWhen working with LLM-generated code, commits become \"save points in a game\": checkpoints that enable instant rollback when probabilistic outputs introduce bugs or architectural drift.\n\n## The Problem: Coarse-Grained Commits in Agentic Workflows\n\nTraditional commit practices optimize for human readability and PR review: \"logical units of work\" that span multiple files and implement complete features.\n\nThis fails in agentic workflows because:\n\n**LLM outputs are probabilistic** — A model might generate correct code for 3 files and introduce subtle bugs in the 4th. Bundling all 4 files into one commit makes rollback destructive.\n\n**Regression to mediocrity** — Without checkpoints, it's difficult to identify where LLM output drifted from the [Spec](/patterns/the-spec) contracts.\n\n**Context loss** — Large commits obscure the sequence of decisions. When debugging, you need to know \"what changed, when, and why.\"\n\n**No emergency exit** — If an LLM generates a tangled mess across 10 files, your only option is manual surgery or discarding hours of work.\n\n## The Solution: Commit After Every Task\n\nMake a commit immediately after:\n- Completing a [PBI](/patterns/the-pbi) subtask\n- Generating a single function or module\n- Making a file pass linting/compilation\n- Adding one test\n- Any LLM-assisted edit that produces working code\n\nThis creates a breadcrumb trail of working states.\n\n## The Practice\n\n### 4.1. Atomic Tasks → Atomic Commits\n\nBreak work into small, testable chunks. Each chunk maps to one commit.\n\n**Example PBI:** \"Add OAuth login flow\"\n\n**Commit sequence:**\n```\n1. feat: add OAuth config schema\n2. feat: implement token exchange endpoint\n3. feat: add session storage for OAuth tokens\n4. test: add OAuth flow integration test\n5. refactor: extract OAuth error handling\n```\n\nThis aligns with atomic [PBIs](/patterns/the-pbi): small, bounded execution units.\n\n### 4.2. Commit Messages as Execution Log\n\nCommit messages document the sequence of LLM-assisted changes. They serve as:\n- **Context for debugging** — \"The bug appeared after commit 7.\"\n- **Briefing material for AI** — Feed recent commits to an LLM to explain current state.\n- **Audit trail** — Track architectural decisions embedded in code changes.\n\n**Format:**\n```\ntype(scope): brief description\n\n- Detail 1\n- Detail 2\n```\n\n**Example:**\n```\nfeat(auth): implement OAuth token validation\n\n- Add JWT verification middleware\n- Extract claims from token payload\n- Return 401 on expired tokens\n```\n\n### 4.3. Branches and Worktrees for Isolation\n\nUse branches or git worktrees to isolate LLM experiments:\n\n**Branches** — Separate experimental work from stable code. Merge only after validation.\n\n**Worktrees** — Run parallel LLM sessions on the same repository without context conflicts. Each worktree is an independent working directory.\n\n**Example workflow:**\n```bash\n# Create worktree for LLM experiment\ngit worktree add ../project-experiment experiment-oauth\n\n# Work in worktree, commit frequently\ncd ../project-experiment\n# ... LLM generates code ...\ngit commit -m \"feat: add OAuth callback handler\"\n\n# If successful, merge into main\ngit checkout main\ngit merge experiment-oauth\n\n# If failed, discard worktree\ngit worktree remove ../project-experiment\n```\n\nThis prevents contaminating the main branch with failed LLM output.\n\n### 4.4. Rollback as First-Class Operation\n\nWhen LLM output introduces bugs:\n\n**Identify the bad commit** — Review recent history to find where issues appeared.\n\n**Rollback to last known good state:**\n```bash\n# Soft reset (keeps changes as uncommitted)\ngit reset --soft HEAD~1\n\n# Hard reset (discards changes entirely)\ngit reset --hard HEAD~1\n```\n\n**Selective revert:**\n```bash\n# Revert specific commit without losing subsequent work\ngit revert <commit-hash>\n```\n\nThis is only safe because micro-commits isolate changes.\n\n### 5. Tidy History for Comprehension\n\nGranular commits create noisy history. Before merging to main, optionally squash related commits into logical units:\n\n```bash\n# Interactive rebase to squash last 5 commits\ngit rebase -i HEAD~5\n```\n\nThis preserves detailed history during development while creating clean history for long-term maintenance.\n\n**Trade-off:** Squashing removes granular rollback points. Only squash after validation passes [Quality Gates](/patterns/context-gates).\n\n## Relationship to The PBI\n\n[PBIs](/patterns/the-pbi) define **what to build**. Micro-Commits define **how to track progress**.\n\n**Atomic PBIs** (small, bounded tasks) naturally produce micro-commits. Each PBI generates 1-5 commits depending on complexity.\n\n**Example mapping:**\n- **PBI:** \"Implement retry logic with exponential backoff\"\n- **Commits:**\n  1. `feat: add retry wrapper function`\n  2. `feat: implement exponential backoff calculation`\n  3. `test: add retry logic unit tests`\n  4. `docs: update retry behavior in spec`\n\nThis makes PBI progress traceable and reversible.\n\nSee also:\n- [The PBI](/patterns/the-pbi) — Atomic execution units that map to commit sequences\n- [Context Gates](/patterns/context-gates) — Validation checkpoints that rely on granular commits\n- [Agentic SDLC](/concepts/agentic-sdlc) — The cybernetic loop where micro-commits enable rapid iteration",
    "tags": ["Version Control", "Git", "Safety", "Rollback"],
    "references": [
      {
        "type": "website",
        "title": "My LLM Coding Workflow Going into 2026",
        "url": "https://addyo.substack.com/p/my-llm-coding-workflow-going-into",
        "author": "Addy Osmani",
        "published": "2026-01-01T00:00:00.000Z",
        "accessed": "2026-01-08T00:00:00.000Z",
        "annotation": "Addy Osmani's practical guide emphasizing commits as 'save points in a game', validating the micro-commit approach for LLM workflows."
      }
    ]
  },
  {
    "slug": "pbi-authoring",
    "collection": "practices",
    "title": "PBI Authoring",
    "description": "How to write Product Backlog Items that agents can read, execute, and verify—with templates and lifecycle guidance.",
    "status": "Live",
    "content": "## Definition\n\n**PBI Authoring** is the practice of writing Product Backlog Items optimized for agent execution. This includes structuring the four-part anatomy, ensuring machine accessibility, and managing the PBI lifecycle from planning through closure.\n\nFollowing this practice produces PBIs that agents can programmatically access, unambiguously interpret, and verifiably complete.\n\n## When to Use\n\n**Use this practice when:**\n- Creating work items for agent execution\n- Planning a sprint with AI-assisted development\n- Converting legacy user stories to agent-ready format\n- Setting up a new project's backlog structure\n\n**Skip this practice when:**\n- Work is purely exploratory with no defined outcome\n- The task is a one-off command (use direct prompting instead)\n- Human-only execution with no agent involvement\n\n## Process\n\n### Step 1: Ensure Accessibility\n\n**Invisibility is a bug.** If an agent cannot read the PBI, the workflow is broken.\n\nA PBI locked inside a web UI without API or MCP integration is useless to an AI developer. The agent must programmatically access the work item without requiring human copy-paste.\n\n**Valid access methods:**\n\n| Method | Description |\n|--------|-------------|\n| **MCP Integration** | Agent connected to Issue Tracker (Linear, Jira, GitHub) via MCP |\n| **Repo-Based** | PBI exists as a markdown file (e.g., `tasks/PBI-123.md`) |\n| **API Access** | Tracker exposes REST/GraphQL API the agent can query |\n\n**If your tracker has no API access:** Mirror PBIs as markdown files during sprint planning, or implement MCP integration.\n\n### Step 2: Write the Directive\n\nState what to do with explicit scope boundaries. Be imperative, not conversational.\n\n**Good:**\n```\nImplement the API Layer for user notification preferences.\nScope: Only touch the `src/api/notifications` folder.\n```\n\n**Bad:**\n```\nAs a user, I want to manage my notification preferences so that I can control what emails I receive.\n```\n\nThe second example requires interpretation. The first is executable.\n\n> [!TIP]\n> **Prompt for the Plan.** Even if your tool handles planning automatically, explicitly instruct the agent to output its plan for review. This forces the Specify → Plan → Execute loop.\n>\n> **Example Directive:** \"Analyze the Spec, propose a step-by-step plan including which files you will touch, and wait for my approval before editing files.\"\n\n### Step 3: Add Context Pointers\n\nReference the permanent spec—don't copy design decisions into the PBI.\n\n```\nReference: `plans/notifications/spec.md` Part A for the schema.\nSee the \"Architecture\" section for endpoint contracts.\n```\n\n**Why pointers, not copies:** Specs evolve. A copied schema in a PBI becomes stale the moment the spec updates. Pointers ensure the agent always reads the authoritative source.\n\n### Step 4: Define Verification Criteria\n\nLink to success criteria in the spec, or define inline checkboxes.\n\n```\nMust pass \"Scenario 3: Preference Update\" defined in \n`plans/notifications/spec.md` Part B (Contract).\n```\n\nOr inline:\n```\n- [ ] POST /preferences returns 201 on valid input\n- [ ] Invalid payload returns 400 with error schema\n- [ ] Unit test coverage > 80%\n```\n\n### Step 5: Declare Dependencies\n\nExplicitly state what blocks this PBI and what it blocks.\n\n```\n## Dependencies\n- Blocked by: PBI-101 (creates the base schema)\n- Must merge before: PBI-103 (extends this endpoint)\n```\n\n**Anti-Pattern:** Implicit dependencies discovered at merge time. Identify during planning; either sequence the work or refactor into independent units.\n\n### Step 6: Set the Refinement Rule\n\nDefine what happens when reality diverges from the spec.\n\n```\nIf implementation requires changing the Architecture, you MUST \nupdate `spec.md` in the same PR with a changelog entry.\n```\n\nOptions to specify:\n- **Update spec in same PR** — Agent has authority to evolve the design\n- **Flag for human review** — Agent stops and requests guidance\n- **Proceed with deviation log** — Agent continues but documents the gap\n\n## Template\n\n```markdown\n# PBI-XXX: [Brief Imperative Title]\n\n## Directive\n[What to build/change in 1-2 sentences]\n\n**Scope:**\n- [Explicit file/folder boundaries]\n- [What NOT to touch]\n\n## Dependencies\n- Blocked by: [PBI-YYY if any, or \"None\"]\n- Must merge before: [PBI-ZZZ if any, or \"None\"]\n\n## Context\nRead: `[path/to/spec.md]`\n- [Specific section to reference]\n\n## Verification\n- [ ] [Criterion 1: Functional requirement]\n- [ ] [Criterion 2: Performance/quality requirement]\n- [ ] [Criterion 3: Test coverage requirement]\n\n## Refinement Protocol\n[What to do if the spec needs updating during implementation]\n```\n\n## PBI Lifecycle\n\n| Phase | Actor | Action |\n|-------|-------|--------|\n| **Planning** | Human | Creates PBI with 4-part structure |\n| **Assignment** | Human/System | PBI assigned to Agent or Developer |\n| **Execution** | Agent | Reads Spec, implements Delta |\n| **Review** | Human | Verifies against Spec's Contract section |\n| **Merge** | Human/System | Code merged, Spec updated if needed |\n| **Closure** | System | PBI archived, linked to commit/PR |\n\n## Common Mistakes\n\n### The User Story Hangover\n\n**Problem:** PBI written as \"As a user, I want...\" with no explicit scope or verification.\n\n**Solution:** Rewrite in imperative form with scope boundaries and checkable criteria.\n\n### The Spec Copy\n\n**Problem:** PBI contains copied design decisions that drift from the spec.\n\n**Solution:** Use pointers to spec sections, never copy content that lives elsewhere.\n\n### The Hidden Dependency\n\n**Problem:** Two PBIs touch the same files; discovered at merge time.\n\n**Solution:** During planning, map file ownership. If overlap exists, sequence the PBIs or refactor scope.\n\n### The Untestable Increment\n\n**Problem:** PBI verification requires another PBI to complete first.\n\n**Solution:** Ensure each PBI is self-testable. If not possible, merge into a single PBI or create test fixtures.\n\n## Related Patterns\n\nThis practice implements:\n\n- **[The PBI](/patterns/the-pbi)** — The structural pattern this practice executes\n\nSee also:\n\n- **[The Spec](/patterns/the-spec)** — The permanent context PBIs reference\n- **[Living Specs](/practices/living-specs)** — How to maintain the specs PBIs point to",
    "tags": ["Agile", "Product Backlog Item", "Workflow", "Agent Execution"],
    "references": [
      {
        "type": "video",
        "title": "Beyond Vibe-Coding: Learn Effective AI-Assisted Coding in 4 minutes",
        "url": "https://www.youtube.com/watch?v=HR5f2TDC65E",
        "publisher": "Vanishing Gradients",
        "annotation": "Source material for the 'Prompt for the Plan' tip. Explains the Specify → Plan → Execute workflow."
      }
    ]
  }
]
